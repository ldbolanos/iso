{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo:\n",
    "- Add Glove Word Vectors\n",
    "- Add Xavier initialisation for all the weights of the networks\n",
    "- Look up on how to structure the auxillary function?\n",
    "- Update the training function to include all the parts of the model\n",
    "- What is KL Annealing???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T16:17:01.266650Z",
     "start_time": "2019-02-02T16:17:01.262547Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import bcolz\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:40:37.774123Z",
     "start_time": "2019-02-02T18:40:37.771296Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new files.. Done.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Need a function to load the GloVe word embeddings.\n",
    "\"\"\"\n",
    "glovePath = \"/media/data/Datasets/glove\"\n",
    "\n",
    "def loadGlove(glove_path, dim=100):\n",
    "    acceptedDimensions = [50, 100, 200, 300]\n",
    "    if dim not in acceptedDimensions:\n",
    "        print(\"You didn't choose a right dimension.\")\n",
    "    pickleWordFile = f'{glove_path}/6B.'+str(dim)+'_words.pkl'\n",
    "    pickleIdFile   = f'{glove_path}/6B.'+str(dim)+'_idx.pkl'\n",
    "    pickleDatFile  = f'{glove_path}/glove.6B.'+str(dim)+'.dat'\n",
    "    pickleDataset  = f'{glove_path}/glove.6B.'+str(dim)+'d.txt'\n",
    "    \n",
    "    if os.path.isfile(pickleWordFile):\n",
    "        # check if we've made the outputs before\n",
    "        print(\"Preloading files..\", end=\" \")\n",
    "        vectors = bcolz.open(pickleDatFile)[:]\n",
    "        words = pickle.load(open(pickleWordFile, 'rb'))\n",
    "        word2idx = pickle.load(open(pickleIdFile, 'rb'))\n",
    "        glove = {w: vectors[word2idx[w]] for w in words}\n",
    "        print(\"Done.\")\n",
    "        return glove\n",
    "    else:\n",
    "        print(\"Creating new files..\", end=\" \")\n",
    "        words    = []\n",
    "        idx      = 0\n",
    "        word2idx = {}\n",
    "        vectors = bcolz.carray(np.zeros(1), rootdir=pickleDatFile, mode='w')\n",
    "\n",
    "        with open(pickleDataset, 'rb') as f:\n",
    "            for l in f:\n",
    "                line = l.decode().split()\n",
    "                word = line[0]\n",
    "                words.append(word)\n",
    "                word2idx[word] = idx\n",
    "                idx += 1\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "                vectors.append(vect)\n",
    "\n",
    "        vectors = bcolz.carray(vectors[1:].reshape((400000, dim)),\n",
    "                               rootdir=pickleDatFile, mode='w')\n",
    "        vectors.flush()\n",
    "        # save the outputs\n",
    "        pickle.dump(words, open(pickleWordFile, 'wb'))\n",
    "        pickle.dump(word2idx, open(pickleIdFile, 'wb'))\n",
    "        # create the dataset\n",
    "        glove = {w: vectors[word2idx[w]] for w in words}\n",
    "        print(\"Done.\")\n",
    "        return glove\n",
    "\n",
    "glove = loadGlove(glovePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.98392315,  1.42974139, -0.08124647,  0.04007039, -0.31925197,\n",
       "         0.5757713 ,  0.09597052, -0.21517332, -0.12606593,  0.93122532,\n",
       "        -0.12218803,  0.27582826,  0.34511007,  0.03050594,  0.84269424,\n",
       "        -1.12956095,  0.2612932 ,  0.44484818,  1.927427  , -0.95003986,\n",
       "         0.38939865, -0.62175537,  0.45345025, -0.21659891,  0.17194881,\n",
       "         0.09370914, -0.5321684 ,  0.08821129,  0.99128168, -0.17201137,\n",
       "         0.08552617, -0.66286482, -1.18550727, -0.09153028,  0.11950196,\n",
       "         0.95075075,  0.16962305,  0.56645186,  0.25223766, -0.13500021,\n",
       "        -0.21090104,  0.30063125, -0.58097386,  1.0647789 , -0.76772187,\n",
       "         1.04934135, -0.20321516,  0.41729599, -0.14035122, -0.19111973,\n",
       "        -0.018021  ,  0.97393941, -0.15037015, -0.23272674,  0.23922272,\n",
       "        -0.29944397,  0.74329089,  0.46211778, -0.87003664, -0.77989967,\n",
       "         0.78152365, -0.5156572 , -0.71853112, -1.00316458,  0.00300219,\n",
       "         0.87408604,  0.54657905, -0.44320988,  0.56794198,  0.05015717,\n",
       "        -0.55208738,  0.11741408,  0.21894418,  0.09392769,  0.54130194,\n",
       "        -0.5407952 , -0.56249632,  0.37078608,  0.87663356,  0.60062017,\n",
       "         0.44078588,  0.26111463,  0.17461218,  0.13862442,  0.43905954,\n",
       "        -0.14644959, -0.33929994,  0.404721  , -0.00687405,  0.24186778,\n",
       "         0.09226805,  0.45372252,  0.07417801, -0.63561336,  0.41015122,\n",
       "         0.53940636,  0.36856639, -0.22655108, -0.69928162,  0.38295158]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createWeightMatrix(targetVocab, glove):\n",
    "    \"\"\"\n",
    "    For each word in the dataset's vocabulary,\n",
    "    check if it's also in the @glove vocab.\n",
    "    If it does, then we load the pre-trained word vector.\n",
    "    Otherwise we use some random vector.\n",
    "    \"\"\"\n",
    "    length  = len(targetVocab)\n",
    "    dim = glove['fail'].shape\n",
    "    wMatrix = np.zeros((length, dim[0]))\n",
    "    wordsFound = 0\n",
    "    for i, word in enumerate(targetVocab):\n",
    "        try:\n",
    "            wMatrix[i] = glove[word]\n",
    "            wordsFound += 1\n",
    "        except KeyError:\n",
    "            wMatrix[i] = np.random.normal(scale=0.6, size=dim)\n",
    "    return wMatrix\n",
    "\n",
    "createWeightMatrix(['noodb'], glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:40:40.519162Z",
     "start_time": "2019-02-02T18:40:40.515806Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 28*28))\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T15:49:50.995429Z",
     "start_time": "2019-02-02T15:49:50.985772Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This'll be a bi-directional GRU.\n",
    "    Utilises equation (1) in the paper.\n",
    "    \n",
    "    The hidden size is 512 as per the paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputSize, hiddenSize=512):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "        # this embedding is a simple lookup table that stores the embeddings of a \n",
    "        # fixed dictionary and size.\n",
    "        \n",
    "        # This module is often used to store word embeddings and retrieve them\n",
    "        # using indices. \n",
    "        # The input to the module is a list of indices, and \n",
    "        # the output is the corresponding word embeddings.\n",
    "        self.embedding = nn.Embedding(inputSize, hiddenSize)\n",
    "        self.gru = nn.GRU(hiddenSize, hiddenSize, bidirectional=True)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # load the input into the embedding before doing GRU computation.\n",
    "        output = self.embedding(x).view(1,1,-1)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1, self.hiddenSize, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate a set of attention weights.\n",
    "- Dot product the attention weights with the encoder output vectors.\n",
    "- This result should contain information about that specific part of the input sequence, which helps the decoder choose the right words. We'll store this into a variable called attentionApplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T19:17:11.259515Z",
     "start_time": "2019-02-02T19:17:11.248672Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO: Add layer normalisation?\n",
    "    https://arxiv.org/abs/1607.06450\n",
    "    \n",
    "    We also set the hidden state size to 512.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, hiddenSize=512, outputSize, maxLength = max_length):\n",
    "        \"\"\"\n",
    "        # dropout omitted\n",
    "        \"\"\"\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.outputSize = outputSize\n",
    "        self.maxLength = maxLength\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.outputSize, self.hiddenSize)\n",
    "        # self.attention is our tiny neural network that takes in the hidden weights\n",
    "        # and the previous hidden weights.\n",
    "        self.attention = nn.Linear(self.hiddenSize * 2, self.maxLength)\n",
    "        self.attentionCombined = nn.Linear(self.hiddenSize * 2, self.hiddenSize)\n",
    "        \n",
    "        self.gru = nn.GRU(self.hiddenSize, self.hiddenSize)\n",
    "        \n",
    "        self.out = nn.Linear(self.hiddenSize, self.outputSize)\n",
    "    \n",
    "    def forward(self, input, hidden, previousHidden, encoderOutputs):\n",
    "        embedded = self.embedding(input).view(1,1,-1)\n",
    "        \n",
    "        # concatenate hidden layer inputs together.\n",
    "        attentionInputs =  torch.cat((embedded[0], hidden[0]), 1)\n",
    "        attentionWeights = F.softmax(self.attention(attentionInputs), dim=1)\n",
    "        \n",
    "        attentionApplied = torch.bmm(attentionWeights.unsqueeze(0),\n",
    "                                    encoderOutputs.unsqueeze(0))\n",
    "        \n",
    "        output = torch.cat((embedded[0], attentionApplied[0]), 1)\n",
    "        output = self.attentionCombined(output).unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attentionWeights\n",
    "\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1, self.hiddenSize, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T19:33:13.673408Z",
     "start_time": "2019-02-02T19:33:13.666250Z"
    }
   },
   "outputs": [],
   "source": [
    "class Inference(nn.Module):\n",
    "    \"\"\"\n",
    "    Note that the inference and prior networks\n",
    "    are a simple 1 layer feed forward neural network.\n",
    "    Therefore the size of the weights are entirely based on the size\n",
    "    of the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_size, class_size, latent_size=400):\n",
    "        super(Inference, self).__init__()\n",
    "        \n",
    "        self.feature_size = feature_size\n",
    "        self.class_size = class_size\n",
    "\n",
    "        # encode\n",
    "        self.fc1  = nn.Linear(feature_size + class_size, 400)\n",
    "        self.mean = nn.Linear(400, latent_size)\n",
    "        self.var = nn.Linear(400, latent_size)\n",
    "\n",
    "        # decode\n",
    "        self.fc3 = nn.Linear(latent_size + class_size, 400)\n",
    "        self.fc4 = nn.Linear(400, feature_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x_forward, c, h_backward): # Q(z|x, c)\n",
    "        '''\n",
    "        x: (bs, feature_size)\n",
    "        c: (bs, class_size)\n",
    "        '''\n",
    "        inputs = torch.cat([x_forward, c, h_backward], 1) # (bs, feature_size+class_size)\n",
    "        h1 = self.relu(self.fc1(inputs))\n",
    "        z_mu = self.mean(h1)\n",
    "        z_var = self.var(h1)\n",
    "        return z_mu, z_var\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        # samples your mu, logvar to get z.\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std) + mu\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z, c): # P(x|z, c)\n",
    "        '''\n",
    "        z: (bs, latent_size)\n",
    "        c: (bs, class_size)\n",
    "        '''\n",
    "        inputs = torch.cat([z, c], 1) # (bs, latent_size+class_size)\n",
    "        h3 = self.relu(self.fc3(inputs))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x.view(-1, 28*28), c)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z, c), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T19:46:56.627386Z",
     "start_time": "2019-02-02T19:46:56.615695Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Prior(nn.Module):\n",
    "    def __init__(self, feature_size, class_size):\n",
    "        super(Prior, self).__init__()\n",
    "        \n",
    "        self.feature_size = feature_size\n",
    "        self.class_size = class_size\n",
    "\n",
    "        # encode\n",
    "        self.fc1  = nn.Linear(feature_size + class_size, 400)\n",
    "        self.mean = nn.Linear(400, latent_size)\n",
    "        self.var = nn.Linear(400, latent_size)\n",
    "\n",
    "        # decode\n",
    "        self.fc3 = nn.Linear(latent_size + class_size, 400)\n",
    "        self.fc4 = nn.Linear(400, feature_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, h, c): # Q(z|x, c)\n",
    "        '''\n",
    "        x: (bs, feature_size)\n",
    "        c: (bs, class_size)\n",
    "        '''\n",
    "        inputs = torch.cat([h, c], 1) # (bs, feature_size+class_size)\n",
    "        h1 = self.relu(self.fc1(inputs))\n",
    "        z_mu = self.mean(h1)\n",
    "        z_var = self.var(h1)\n",
    "        return z_mu, z_var\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        # samples your mu, logvar to get z.\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std) + mu\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z, c): # P(x|z, c)\n",
    "        '''\n",
    "        z: (bs, latent_size)\n",
    "        c: (bs, class_size)\n",
    "        '''\n",
    "        inputs = torch.cat([z, c], 1) # (bs, latent_size+class_size)\n",
    "        h3 = self.relu(self.fc3(inputs))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x.view(-1, 28*28), c)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z, c), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Auxillary(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        self.fc1  = nn.Linear(latent_size, 400)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        The motive here is to produce an auxillary loss for our \n",
    "        training objective.\n",
    "        \n",
    "        We do this by Sequential Bag of Words (SBOW) as the\n",
    "        auxillary objective for the proposed VAD model. \n",
    "        \n",
    "        We want to predict the bag of succeeding words\n",
    "        in the response using the latent variable z at each\n",
    "        time step.\n",
    "        \"\"\"\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:43:18.849088Z",
     "start_time": "2019-02-02T18:43:18.841608Z"
    }
   },
   "outputs": [],
   "source": [
    "teacherForcingRatio = 0.5\n",
    "\n",
    "def train(x, \n",
    "          y, \n",
    "          encoder, \n",
    "          decoder, \n",
    "          backwards,\n",
    "          inference,\n",
    "          prior,\n",
    "          encoderOptimiser, \n",
    "          decoderOptimiser, \n",
    "          backwardsOptimiser, \n",
    "          inferenceOptimiser,\n",
    "          priorOptimiser,\n",
    "          maxLength = max_length):\n",
    "    \n",
    "    # initialise hidden variables\n",
    "    encoderHidden = encoder.initHidden()\n",
    "    backwardsHidden = backwards.initHidden()\n",
    "    \n",
    "    # initialise gradients (IMPORTANT!)\n",
    "    encoderOptimiser.zero_grad()\n",
    "    decoderOptimiser.zero_grad()\n",
    "    backwardsOptimiser.zero_grad()\n",
    "    inferenceOptimiser.zero_grad()\n",
    "    priorOptimiser.zero_grad()\n",
    "    \n",
    "    inputLength = x.size(0)\n",
    "    targetLength = y.size(0)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # set up encoder computation\n",
    "    encoderOutputs = torch.zeros(maxLength, encoder.hiddenSize, device=device)\n",
    "    backwardOutputs = torch.zeros(maxLength, encoder.hiddenSize, device=device)\n",
    "\n",
    "    for ei in range(inputLength):\n",
    "        encoderOutput, encoderHidden = encoder(x[ei], encoderHidden)\n",
    "        encoderOutputs[ei] = encoderOutput[0,0]\n",
    "    \n",
    "    for t in range(targetLength-1, 0, 1):\n",
    "        # here we can also build the backwards RNN that takes in the y.\n",
    "        # this backwards RNN conditions our latent variable.\n",
    "        backwardOutput, backwardsHidden = backwards(y[t+1], backwardsHidden)\n",
    "        backwardOutputs[t] = backwardOutput[0,0]\n",
    "    \n",
    "    # --------------------------\n",
    "    \n",
    "    # set up decoder variables\n",
    "    decoderInput = torch.tensor([[SOS_token]], device=device)\n",
    "    decoderHidden = encoderHidden\n",
    "    \n",
    "    enableTeacherForcing = False\n",
    "    if random.random() < teacherForcingRatio:\n",
    "        enableTeacherForcing = True\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we traverse through the decoder.\n",
    "    \n",
    "    in this part we can also feed the backwards rnn at y.\n",
    "    \"\"\"\n",
    "    \n",
    "    if enableTeacherForcing:\n",
    "        # teacher forcing: feeds the target as the next input.\n",
    "        for di in range(targetLength):\n",
    "            # compute the output of each decoder state\n",
    "            DecoderOut = decoder(decoderInput, decoderHidden, encoderOutputs)\n",
    "            (decoderOutput, decoderHidden, decoderAttention) = DecoderOut\n",
    "            \n",
    "            # calculate the loss\n",
    "            loss += loss_function(decoderOutput, y[di])\n",
    "            # feed this output to the next input\n",
    "            decoderInput = y[di]\n",
    "    else:\n",
    "        # no techer forcing: use the predicted output as the next input.\n",
    "        for di in range(targetLength):\n",
    "            # compute the output of each decoder state\n",
    "            DecoderOut = decoder(decoderInput, decoderHidden, encoderOutputs)\n",
    "            (decoderOutput, decoderHidden, decoderAttention) = DecoderOut\n",
    "            \n",
    "            toPV, toPI = decoderOutput.topk(1)\n",
    "            # detach from history as input\n",
    "            decoderInput = toPI.squeeze().detach()\n",
    "            # calculate the loss\n",
    "            loss += loss_function(decoderOutput, y[di])\n",
    "            # if we found `<EOS>` at this iteration, then break.\n",
    "            if decoderInput.item() == EOS_token:\n",
    "                break\n",
    "    \n",
    "    # possible because our loss_function uses gradient storing calculatioons\n",
    "    loss.backward()\n",
    "    \n",
    "    encoderOptimiser.step()\n",
    "    decoderOptimiser.step()\n",
    "    \n",
    "    return loss.item()/targetLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, iterations, printEvery=1000, plotEvery=100, learningRate=0.01):\n",
    "    # store statistics so we can use them to \n",
    "    # show progress.\n",
    "    start = time.time()\n",
    "    plotLosses = []\n",
    "    printLossTotal = 0\n",
    "    plotLossTotal = 0\n",
    "    \n",
    "    # setup optimisers\n",
    "    encoderOptimiser = optim.SGD(encoder.parameters(), lr=learningRate)\n",
    "    decoderOptimiser = optim.SGD(decoder.parameters(), lr=learningRate)\n",
    "    trainingPairs = [tensorsFromPair(random.choice(pairs)) for i in range(iterations)]\n",
    "    \n",
    "    for i in range(1, iterations + 1):\n",
    "        # set up variables needed for training.\n",
    "        trainingPair = trainingPairs[i-1]\n",
    "        x, y = trainingPair[0], trainingPair[1]\n",
    "        # calculate loss.\n",
    "        loss = train(x, y, encoder, decoder, encoderOptimiser, decoderOptimiser)\n",
    "        # increment our print and plot.\n",
    "        printLossTotal += loss\n",
    "        plotLossTotal += loss\n",
    "        \n",
    "        # print mechanism\n",
    "        if i % printEvery == 0:\n",
    "            printLossAvg = printLossTotal / printEvery\n",
    "            # reset the print loss.\n",
    "            printLossTotal = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, i / iterations),\n",
    "                                         i, i / iterations * 100, printLossAvg))\n",
    "        # plot mechanism\n",
    "        if i % plotEvery == 0:\n",
    "            plotLossAvg = plotLossTotal / plotEvery\n",
    "            plotLosses.append(plotLossAvg)\n",
    "            plotLossTotal = 0\n",
    "    \n",
    "    showPlot(plotLosses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
