{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo:\n",
    "- Add Xavier initialisation for all the weights of the networks\n",
    "- Look up on how to structure the auxillary function?\n",
    "- Update the training function to include all the parts of the model\n",
    "- What is KL Annealing???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T16:17:01.266650Z",
     "start_time": "2019-02-02T16:17:01.262547Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import bcolz\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:40:37.774123Z",
     "start_time": "2019-02-02T18:40:37.771296Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines..\n",
      "Read 135842 sentence pairs.\n",
      "Trimmed to 13465 sentence pairs.\n",
      "Counting words..\n",
      "Counted words:\n",
      "fra 5392\n",
      "eng 3552\n",
      "['vous etes depourvue d ambition .', 'you re unambitious .']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dataset processing functions\n",
    "\"\"\"\n",
    "\n",
    "# start_of_sentence and end_of_sentence token\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {\n",
    "            0: \"SOS\",\n",
    "            1: \"EOS\"\n",
    "        }\n",
    "        self.n_words = len(self.index2word.keys())\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def norm(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines..\")\n",
    "    \n",
    "    # read the file and split into lines\n",
    "    directory = \"../Datasets/Tutorials/seq2seq\"\n",
    "    filename = lang1 + \"-\" + lang2 + \".txt\"\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    lines = open(filepath, encoding='utf-8').\\\n",
    "        read().strip().split(\"\\n\")\n",
    "    \n",
    "    # split every line into pairs\n",
    "    # note that the language phrases are split by a tab.\n",
    "    pairs = [[norm(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # reverse the pairs\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "englishPrefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    "    )\n",
    "\n",
    "def filterPair(p):\n",
    "    check1 = len(p[0].split(' ')) < max_length\n",
    "    check2 = len(p[1].split(' ')) < max_length\n",
    "    check3 = p[1].startswith(englishPrefixes)\n",
    "    return check1 and check2 and check3\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def prepare(lang1, lang2, reverse=False):\n",
    "    inputLang, outputLang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs.\" % len(pairs))\n",
    "    \n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs.\" % len(pairs))\n",
    "    \n",
    "    print(\"Counting words..\")\n",
    "    \n",
    "    for pair in pairs:\n",
    "        inputLang.addSentence(pair[0])\n",
    "        outputLang.addSentence(pair[1])\n",
    "        \n",
    "    print(\"Counted words:\")\n",
    "    print(inputLang.name, inputLang.n_words)\n",
    "    print(outputLang.name, outputLang.n_words)\n",
    "    \n",
    "    return inputLang, outputLang, pairs\n",
    "\n",
    "inputLang, outputLang, pairs = prepare(\"eng\", \"fra\", True)\n",
    "print(random.choice(pairs))\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1,1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    inputTensor = tensorFromSentence(inputLang, pair[0])\n",
    "    targetTensor = tensorFromSentence(outputLang, pair[1])\n",
    "    return (inputTensor, targetTensor)\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading files.. Done.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Need a function to load the GloVe word embeddings.\n",
    "Based on this method:\n",
    "https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "\"\"\"\n",
    "glovePath = \"/media/data/Datasets/glove\"\n",
    "\n",
    "def loadGlove(glove_path, dim=50):\n",
    "    acceptedDimensions = [50, 100, 200, 300]\n",
    "    if dim not in acceptedDimensions:\n",
    "        print(\"You didn't choose a right dimension.\")\n",
    "        print(\"Try one of these:\", acceptedDimensions)\n",
    "        return None\n",
    "    pickleWordFile = f'{glove_path}/6B.'+str(dim)+'_words.pkl'\n",
    "    pickleIdFile   = f'{glove_path}/6B.'+str(dim)+'_idx.pkl'\n",
    "    pickleDatFile  = f'{glove_path}/glove.6B.'+str(dim)+'.dat'\n",
    "    pickleDataset  = f'{glove_path}/glove.6B.'+str(dim)+'d.txt'\n",
    "    \n",
    "    if os.path.isfile(pickleWordFile):\n",
    "        # check if we've made the outputs before\n",
    "        print(\"Preloading files..\", end=\" \")\n",
    "        vectors = bcolz.open(pickleDatFile)[:]\n",
    "        words = pickle.load(open(pickleWordFile, 'rb'))\n",
    "        word2idx = pickle.load(open(pickleIdFile, 'rb'))\n",
    "        glove = {w: vectors[word2idx[w]] for w in words}\n",
    "        print(\"Done.\")\n",
    "        return glove\n",
    "    else:\n",
    "        print(\"Creating new files..\", end=\" \")\n",
    "        words    = []\n",
    "        idx      = 0\n",
    "        word2idx = {}\n",
    "        vectors = bcolz.carray(np.zeros(1), rootdir=pickleDatFile, mode='w')\n",
    "\n",
    "        with open(pickleDataset, 'rb') as f:\n",
    "            for l in f:\n",
    "                line = l.decode().split()\n",
    "                word = line[0]\n",
    "                words.append(word)\n",
    "                word2idx[word] = idx\n",
    "                idx += 1\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "                vectors.append(vect)\n",
    "\n",
    "        vectors = bcolz.carray(vectors[1:].reshape((400000, dim)),\n",
    "                               rootdir=pickleDatFile, mode='w')\n",
    "        vectors.flush()\n",
    "        # save the outputs\n",
    "        pickle.dump(words, open(pickleWordFile, 'wb'))\n",
    "        pickle.dump(word2idx, open(pickleIdFile, 'wb'))\n",
    "        # create the dataset\n",
    "        glove = {w: vectors[word2idx[w]] for w in words}\n",
    "        print(\"Done.\")\n",
    "        return glove\n",
    "\n",
    "glove = loadGlove(glovePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10900477,  0.39005339,  0.41426117,  0.73226437, -0.39847988,\n",
       "        -1.1908525 ,  0.86290411, -0.26205337,  0.24860828, -0.2872339 ,\n",
       "         0.06183626, -0.29048577,  0.0494523 ,  0.08885237, -0.01168055,\n",
       "         0.09605561,  0.53305146, -0.73836863,  0.26562488, -0.18981482,\n",
       "        -0.29566844, -0.29841539,  0.69372019,  0.15864261,  1.00231501,\n",
       "        -0.62599312,  0.08614995, -0.12092209, -0.48454993,  1.04824008,\n",
       "        -0.72058916,  0.58283893,  0.02703291, -0.20486233, -0.24587728,\n",
       "        -1.06012552, -0.22821207,  0.19279423,  0.53912303, -0.18990544,\n",
       "        -0.21820726,  0.41613238,  0.48022712, -0.77217027, -1.05255587,\n",
       "        -0.02258505, -0.30678506, -0.57693466, -0.06212577, -0.73361708]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createWeightMatrix(targetVocab, glove):\n",
    "    \"\"\"\n",
    "    For each word in the dataset's vocabulary,\n",
    "    check if it's also in the @glove vocab.\n",
    "    If it does, then we load the pre-trained word vector.\n",
    "    Otherwise we use some random vector.\n",
    "    \"\"\"\n",
    "    length  = len(targetVocab)\n",
    "    dim = glove['fail'].shape\n",
    "    wMatrix = np.zeros((length, dim[0]))\n",
    "    wordsFound = 0\n",
    "    for i, word in enumerate(targetVocab):\n",
    "        try:\n",
    "            wMatrix[i] = glove[word]\n",
    "            wordsFound += 1\n",
    "        except KeyError:\n",
    "            wMatrix[i] = np.random.normal(scale=0.6, size=dim)\n",
    "    return wMatrix\n",
    "\n",
    "createWeightMatrix(['noodb'], glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEmbeddingLayer(wMatrix, trainable=True):\n",
    "    \"\"\"\n",
    "    This function is to be called when initialising\n",
    "    a new neural network class.\n",
    "    \"\"\"\n",
    "    num, dim = wMatrix.size()\n",
    "    layer = nn.Embedding(num, dim)\n",
    "    layer.load_state_dict({'weight': wMatrix})\n",
    "    layer.weight.requires_grad = True if trainable else False\n",
    "    return layer, num, dim\n",
    "\n",
    "class ToyNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers):\n",
    "        super(self).__init__()\n",
    "        self.embedding, num_embeddings, embedding_dim = createEmbeddingLayer(weights_matrix, True)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, inp, hidden):\n",
    "        return self.gru(self.embedding(inp), hidden)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "#         torch.nn.init.xavier_uniform_(tensor, gain=1)\n",
    "        return Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T15:49:50.995429Z",
     "start_time": "2019-02-02T15:49:50.985772Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This'll be a bi-directional GRU.\n",
    "    Utilises equation (1) in the paper.\n",
    "    \n",
    "    The hidden size is 512 as per the paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddingMatrix, inputSize, hiddenSize=512):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "        # this embedding is a simple lookup table that stores the embeddings of a \n",
    "        # fixed dictionary and size.\n",
    "        \n",
    "        # This module is often used to store word embeddings and retrieve them\n",
    "        # using indices. \n",
    "        # The input to the module is a list of indices, and \n",
    "        # the output is the corresponding word embeddings.\n",
    "#         self.embedding = nn.Embedding(inputSize, hiddenSize)\n",
    "        self.embedding, numEmbeddings, embeddingDim = createEmbeddingLayer(embeddingMatrix)\n",
    "        self.embedding.to(device)\n",
    "        self.gru = nn.GRU(inputSize, embeddingDim, hiddenSize, bidirectional=True)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # load the input into the embedding before doing GRU computation.\n",
    "        output = self.embedding(x).view(1,1,-1)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1, self.hiddenSize, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backwards(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Backwards, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO: Add layer normalisation?\n",
    "    https://arxiv.org/abs/1607.06450\n",
    "    \n",
    "    We also set the hidden state size to 512.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, outputSize, hiddenSize=512, maxLength = max_length):\n",
    "        \"\"\"\n",
    "        # dropout omitted\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.maxLength = maxLength\n",
    "        \n",
    "        # self.attention is our tiny neural network that takes \n",
    "        # in the hidden weights and the previous hidden weights.\n",
    "        self.attention = nn.Linear(self.hiddenSize * 2, self.maxLength, \n",
    "                                   device=device)\n",
    "        self.attentionCombined = nn.Linear(self.hiddenSize * 2, self.hiddenSize, \n",
    "                                           device=device)\n",
    "        torch.nn.init.xavier_uniform_(self.attention)\n",
    "        torch.nn.init.xavier_uniform_(self.attentionCombined)\n",
    "    \n",
    "    def forward(self, prevHidden, encoderOutputs):\n",
    "\n",
    "        # concatenate hidden layer inputs together.\n",
    "        attentionInputs  = prevHidden\n",
    "        attentionWeights = F.softmax(self.attention(attentionInputs), dim=1)\n",
    "        \n",
    "        # batch matrix multiplication\n",
    "        attentionApplied = torch.bmm(attentionWeights.unsqueeze(0),\n",
    "                                    encoderOutputs.unsqueeze(0))\n",
    "        # reshape to produce context vector.\n",
    "        context = self.attentionCombined(context).unsqueeze(0)\n",
    "        context = F.relu(context)\n",
    "        return context, attentionWeights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1, self.hiddenSize, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO: Add layer normalisation?\n",
    "    https://arxiv.org/abs/1607.06450\n",
    "    \n",
    "    We also set the hidden state size to 512.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, outputSize, hiddenSize=512, maxLength = max_length):\n",
    "        \"\"\"\n",
    "        # dropout omitted\n",
    "        \"\"\"\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.outputSize = outputSize\n",
    "        self.maxLength = maxLength\n",
    "        \n",
    "        self.embedding, numEmbeddings, embeddingDim = createEmbeddingLayer(embeddingMatrix)\n",
    "        self.embedding.to(device)\n",
    "        \n",
    "        self.gru = nn.GRU(self.hiddenSize, self.hiddenSize)\n",
    "        self.out = nn.Linear(self.hiddenSize, self.outputSize, device=device)\n",
    "        torch.nn.init.xavier_uniform_(self.out)\n",
    "    \n",
    "    def forward(self, previousY, previousHidden, context, z):\n",
    "        # get the embedding value of our previous Y\n",
    "        embedded = self.embedding(previousY).view(1,1,-1)\n",
    "        # concatenate hidden layer inputs together.\n",
    "        inputs =  torch.cat((embedded[0], context, z), 1)\n",
    "        # do a forward GRU\n",
    "        output, hidden = self.gru(context, previousHidden)\n",
    "        # softmax the output\n",
    "        output = self.out(torch.cat((output[0], context), 1))\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1, self.hiddenSize, device=device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate a set of attention weights.\n",
    "- Dot product the attention weights with the encoder output vectors.\n",
    "- This result should contain information about that specific part of the input sequence, which helps the decoder choose the right words. We'll store this into a variable called attentionApplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T19:33:13.673408Z",
     "start_time": "2019-02-02T19:33:13.666250Z"
    }
   },
   "outputs": [],
   "source": [
    "class Inference(nn.Module):\n",
    "    \"\"\"\n",
    "    Note that the inference and prior networks\n",
    "    are a simple 1 layer feed forward neural network.\n",
    "    Therefore the size of the weights are entirely based on the size\n",
    "    of the input and outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_size, class_size, latent_size=400):\n",
    "        super(Inference, self).__init__()\n",
    "        \n",
    "        self.feature_size = feature_size\n",
    "        self.class_size = class_size\n",
    "\n",
    "        # encode\n",
    "        self.fc1  = nn.Linear(feature_size + class_size, 400)\n",
    "        self.mean = nn.Linear(400, latent_size)\n",
    "        self.var = nn.Linear(400, latent_size)\n",
    "\n",
    "        # decode\n",
    "        self.fc3 = nn.Linear(latent_size + class_size, 400)\n",
    "        self.fc4 = nn.Linear(400, feature_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x_forward, c, h_backward): # Q(z|x, c)\n",
    "        '''\n",
    "        x: (bs, feature_size)\n",
    "        c: (bs, class_size)\n",
    "        '''\n",
    "        inputs = torch.cat([x_forward, c, h_backward], 1) # (bs, feature_size+class_size)\n",
    "        h1 = self.relu(self.fc1(inputs))\n",
    "        z_mu = self.mean(h1)\n",
    "        z_var = self.var(h1)\n",
    "        return z_mu, z_var\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        # samples your mu, logvar to get z.\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std) + mu\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z, c): # P(x|z, c)\n",
    "        '''\n",
    "        z: (bs, latent_size)\n",
    "        c: (bs, class_size)\n",
    "        '''\n",
    "        inputs = torch.cat([z, c], 1) # (bs, latent_size+class_size)\n",
    "        h3 = self.relu(self.fc3(inputs))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z, c), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T19:46:56.627386Z",
     "start_time": "2019-02-02T19:46:56.615695Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Prior(nn.Module):\n",
    "    def __init__(self, feature_size, class_size):\n",
    "        super(Prior, self).__init__()\n",
    "        \n",
    "        self.feature_size = feature_size\n",
    "        self.class_size = class_size\n",
    "\n",
    "        # encode\n",
    "        self.fc1  = nn.Linear(feature_size + class_size, 400)\n",
    "        self.mean = nn.Linear(400, latent_size)\n",
    "        self.var = nn.Linear(400, latent_size)\n",
    "\n",
    "        # decode\n",
    "        self.fc3 = nn.Linear(latent_size + class_size, 400)\n",
    "        self.fc4 = nn.Linear(400, feature_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, h, c): # Q(z|x, c)\n",
    "        '''\n",
    "        x: (bs, feature_size)\n",
    "        c: (bs, class_size)\n",
    "        '''\n",
    "        inputs = torch.cat([h, c], 1) # (bs, feature_size+class_size)\n",
    "        h1 = self.relu(self.fc1(inputs))\n",
    "        z_mu = self.mean(h1)\n",
    "        z_var = self.var(h1)\n",
    "        return z_mu, z_var\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        # samples your mu, logvar to get z.\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std) + mu\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z, c): # P(x|z, c)\n",
    "        '''\n",
    "        z: (bs, latent_size)\n",
    "        c: (bs, class_size)\n",
    "        '''\n",
    "        inputs = torch.cat([z, c], 1) # (bs, latent_size+class_size)\n",
    "        h3 = self.relu(self.fc3(inputs))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z, c), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Auxillary(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        self.fc1  = nn.Linear(latent_size, 400)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        The motive here is to produce an auxillary loss for our \n",
    "        training objective.\n",
    "        \n",
    "        We do this by Sequential Bag of Words (SBOW) as the\n",
    "        auxillary objective for the proposed VAD model. \n",
    "        \n",
    "        We want to predict the bag of succeeding words\n",
    "        in the response using the latent variable z at each\n",
    "        time step.\n",
    "        \"\"\"\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(y_predicted, y, z_inference, z_prior):\n",
    "    LL = criterion(y_predicted, y)\n",
    "    KL = F.kl_div(z_inference, z_prior)\n",
    "    return LL - KL\n",
    "\n",
    "def KLD(meanA, logvarA, meanB, logvarB):\n",
    "    \"\"\"\n",
    "    Not used as PyTorch has its own KL-Divergence function\n",
    "    which is most likely to be better optimised than whatever i have.\n",
    "    \"\"\"\n",
    "    # Univariate\n",
    "    # https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians\n",
    "    # return logvarB - logvarA + (2*logvarA + (meanA - meanB) ** 2)/(4 * logvarB) - 0.5\n",
    "    # Multivariate\n",
    "    # https://stats.stackexchange.com/questions/60680/kl-divergence-between-two-multivariate-gaussians\n",
    "    d = meanA.shape\n",
    "    divergence = logvarB - logvarA - torch.trace(torch.matmul(logvarB.inverse(), logvarA))\n",
    "    cov = meanB - meanA\n",
    "    divergence += torch.matmul(torch.matmul(cov.t(), logvarB.inverse()), cov)\n",
    "    return 0.5 * divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper Functions\n",
    "\"\"\"\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainVAD(x,y,\n",
    "             encoder,\n",
    "             attention,\n",
    "             backwards,\n",
    "             inference,\n",
    "             prior,\n",
    "             decoder,\n",
    "             encoderOpt,\n",
    "             attentionOpt,\n",
    "             backwardsOpt,\n",
    "             inferenceOpt,\n",
    "             priorOpt,\n",
    "             decoderOpt,\n",
    "             criterion = nn.NLLLoss(),\n",
    "             maxLength = max_length\n",
    "            ):\n",
    "    \n",
    "    # initialise gradients\n",
    "    encoderOpt.zero_grad()\n",
    "    attentionOpt.zero_grad()\n",
    "    backwardsOpt.zero_grad()\n",
    "    inferenceOpt.zero_grad()\n",
    "    priorOpt.zero_grad()\n",
    "    decoderOpt.zero_grad()\n",
    "    \n",
    "    # initalise input and target lengths\n",
    "    inputLength = x.size(0)\n",
    "    targetLength = y.size(0)\n",
    "    \n",
    "    # set default loss\n",
    "    loss = 0\n",
    "    \n",
    "    # set up encoder computation\n",
    "    encoderOutputs  = torch.zeros(maxLength, encoder.hiddenSize, device=device)\n",
    "    backwardOutputs = torch.zeros(maxLength, encoder.hiddenSize, device=device)\n",
    "    decoderHiddens  = torch.zeros(maxLength, decoder.hiddenSize, device=device)\n",
    "    \n",
    "    # set up encoder outputs\n",
    "    for ei in range(inputLength):\n",
    "        encoderOutput, encoderHidden = encoder(x[ei], encoderHidden)\n",
    "        encoderOutputs[ei] = encoderOutput[0,0]\n",
    "    \n",
    "    # set up backwards RNN\n",
    "    for t in range(targetLength-1, 0, 1):\n",
    "        # here we can also build the backwards RNN that takes in the y.\n",
    "        # this backwards RNN conditions our latent variable.\n",
    "        backwardOutput, backwardsHidden = backwards(y[t+1], backwardsHidden)\n",
    "        # get the values of our backwards network.\n",
    "        backwardOutputs[t] = backwardOutput[0,0]\n",
    "        \n",
    "    # set up the decoder computation\n",
    "    decoderInput = torch.tensor([[SOS_token]], device=device)\n",
    "    decoderHidden = encoderHidden\n",
    "    \n",
    "    \n",
    "    for t in range(targetLength):\n",
    "        # get the context vector c\n",
    "        c, _ = attention(decoderH, encoderOutputs[t])\n",
    "        # compute the inference layer\n",
    "        z_infer, infMean, infLogvar = inference(decoderOutput, c, backwardOutputs[t])\n",
    "        # compute the prior layer\n",
    "        z_prior, priMean, priLogvar = prior(decoderOutput, c)\n",
    "        # compute the output of each decoder state\n",
    "        DecoderOut = decoder(decoderInput, c, z_infer, decoderHidden)\n",
    "        decoderOutput, decoderHidden = DecoderOut\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss += loss_function(decoderOutput, y[t], z_infer, z_prior)\n",
    "        # feed this output to the next input\n",
    "        decoderInput = y[t]\n",
    "    \n",
    "    # possible because our loss_function uses gradient storing calculations\n",
    "    loss.backward()\n",
    "    \n",
    "    encoderOpt.step()\n",
    "    attentionOpt.step()\n",
    "    backwardsOpt.step()\n",
    "    inferenceOpt.step()\n",
    "    priorOpt.step()\n",
    "    decoderOpt.step()\n",
    "    \n",
    "    return loss.item()/targetLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIteration(encoder,\n",
    "                attention,\n",
    "                backwards,\n",
    "                inference,\n",
    "                prior,\n",
    "                decoder,\n",
    "                iterations,\n",
    "                criterion = nn.NLLLoss(),\n",
    "                learningRate = 0.0001,\n",
    "                printEvery = 1000,\n",
    "                plotEvery = 100):\n",
    "    \n",
    "    start = time.time()\n",
    "    plotLosses = []\n",
    "    printLossTotal = 0\n",
    "    plotLossTotal = 0\n",
    "    \n",
    "    encoderOpt   = optim.Adam(encoder.parameters(),   lr=learningRate)\n",
    "    attentionOpt = optim.Adam(attention.parameters(), lr=learningRate)\n",
    "    backwardsOpt = optim.Adam(backwards.parameters(), lr=learningRate)\n",
    "    inferenceOpt = optim.Adam(inference.parameters(), lr=learningRate)\n",
    "    priorOpt     = optim.Adam(prior.parameters(),     lr=learningRate)\n",
    "    decoderOpt   = optim.Adam(decoder.parameters(),   lr=learningRate)\n",
    "    \n",
    "    trainingPairs = [tensorsFromPair(random.choice(pairs)) for i in range(iterations)]\n",
    "    \n",
    "    for i in range(1, iterations + 1):\n",
    "        # set up variables needed for training.\n",
    "        trainingPair = trainingPairs[i-1]\n",
    "        x, y = trainingPair[0], trainingPair[1]\n",
    "        # calculate loss.\n",
    "        loss = trainVAD(x, y, \n",
    "             encoder,\n",
    "             attention,\n",
    "             backwards,\n",
    "             inference,\n",
    "             prior,\n",
    "             decoder,\n",
    "             encoderOpt,\n",
    "             attentionOpt,\n",
    "             backwardsOpt,\n",
    "             inferenceOpt,\n",
    "             priorOpt,\n",
    "             decoderOpt,\n",
    "             criterion\n",
    "            )\n",
    "        # increment our print and plot.\n",
    "        printLossTotal += loss\n",
    "        plotLossTotal += loss\n",
    "        \n",
    "        # print mechanism\n",
    "        if i % printEvery == 0:\n",
    "            printLossAvg = printLossTotal / printEvery\n",
    "            # reset the print loss.\n",
    "            printLossTotal = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, i / iterations),\n",
    "                                         i, i / iterations * 100, printLossAvg))\n",
    "        # plot mechanism\n",
    "        if i % plotEvery == 0:\n",
    "            plotLossAvg = plotLossTotal / plotEvery\n",
    "            plotLosses.append(plotLossAvg)\n",
    "            plotLossTotal = 0\n",
    "            \n",
    "    showPlot(plotLosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'inputSize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-aba2d63142f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodelEncoder\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputLang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodelAttention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodelBackwards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBackwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodelInference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodelPrior\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mPrior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'inputSize'"
     ]
    }
   ],
   "source": [
    "modelEncoder   = Encoder(inputLang.n_words).to(device)\n",
    "modelAttention = Attention().to(device)\n",
    "modelBackwards = Backwards().to(device)\n",
    "modelInference = Inference().to(device)\n",
    "modelPrior     = Prior().to(device)\n",
    "modelDecoder   = Decoder(outputLang.n_words, dropout=0.1).to(device)\n",
    "trainIters(encoder,\n",
    "           attention,\n",
    "           backwards,\n",
    "           inference,\n",
    "           prior,\n",
    "           decoder,\n",
    "           iterations,\n",
    "           75000, \n",
    "           printEvery=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
