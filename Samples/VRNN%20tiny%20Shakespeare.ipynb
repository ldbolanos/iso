{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Recurrent Network (VRNN)\n",
    "\n",
    "Implementation based on Chung's *A Recurrent Latent Variable Model for Sequential Data* [arXiv:1506.02216v6].\n",
    "https://lirnli.wordpress.com/2017/09/27/variational-recurrent-neural-network-vrnn-with-pytorch/\n",
    "\n",
    "### 1. Network design\n",
    "\n",
    "There are three types of layers: input (x), hidden(h) and latent(z). We can compare VRNN sided by side with RNN to see how it works in generation phase.\n",
    "\n",
    "- RNN: $h_o + x_o -> h_1 + x_1 -> h_2 + x_2 -> ...$\n",
    "- VRNN: with $ h_o \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      h_o -> z_1 \\\\\n",
    "      z_1 + h_o -> x_1\\\\\n",
    "      z_1 + x_1 + h_o -> h_1 \\\\\n",
    "\\end{array} \n",
    "\\right .$ \n",
    "with $ h_1 \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      h_1 -> z_2 \\\\\n",
    "      z_2 + h_1 -> x_2\\\\\n",
    "      z_2 + x_2 + h_1 -> h_2 \\\\\n",
    "\\end{array} \n",
    "\\right .$\n",
    "\n",
    "It is clearer to see how it works in the code blocks below. This loop is used to generate new text when the network is properly trained. x is wanted output, h is deterministic hidden state, and z is latent state (stochastic hidden state). Both h and z are changing with repect to time.\n",
    "\n",
    "### 2. Training\n",
    "\n",
    "The VRNN above contains three components, a latent layer genreator $h_o -> z_1$, a decoder net to get $x_1$, and a recurrent net to get $h_1$ for the next cycle.\n",
    "\n",
    "The training objective is to make sure $x_0$ is realistic. To do that, an encoder layer is added to transform $x_1 + h_0 -> z_1$. Then the decoder should transform $z_1 + h_o -> x_1$ correctly. This implies a cross-entropy loss in the \"tiny shakespear\" or MSE in image reconstruction.\n",
    "\n",
    "Another loose end is  $h_o -> z_1$. Statistically, $x_1 + h_0 -> z_1$ should be the same as $h_o -> z_1$, if $x_1$ is sampled randomly. This constraint is formularize as a KL divergence between the two.\n",
    "\n",
    ">#### KL Divergence of Multivariate Normal Distribution\n",
    ">![](https://wikimedia.org/api/rest_v1/media/math/render/svg/8dad333d8c5fc46358036ced5ab8e5d22bae708c)\n",
    "\n",
    "Now putting everything together for one training cycle.\n",
    "\n",
    "$\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      h_o -> z_{1,prior} \\\\\n",
    "      x_1 + h_o -> z_{1,infer}\\\\\n",
    "      z_1 <- sampling N(z_{1,infer})\\\\\n",
    "      z_1 + h_o -> x_{1,reconstruct}\\\\\n",
    "      z_1 + x_1 + h_o -> h_1 \\\\\n",
    "\\end{array} \n",
    "\\right . $\n",
    "=>\n",
    "$\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      loss\\_latent = DL(z_{1,infer} | z_{1,prior}) \\\\\n",
    "      loss\\_reconstruct = x_1 - x_{1,reconstruct} \\\\\n",
    "\\end{array} \n",
    "\\right .\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-753b97dedfdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-753b97dedfdc>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(self, hidden, temperature, n)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mx_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-753b97dedfdc>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, hidden, temperature)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# 3. x + z => h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mhidden_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class VRNNCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VRNNCell,self).__init__()\n",
    "        self.phi_x = nn.Sequential(nn.Embedding(128,64), nn.Linear(64,64), nn.ELU())\n",
    "        self.encoder = nn.Linear(128,64*2) # output hyperparameters\n",
    "        self.phi_z = nn.Sequential(nn.Linear(64,64), nn.ELU())\n",
    "        self.decoder = nn.Linear(128,128) # logits\n",
    "        self.prior = nn.Linear(64,64*2) # output hyperparameters\n",
    "        self.rnn = nn.GRUCell(128,64)\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.phi_x(x)\n",
    "        # 1. h => z\n",
    "        z_prior = self.prior(hidden)\n",
    "        # 2. x + h => z\n",
    "        z_infer = self.encoder(torch.cat([x,hidden], dim=1))\n",
    "        # sampling\n",
    "        z = Variable(torch.randn(x.size(0),64))*z_infer[:,64:].exp()+z_infer[:,:64]\n",
    "        z = self.phi_z(z)\n",
    "        # 3. h + z => x\n",
    "        x_out = self.decoder(torch.cat([hidden, z], dim=1))\n",
    "        # 4. x + z => h\n",
    "        hidden_next = self.rnn(torch.cat([x,z], dim=1),hidden)\n",
    "        return x_out, hidden_next, z_prior, z_infer\n",
    "    def calculate_loss(self, x, hidden):\n",
    "        x_out, hidden_next, z_prior, z_infer = self.forward(x, hidden)\n",
    "        # 1. logistic regression loss\n",
    "        loss1 = nn.functional.cross_entropy(x_out, x) \n",
    "        # 2. KL Divergence between Multivariate Gaussian\n",
    "        mu_infer, log_sigma_infer = z_infer[:,:64], z_infer[:,64:]\n",
    "        mu_prior, log_sigma_prior = z_prior[:,:64], z_prior[:,64:]\n",
    "        loss2 = (2*(log_sigma_infer-log_sigma_prior)).exp() \\\n",
    "                + ((mu_infer-mu_prior)/log_sigma_prior.exp())**2 \\\n",
    "                - 2*(log_sigma_infer-log_sigma_prior) - 1\n",
    "        loss2 = 0.5*loss2.sum(dim=1).mean()\n",
    "        return loss1, loss2, hidden_next\n",
    "    def generate(self, hidden=None, temperature=None):\n",
    "        if hidden is None:\n",
    "            hidden=Variable(torch.zeros(1,64))\n",
    "        if temperature is None:\n",
    "            temperature = 0.8\n",
    "        # 1. h => z\n",
    "        z_prior = self.prior(hidden)\n",
    "        # sampling\n",
    "        z = Variable(torch.randn(z_prior.size(0),64))*z_prior[:,64:].exp()+z_prior[:,:64]\n",
    "        z = self.phi_z(z)\n",
    "        # 2. h + z => x\n",
    "        x_out = self.decoder(torch.cat([hidden, z], dim=1))\n",
    "        # sampling\n",
    "        x_sample = x = x_out.div(temperature).exp().multinomial(1).squeeze()\n",
    "        x = self.phi_x(x)\n",
    "        # 3. x + z => h\n",
    "        hidden_next = self.rnn(torch.cat([x,z], dim=1),hidden)\n",
    "        return x_sample, hidden_next\n",
    "    def generate_text(self, hidden=None,temperature=None, n=100):\n",
    "        res = []\n",
    "        hidden = None\n",
    "        for _ in range(n):\n",
    "            x_sample, hidden = self.generate(hidden,temperature)\n",
    "            res.append(chr(x_sample.data[0]))\n",
    "        return \"\".join(res)\n",
    "        \n",
    "# Test\n",
    "net = VRNNCell()\n",
    "x = Variable(torch.LongTensor([12,13,14]))\n",
    "hidden = Variable(torch.rand(3,64))\n",
    "output, hidden_next, z_infer, z_prior = net(x, hidden)\n",
    "loss1, loss2, _ = net.calculate_loss(x, hidden)\n",
    "loss1, loss2\n",
    "hidden = Variable(torch.zeros(1,64))\n",
    "net.generate_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Download tiny shakspear text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves.urllib import request\n",
    "url = \"https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt\"\n",
    "text = request.urlopen(url).read().decode()\n",
    "\n",
    "print('-----SAMPLE----\\n')\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A convinient function to sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch_generator(seq_size=300, batch_size=64):\n",
    "    cap = len(text) - seq_size*batch_size\n",
    "    while True:\n",
    "        idx = np.random.randint(0, cap, batch_size)\n",
    "        res = []\n",
    "        for _ in range(seq_size):\n",
    "            batch = torch.LongTensor([ord(text[i]) for i in idx])\n",
    "            res.append(batch)\n",
    "            idx += 1\n",
    "        yield res\n",
    "\n",
    "g = batch_generator()\n",
    "batch = next(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = VRNNCell()\n",
    "max_epoch = 2000\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "g = batch_generator()\n",
    "\n",
    "hidden = Variable(torch.zeros(64,64)) #batch_size x hidden_size\n",
    "for epoch in range(max_epoch):\n",
    "    batch = next(g)\n",
    "    loss_seq = 0\n",
    "    loss1_seq, loss2_seq = 0, 0\n",
    "    optimizer.zero_grad()\n",
    "    for x in batch:\n",
    "        loss1, loss2, hidden = net.calculate_loss(Variable(x),hidden)\n",
    "        loss1_seq += loss1.data[0]\n",
    "        loss2_seq += loss2.data[0]\n",
    "        loss_seq = loss_seq + loss1+loss2\n",
    "    loss_seq.backward()\n",
    "    optimizer.step()\n",
    "    hidden.detach_()\n",
    "    if epoch%100==0:\n",
    "        print('>> epoch {}, loss {:12.4f}, decoder loss {:12.4f}, latent loss {:12.4f}'.format(epoch, loss_seq.data[0], loss1_seq, loss2_seq))\n",
    "        print(net.generate_text())\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = net.generate_text(n=1000, temperature=1)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "- Denifinitely train longer to get better results. \n",
    "- Keep in mind the rnn kernel only has 1 layer, with 64 neurons.\n",
    "- Seems no need to tune temperature here. temperature = 0.8 generates a lot of obscure spelling. temperature = 1 works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
