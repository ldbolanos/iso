{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import gzip\n",
    "import bcolz\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickleFile = '../Datasets/Reviews/dataset.pkl'\n",
    "start = time.clock()\n",
    "dataset = pickle.load( open( pickleFile, \"rb\" ))\n",
    "duration = time.clock() - start\n",
    "print(duration, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Glove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading files.. Done.\n"
     ]
    }
   ],
   "source": [
    "# def loadGlove(glove_path, dim=50):\n",
    "#     acceptedDimensions = [50, 100, 200, 300]\n",
    "#     if dim not in acceptedDimensions:\n",
    "#         print(\"You didn't choose a right dimension.\")\n",
    "#         print(\"Try one of these:\", acceptedDimensions)\n",
    "#         return None\n",
    "#     pickleWordFile = f'{glove_path}/6B.'+str(dim)+'_words.pkl'\n",
    "    \n",
    "#     if os.path.isfile(pickleWordFile):\n",
    "#         # check if we've made the outputs before\n",
    "#         words = pickle.load(open(pickleWordFile, 'rb'))\n",
    "#         return words\n",
    "#     else:\n",
    "#         print(\"Doesn't work\")\n",
    "#         return -1\n",
    "    \n",
    "def loadGlove(glove_path, dim=50):\n",
    "    acceptedDimensions = [50, 100, 200, 300]\n",
    "    if dim not in acceptedDimensions:\n",
    "        print(\"You didn't choose a right dimension.\")\n",
    "        print(\"Try one of these:\", acceptedDimensions)\n",
    "        return None\n",
    "    pickleWordFile = f'{glove_path}/6B.'+str(dim)+'_words.pkl'\n",
    "    pickleIdFile   = f'{glove_path}/6B.'+str(dim)+'_idx.pkl'\n",
    "    pickleDatFile  = f'{glove_path}/glove.6B.'+str(dim)+'.dat'\n",
    "    pickleDataset  = f'{glove_path}/glove.6B.'+str(dim)+'d.txt'\n",
    "    \n",
    "    if os.path.isfile(pickleWordFile):\n",
    "        # check if we've made the outputs before\n",
    "        print(\"Preloading files..\", end=\" \")\n",
    "        vectors = bcolz.open(pickleDatFile)[:]\n",
    "        words = pickle.load(open(pickleWordFile, 'rb'))\n",
    "        word2idx = pickle.load(open(pickleIdFile, 'rb'))\n",
    "        glove = {w: vectors[word2idx[w]] for w in words}\n",
    "        print(\"Done.\")\n",
    "        return glove\n",
    "    else:\n",
    "        print(\"Doesn't work.\", end=\" \")\n",
    "\n",
    "\n",
    "gloveDimension = 50\n",
    "glovePath = \"/media/data/Datasets/glove\"\n",
    "glove = loadGlove(glovePath, dim=gloveDimension)\n",
    "gloveWords = glove.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "print(len(gloveWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(paragraph):\n",
    "    # split paragraph by full stops\n",
    "    paragraph = paragraph.lower()\n",
    "    paragraph = re.sub(\"([,!?()-+&Â£$.%*'])\", r' \\1 ', paragraph)\n",
    "    paragraph = re.sub('\\s{2,}', ' ', paragraph)\n",
    "    paragraph = paragraph.split(\" \")\n",
    "    # remove empty string\n",
    "    return paragraph\n",
    "\n",
    "def padSentence(words, maxLength, padString=\"<pad>\"):\n",
    "    if len(words) > maxLength:\n",
    "        return words[:maxLength]\n",
    "    else:\n",
    "        return words + [padString for i in range(maxLength - len(words))]\n",
    "    \n",
    "def discretise(value, word):\n",
    "    return word + \"_\" + str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle(itemID, dataset=dataset, \n",
    "           wordbase=gloveWords, \n",
    "           minWords=6, \n",
    "           minEntries=5, \n",
    "           maxSummaryLength=10, \n",
    "           maxReviewLength=60):\n",
    "    \"\"\"\n",
    "    Filters words out based on whether they're in the GloVe dataset or not.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    legit = 0\n",
    "    printDebug = False\n",
    "\n",
    "    reviews = []\n",
    "#     for itemID in datasetKeys:\n",
    "#         print(\"YEAH\",itemID)\n",
    "    # check if there are more than 5 reviews.\n",
    "    if len(dataset[itemID]) > minEntries:\n",
    "        review = []\n",
    "        for i in range(len(dataset[itemID])):\n",
    "            # initialise variables\n",
    "            entry = dataset[itemID][i]\n",
    "            reviewerID = entry['reviewerID']\n",
    "\n",
    "            \"\"\"\n",
    "            Review Text Processing\n",
    "            \"\"\"\n",
    "            # preprocess review\n",
    "            words = preprocess(entry['reviewText'])\n",
    "            words = [w for w in words if w in wordbase]\n",
    "            words = words[:maxReviewLength]\n",
    "            # preprocess summary\n",
    "            summary = preprocess(entry['summary'])\n",
    "            summary = [w for w in summary if w in wordbase]\n",
    "            summary = summary[:maxSummaryLength]\n",
    "\n",
    "            # visualise\n",
    "            if printDebug:\n",
    "                print(dataset[itemID][i])\n",
    "                print(numWords, words, summary, \"\\n\")\n",
    "\n",
    "            # if theres more than 6 tokens, keep the first 50 tokens.\n",
    "            if len(words) > minWords:\n",
    "                # add padding tokens here\n",
    "                summary  = padSentence(summary, maxSummaryLength)\n",
    "                words    = padSentence(words,   maxReviewLength)\n",
    "                # also need to process rating, polarity and item_id\n",
    "                rating   = [discretise(entry['overall'], \"rating\")]\n",
    "                # process polarity\n",
    "                polarity = np.round(np.tanh(entry['helpful'][0]-entry['helpful'][1]),1)\n",
    "                polarity = [discretise(polarity, \"polarity\")]\n",
    "                reviewID = [itemID]\n",
    "                entry = reviewID + summary + rating + polarity + words\n",
    "                review.append(entry)\n",
    "\n",
    "        # check if theres less than 5 filtered reviews\n",
    "        if len(review) > minEntries:\n",
    "            reviews.append(review)\n",
    "#                 break          \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetKeys = list(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(datasetKeys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imap_unordered_bar(func, args, n_processes = 8):\n",
    "    p = Pool(n_processes)\n",
    "    res_list = []\n",
    "    with tqdm(total = len(args)) as pbar:\n",
    "        for i, res in enumerate(p.imap_unordered(func, args)):\n",
    "            pbar.update()\n",
    "            res_list.append(res)\n",
    "    pbar.close()\n",
    "    p.close()\n",
    "    p.join()\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = imap_unordered_bar(handle,datasetKeys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Filtered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickleFile = '../Datasets/Reviews/dataset_filtered.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset to a pickle file.\n",
    "output = open(pickleFile, 'wb')\n",
    "pickle.dump(reviews, output)\n",
    "output.close()\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Reviews 63001\n"
     ]
    }
   ],
   "source": [
    "# Here we'll load the pickled file again so we can clear the memory.\n",
    "reviews = pickle.load(open(pickleFile, 'rb'))\n",
    "print(\"Loaded Reviews\", len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ID's of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the throughput of the model, we should reduce the embedding size. Here we'll look at all the words and keep track ones that exist. We'll make a reduced word2id based on this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15751"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we reduce the size of the dataset so we can debug our model.\n",
    "reviews = reviews[::4]\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 15751/15751 [00:06<00:00, 2442.36it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(reviews):\n",
    "    if len(i) > 0:\n",
    "        for section in i:\n",
    "            for review in section:\n",
    "                for word in review:\n",
    "                    if word not in wordcounts:\n",
    "                        wordcounts[word] = 0\n",
    "                    wordcounts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get words that are not in the glove dataset\n",
    "knowns   = [word for word in wordcounts if word in glove]\n",
    "unknowns = [word for word in wordcounts if word not in glove]\n",
    "# sort words by their frequency\n",
    "wordOrder = list(sorted(knowns, key=lambda x: wordcounts[x], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 50252/50252 [00:00<00:00, 1396498.78it/s]\n"
     ]
    }
   ],
   "source": [
    "weights = [glove[word] for word in tqdm(wordOrder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in unknowns:\n",
    "    if (\"rating\" in word) or (\"polarity\" in word):\n",
    "        part = word.split(\"_\")\n",
    "        weight = glove[part[0]] + glove[part[1]]\n",
    "    else:\n",
    "        # generate a random weight\n",
    "        weight = np.random.normal(0,0.5,gloveDimension)\n",
    "    wordOrder.append(word)\n",
    "    weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries for constant time referencing\n",
    "id2word = {idx: w for (idx, w) in enumerate(wordOrder)}\n",
    "word2id = {w: idx for (idx, w) in enumerate(wordOrder)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 15751/15751 [00:03<00:00, 4379.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert words to their id's in the review.\n",
    "for i in tqdm(range(len(reviews))):\n",
    "    part = reviews[i]\n",
    "    if len(part) > 0:\n",
    "        for j in range(len(part)):\n",
    "            section = part[j]\n",
    "            for k in range(len(section)):\n",
    "                review = section[k]\n",
    "                reviews[i][j][k] = [word2id[w] for w in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 15751/15751 [00:00<00:00, 779595.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# need to flatten all the arrays to a 2d!\n",
    "flat = []\n",
    "for i in tqdm(reviews):\n",
    "    if len(i) > 0:\n",
    "        for j in i:\n",
    "            flat += j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create container ready for use in dataset\n",
    "container = {\n",
    "    'id2word' : id2word,\n",
    "    'word2id' : word2id,\n",
    "    'reviews' : flat,\n",
    "    'weights' : np.array(weights)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "datasetFile = '../Datasets/Reviews/dataset_ready.pkl'\n",
    "# save the dataset to a pickle file.\n",
    "output = open(datasetFile, 'wb')\n",
    "pickle.dump(container, output)\n",
    "output.close()\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id2word', 'word2id', 'reviews', 'weights'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
