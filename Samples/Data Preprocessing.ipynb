{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import gzip\n",
    "import bcolz\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the dataset in 7.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "pickleFile = '../Datasets/Reviews/dataset.pkl'\n",
    "start = time.clock()\n",
    "dataset = pickle.load( open( pickleFile, \"rb\" ))\n",
    "duration = time.clock() - start\n",
    "print(\"Loaded the dataset in\", round(duration,2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 63001 amazon items.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\",len(dataset), \"amazon items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Glove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading files.. Done.\n"
     ]
    }
   ],
   "source": [
    "def loadGlove(glove_path, dim=50):\n",
    "    acceptedDimensions = [50, 100, 200, 300]\n",
    "    if dim not in acceptedDimensions:\n",
    "        print(\"You didn't choose a right dimension.\")\n",
    "        print(\"Try one of these:\", acceptedDimensions)\n",
    "        return None\n",
    "    pickleWordFile = f'{glove_path}/6B.'+str(dim)+'_words.pkl'\n",
    "    pickleIdFile   = f'{glove_path}/6B.'+str(dim)+'_idx.pkl'\n",
    "    pickleDatFile  = f'{glove_path}/glove.6B.'+str(dim)+'.dat'\n",
    "    pickleDataset  = f'{glove_path}/glove.6B.'+str(dim)+'d.txt'\n",
    "    \n",
    "    if os.path.isfile(pickleWordFile):\n",
    "        # check if we've made the outputs before\n",
    "        print(\"Preloading files..\", end=\" \")\n",
    "        vectors = bcolz.open(pickleDatFile)[:]\n",
    "        words = pickle.load(open(pickleWordFile, 'rb'))\n",
    "        word2idx = pickle.load(open(pickleIdFile, 'rb'))\n",
    "        glove = {w: vectors[word2idx[w]] for w in words}\n",
    "        print(\"Done.\")\n",
    "        return glove\n",
    "    else:\n",
    "        print(\"Doesn't work.\", end=\" \")\n",
    "\n",
    "\n",
    "gloveDimension = 50\n",
    "glovePath = \"/media/data/Datasets/glove\"\n",
    "glove = loadGlove(glovePath, dim=gloveDimension)\n",
    "gloveWords = glove.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Vocabulary Size: 400000\n"
     ]
    }
   ],
   "source": [
    "print(\"Glove Vocabulary Size:\",len(gloveWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(paragraph):\n",
    "    # split paragraph by full stops\n",
    "    paragraph = paragraph.lower()\n",
    "    paragraph = re.sub(\"([,!?()-+&£$.%*'])\", r' \\1 ', paragraph)\n",
    "    paragraph = re.sub('\\s{2,}', ' ', paragraph)\n",
    "    paragraph = paragraph.split(\" \")\n",
    "    # remove empty string\n",
    "    return paragraph\n",
    "\n",
    "def padSentence(words, maxLength, eosString=\"<eos>\", padString=\"<pad>\"):\n",
    "    words = words[:maxLength-1] + [eosString]\n",
    "    return words + [padString for i in range(maxLength - len(words))]\n",
    "    \n",
    "def discretise(value, word):\n",
    "    return word + \"_\" + str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleItem(itemID, dataset=dataset):\n",
    "    \"\"\"\n",
    "    Filters words out based on whether they're in the GloVe dataset or not.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    legit = 0\n",
    "    printDebug = False\n",
    "\n",
    "    reviews = []\n",
    "    for i in range(len(dataset[itemID])):\n",
    "        # initialise variables\n",
    "        entry = dataset[itemID][i]\n",
    "        reviewerID = entry['reviewerID']\n",
    "\n",
    "        \"\"\"\n",
    "        Review Text Processing\n",
    "        \"\"\"\n",
    "        # preprocess review\n",
    "\n",
    "        # split sentences\n",
    "        sentences = re.split(r'(?<=\\.) ', entry['reviewText'])\n",
    "        for s in range(len(sentences)):\n",
    "            sentence = sentences[s]\n",
    "            sentences[s] = list(filter(None, preprocess(sentence)))\n",
    "\n",
    "#             words = preprocess(entry['reviewText'])\n",
    "#             words = [w for w in words if w in wordbase]\n",
    "#             words = words[:maxReviewLength]\n",
    "\n",
    "        # preprocess summary\n",
    "        summary = preprocess(entry['summary'])\n",
    "\n",
    "        # start of sequence\n",
    "        # start of review\n",
    "        # end of review\n",
    "        entries =  [[\"<sos>\"]] + [summary] + [[\"<sor>\"]] + sentences + [[\"<eos>\"]]\n",
    "\n",
    "        # setup review parameters\n",
    "        rating   = [discretise(entry['overall'], \"rating\")]\n",
    "\n",
    "        # process polarity\n",
    "        good, bad = entry['helpful'][0], entry['helpful'][1]\n",
    "        try:\n",
    "            polarity = (good - bad) / (good + bad)\n",
    "        except ZeroDivisionError:\n",
    "            polarity = 0\n",
    "        polarity = np.tanh(polarity)\n",
    "        polarity = np.round(polarity, 1)\n",
    "        polarity = [discretise(polarity, \"polarity\")]\n",
    "\n",
    "        # create identity/conditioning entry\n",
    "        identifier = itemID.lower()\n",
    "        identity = [l for l in identifier] + rating + polarity\n",
    "\n",
    "        # add conditionining entry to each entry\n",
    "        formatted = [identity + entry for entry in entries]\n",
    "\n",
    "\n",
    "        if printDebug:\n",
    "            print(dataset[itemID][i])\n",
    "            print(identity)\n",
    "            print(entries)\n",
    "            print()\n",
    "#                 print(\"----------\")\n",
    "            print(formatted)\n",
    "            print()\n",
    "            print(\"--------------\")\n",
    "\n",
    "        for i in range(len(formatted)-1):\n",
    "            reviews.append([formatted[i], formatted[i+1]])\n",
    "    \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetKeys = list(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63001\n"
     ]
    }
   ],
   "source": [
    "print(len(datasetKeys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processItems(func, args, n_processes = 7):\n",
    "    p = Pool(n_processes)\n",
    "    res_list = []\n",
    "    with tqdm(total = len(args)) as pbar:\n",
    "        for i, res in enumerate(p.imap_unordered(func, args)):\n",
    "            pbar.update()\n",
    "            res_list.append(res)\n",
    "    pbar.close()\n",
    "    p.close()\n",
    "    p.join()\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3938/3938 [00:06<00:00, 644.20it/s]\n"
     ]
    }
   ],
   "source": [
    "reviews = processItems(handleItem,datasetKeys[::16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3150 788\n",
      "751694 179357\n"
     ]
    }
   ],
   "source": [
    "datasetSize = len(reviews)\n",
    "trainPortion = 0.8\n",
    "valPortion = 0.2\n",
    "\n",
    "trainRatio = int(datasetSize * trainPortion)\n",
    "valRatio = int(datasetSize * valPortion)\n",
    "\n",
    "train = reviews[:trainRatio]\n",
    "validation = reviews[trainRatio:]\n",
    "\n",
    "print(len(train), len(validation))\n",
    "\n",
    "# now we need to flatten train and validation.\n",
    "trainents = []\n",
    "for review in train:\n",
    "    trainents += [entry for entry in review]\n",
    "valents = []\n",
    "for review in validation:\n",
    "    valents += [entry for entry in review]\n",
    "    \n",
    "train = trainents\n",
    "validation = valents\n",
    "\n",
    "print(len(train), len(validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ID's of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the throughput of the model, we should reduce the embedding size. Here we'll look at all the words and keep track ones that exist. We'll make a reduced word2id based on this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting Reviews..\n",
      "We now have 751694 reviews.\n"
     ]
    }
   ],
   "source": [
    "# here we reduce the size of the dataset so we can debug our model.\n",
    "print(\"Subsetting Reviews..\")\n",
    "print(\"We now have\", len(train), \"reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 751694/751694 [00:12<00:00, 58307.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# get word frequency for words in training data\n",
    "for row in tqdm(train):\n",
    "    for sequences in row:\n",
    "        for word in sequences:\n",
    "            word = str(word)\n",
    "            if word not in wordcounts:\n",
    "                wordcounts[word] = 0\n",
    "            wordcounts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get words that are not in the glove dataset\n",
    "knowns   = [word for word in wordcounts if word in glove]\n",
    "unknowns = [word for word in wordcounts if word not in glove]\n",
    "# sort words by their frequency\n",
    "wordOrder = list(sorted(knowns, key=lambda x: wordcounts[x], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43017 96094\n"
     ]
    }
   ],
   "source": [
    "print(len(knowns), len(unknowns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularyLimit = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordOrder = wordOrder[:vocabularyLimit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [glove[word] for word in wordOrder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in unknowns:\n",
    "    if (\"rating\" in word) or (\"polarity\" in word):\n",
    "        try:\n",
    "            part = word.split(\"_\")\n",
    "            if part[1] == \"-0.0\":\n",
    "                part[1] = \"0.0\"\n",
    "            weight = glove[part[0]] + glove[part[1]]\n",
    "            wordOrder.append(word)\n",
    "            weights.append(weight)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries for constant time referencing\n",
    "id2word = {idx: w for (idx, w) in enumerate(wordOrder)}\n",
    "word2id = {w: idx for (idx, w) in enumerate(wordOrder)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = len(word2id)\n",
    "# add <eos>\n",
    "weights.append(glove['eos'])\n",
    "word2id['<eos>'] = lim\n",
    "id2word[lim] = '<eos>'\n",
    "lim += 1\n",
    "# add <sos>\n",
    "weights.append(glove['sos'])\n",
    "word2id['<sos>'] = lim\n",
    "id2word[lim] = ['<sos>']\n",
    "lim += 1\n",
    "\n",
    "# add <sor> (start of review)\n",
    "sorWeight = np.random.normal(0,0.5,gloveDimension)\n",
    "weights.append(sorWeight)\n",
    "word2id['<sor>'] = lim\n",
    "id2word[lim] = '<sor>'\n",
    "lim += 1\n",
    "\n",
    "weights.append(glove['unk'])\n",
    "word2id['<unk>'] = lim\n",
    "id2word[lim] = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordToID(word,corp=word2id):\n",
    "    if word in corp:\n",
    "        return corp[word]\n",
    "    return corp['<unk>']\n",
    "\n",
    "def IDToWord(id,corp=id2word, ref=word2id):\n",
    "    if word in corp:\n",
    "        return corp[word]\n",
    "    return corp[ref['<unk>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 751694/751694 [00:09<00:00, 81200.00it/s]\n",
      "100%|██████████| 179357/179357 [00:01<00:00, 108398.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert words to their id's in the review.\n",
    "def entriesToWordIDs(group):\n",
    "    return [[[wordToID(word) for word in seq] for seq in row] for row in tqdm(group)]\n",
    "    \n",
    "train = entriesToWordIDs(train)\n",
    "validation = entriesToWordIDs(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 751694/751694 [00:00<00:00, 1389472.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1708"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes = {}\n",
    "for row in tqdm(train):\n",
    "    for seq in row:\n",
    "        length = len(seq)\n",
    "        if length not in sizes:\n",
    "            sizes[length] = 0\n",
    "        sizes[length] += 1\n",
    "\n",
    "seqlengths = list(sorted(sizes.keys(), key=lambda x: sizes[x], reverse=True))\n",
    "max(seqlengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1709):\n",
    "    if i not in sizes:\n",
    "        sizes[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ents = [x for x in range(0,50)]\n",
    "bins = [sizes[x] for x in ents]\n",
    "plt.bar(ents,bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff sequence length: 50\n"
     ]
    }
   ],
   "source": [
    "cutoff = seqlengths[37]\n",
    "print(\"Cutoff sequence length:\", cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 751694/751694 [00:02<00:00, 268879.06it/s]\n",
      "100%|██████████| 179357/179357 [00:00<00:00, 713219.73it/s]\n"
     ]
    }
   ],
   "source": [
    "def trimSeq(group):\n",
    "    return [[seq[:cutoff] for seq in row] for row in tqdm(group)]\n",
    "\n",
    "train = trimSeq(train)\n",
    "validation = trimSeq(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create container ready for use in dataset\n",
    "container = {\n",
    "    'id2word' : id2word,\n",
    "    'word2id' : word2id,\n",
    "    'train' : train,\n",
    "    'validation': validation,\n",
    "    'weights' : np.matrix(weights)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    this function will convert bytes to MB.... GB... etc\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved! 138.7 MB\n"
     ]
    }
   ],
   "source": [
    "datasetFile = '../Datasets/Reviews/dataset_ready.pkl'\n",
    "# save the dataset to a pickle file.\n",
    "output = open(datasetFile, 'wb')\n",
    "pickle.dump(container, output)\n",
    "output.close()\n",
    "print(\"Saved!\", convert_bytes(os.stat(datasetFile).st_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
