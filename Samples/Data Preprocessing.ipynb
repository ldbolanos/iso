{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import gzip\n",
    "import bcolz\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the dataset in 7.61 seconds.\n"
     ]
    }
   ],
   "source": [
    "pickleFile = '../Datasets/Reviews/dataset.pkl'\n",
    "start = time.clock()\n",
    "dataset = pickle.load( open( pickleFile, \"rb\" ))\n",
    "duration = time.clock() - start\n",
    "print(\"Loaded the dataset in\", round(duration,2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 63001 amazon items.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\",len(dataset), \"amazon items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Glove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading files.. Done.\n"
     ]
    }
   ],
   "source": [
    "def loadGlove(glove_path, dim=50):\n",
    "    acceptedDimensions = [50, 100, 200, 300]\n",
    "    if dim not in acceptedDimensions:\n",
    "        print(\"You didn't choose a right dimension.\")\n",
    "        print(\"Try one of these:\", acceptedDimensions)\n",
    "        return None\n",
    "    pickleWordFile = f'{glove_path}/6B.'+str(dim)+'_words.pkl'\n",
    "    pickleIdFile   = f'{glove_path}/6B.'+str(dim)+'_idx.pkl'\n",
    "    pickleDatFile  = f'{glove_path}/glove.6B.'+str(dim)+'.dat'\n",
    "    pickleDataset  = f'{glove_path}/glove.6B.'+str(dim)+'d.txt'\n",
    "    \n",
    "    if os.path.isfile(pickleWordFile):\n",
    "        # check if we've made the outputs before\n",
    "        print(\"Preloading files..\", end=\" \")\n",
    "        vectors = bcolz.open(pickleDatFile)[:]\n",
    "        words = pickle.load(open(pickleWordFile, 'rb'))\n",
    "        word2idx = pickle.load(open(pickleIdFile, 'rb'))\n",
    "        glove = {w: vectors[word2idx[w]] for w in words}\n",
    "        print(\"Done.\")\n",
    "        return glove\n",
    "    else:\n",
    "        print(\"Doesn't work.\", end=\" \")\n",
    "\n",
    "\n",
    "gloveDimension = 50\n",
    "glovePath = \"/media/data/Datasets/glove\"\n",
    "glove = loadGlove(glovePath, dim=gloveDimension)\n",
    "gloveWords = glove.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Vocabulary Size: 400000\n"
     ]
    }
   ],
   "source": [
    "print(\"Glove Vocabulary Size:\",len(gloveWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(paragraph):\n",
    "    # split paragraph by full stops\n",
    "    paragraph = paragraph.lower()\n",
    "    paragraph = re.sub(\"([,!?()-+&£$.%*'])\", r' \\1 ', paragraph)\n",
    "    paragraph = re.sub('\\s{2,}', ' ', paragraph)\n",
    "    paragraph = paragraph.split(\" \")\n",
    "    # remove empty string\n",
    "    return paragraph\n",
    "\n",
    "def padSentence(words, maxLength, eosString=\"<eos>\", padString=\"<pad>\"):\n",
    "    words = words[:maxLength-1] + [eosString]\n",
    "    return words + [padString for i in range(maxLength - len(words))]\n",
    "    \n",
    "def discretise(value, word):\n",
    "    return word + \"_\" + str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleItem(itemID, dataset=dataset, printDebug=False):\n",
    "    \"\"\"\n",
    "    Filters words out based on whether they're in the GloVe dataset or not.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    reviews = []\n",
    "    for i in range(len(dataset[itemID])):\n",
    "        # initialise variables\n",
    "        entry = dataset[itemID][i]\n",
    "        reviewerID = entry['reviewerID']\n",
    "\n",
    "        \"\"\"\n",
    "        Review Text Processing\n",
    "        \"\"\"\n",
    "        # split sentences\n",
    "        sentences = re.split(r'(?<=\\.) ', entry['reviewText'])\n",
    "        for s in range(len(sentences)):\n",
    "            sentence = sentences[s]\n",
    "            sentences[s] = list(filter(None, preprocess(sentence)))\n",
    "            \n",
    "        # preprocess summary\n",
    "        summary = list(filter(None,preprocess(entry['summary'])))\n",
    "        \n",
    "        # merge summary sequence and review sequences together into overall entries.\n",
    "        if len(sentences) < 2:\n",
    "            entries =  [[\"<sos>\"] + summary + [\"<sor>\"]] + [sentences[0] + [\"<eos>\"]]\n",
    "        else:\n",
    "            entries =  [[\"<sos>\"] + summary + [\"<sor>\"]] + [sentences[0]] + sentences[1:-1] + [sentences[-1] + [\"<eos>\"]]\n",
    "\n",
    "        # setup review parameters\n",
    "        rating   = [discretise(entry['overall'], \"rating\")]\n",
    "\n",
    "        # compute polarity\n",
    "        good, bad = entry['helpful'][0], entry['helpful'][1]\n",
    "        try:\n",
    "            polarity = (good - bad) / (good + bad)\n",
    "        except ZeroDivisionError:\n",
    "            polarity = 0\n",
    "        polarity = np.tanh(polarity)\n",
    "        polarity = np.round(polarity, 1)\n",
    "        polarity = [discretise(polarity, \"polarity\")]\n",
    "\n",
    "        # create identity/conditioning entry\n",
    "        identifier = itemID.lower()\n",
    "        identity = [l for l in identifier] + rating + polarity\n",
    "\n",
    "        # add conditionining entry to each entry\n",
    "        formatted = [identity + entry for entry in entries]\n",
    "\n",
    "\n",
    "        if printDebug:\n",
    "            print(\"ENTRY:\",dataset[itemID][i])\n",
    "            print(\"IDENTITY:\",identity)\n",
    "            \n",
    "            su = len(identity)\n",
    "            print()\n",
    "            for entry in formatted:\n",
    "                print(entry[su:])\n",
    "            print()\n",
    "\n",
    "        for i in range(len(formatted)-1):\n",
    "            reviews.append([formatted[i], formatted[i+1]])\n",
    "        if printDebug:\n",
    "            break\n",
    "            \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTRY: {'reviewerID': 'AA8JH8LD2H4P9', 'asin': '7214047977', 'reviewerName': 'Claudia J. Frier', 'helpful': [3, 4], 'reviewText': 'This fits my 7\" kindle fire hd perfectly! I love it. It even has a slot for a stylus. The kindle is velcroed in so it\\'s nice and secure. Very glad I bought this!', 'overall': 5.0, 'summary': 'love it', 'unixReviewTime': 1354665600, 'reviewTime': '12 5, 2012'}\n",
      "IDENTITY: ['7', '2', '1', '4', '0', '4', '7', '9', '7', '7', 'rating_5.0', 'polarity_-0.1']\n",
      "\n",
      "['<sos>', 'love', 'it', '<sor>']\n",
      "['this', 'fits', 'my', '7\"', 'kindle', 'fire', 'hd', 'perfectly', '!', 'i', 'love', 'it', '.']\n",
      "['it', 'even', 'has', 'a', 'slot', 'for', 'a', 'stylus', '.']\n",
      "['the', 'kindle', 'is', 'velcroed', 'in', 'so', 'it', \"'\", 's', 'nice', 'and', 'secure', '.']\n",
      "['very', 'glad', 'i', 'bought', 'this', '!', '<eos>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasetKeys = list(dataset.keys())\n",
    "k = handleItem(datasetKeys[19],printDebug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63001\n"
     ]
    }
   ],
   "source": [
    "print(len(datasetKeys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processItems(func, args, n_processes = 7):\n",
    "    p = Pool(n_processes)\n",
    "    res_list = []\n",
    "    with tqdm(total = len(args)) as pbar:\n",
    "        for i, res in enumerate(p.imap_unordered(func, args)):\n",
    "            pbar.update()\n",
    "            res_list.append(res)\n",
    "    pbar.close()\n",
    "    p.close()\n",
    "    p.join()\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7876/7876 [00:13<00:00, 583.51it/s]\n"
     ]
    }
   ],
   "source": [
    "reviews = processItems(handleItem,datasetKeys[::8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6300 1576\n",
      "1004741 272484\n"
     ]
    }
   ],
   "source": [
    "datasetSize = len(reviews)\n",
    "trainPortion = 0.8\n",
    "valPortion = 0.2\n",
    "\n",
    "trainRatio = int(datasetSize * trainPortion)\n",
    "valRatio = int(datasetSize * valPortion)\n",
    "\n",
    "train = reviews[:trainRatio]\n",
    "validation = reviews[trainRatio:]\n",
    "\n",
    "print(len(train), len(validation))\n",
    "\n",
    "# now we need to flatten train and validation.\n",
    "trainents = []\n",
    "for review in train:\n",
    "    trainents += [entry for entry in review]\n",
    "valents = []\n",
    "for review in validation:\n",
    "    valents += [entry for entry in review]\n",
    "    \n",
    "train = trainents\n",
    "validation = valents\n",
    "\n",
    "print(len(train), len(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['3', '9', '3', '0', '9', '9', '2', '8', '6', '8', 'rating_5.0', 'polarity_0.0', '<sos>', 'harddrive', '<sor>'], ['3', '9', '3', '0', '9', '9', '2', '8', '6', '8', 'rating_5.0', 'polarity_0.0', 'brought', 'back', 'my', 'ipod', 'video', '30g', 'back', 'life', ',', 'easy', 'to', 'replace', '.']]\n"
     ]
    }
   ],
   "source": [
    "# get the number of itemIDs\n",
    "for row in train:\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ID's of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the throughput of the model, we should reduce the embedding size. Here we'll look at all the words and keep track ones that exist. We'll make a reduced word2id based on this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting Reviews..\n",
      "We now have 1004741 reviews.\n"
     ]
    }
   ],
   "source": [
    "# here we reduce the size of the dataset so we can debug our model.\n",
    "print(\"Subsetting Reviews..\")\n",
    "print(\"We now have\", len(train), \"reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1004741/1004741 [00:22<00:00, 45313.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# get word frequency for words in training data\n",
    "for row in tqdm(train):\n",
    "    for sequences in row:\n",
    "        for word in sequences:\n",
    "            word = str(word)\n",
    "            if word not in wordcounts:\n",
    "                wordcounts[word] = 0\n",
    "            wordcounts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get words that are not in the glove dataset\n",
    "knowns   = [word for word in wordcounts if word in glove]\n",
    "unknowns = [word for word in wordcounts if word not in glove]\n",
    "# sort words by their frequency\n",
    "wordOrder = list(sorted(knowns, key=lambda x: wordcounts[x], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53587 169840\n"
     ]
    }
   ],
   "source": [
    "print(len(knowns), len(unknowns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularyLimit = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordOrder = wordOrder[:vocabularyLimit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [glove[word] for word in wordOrder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in unknowns:\n",
    "    if (\"rating\" in word) or (\"polarity\" in word):\n",
    "        try:\n",
    "            part = word.split(\"_\")\n",
    "            if part[1] == \"-0.0\":\n",
    "                part[1] = \"0.0\"\n",
    "            weight = glove[part[0]] + glove[part[1]]\n",
    "            wordOrder.append(word)\n",
    "            weights.append(weight)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries for constant time referencing\n",
    "id2word = {idx: w for (idx, w) in enumerate(wordOrder)}\n",
    "word2id = {w: idx for (idx, w) in enumerate(wordOrder)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = len(word2id)\n",
    "# add <eos> (end of sequence)\n",
    "weights.append(glove['eos'])\n",
    "word2id['<eos>'] = lim\n",
    "id2word[lim] = '<eos>'\n",
    "lim += 1\n",
    "\n",
    "# add <sos> (start of sequence)\n",
    "weights.append(glove['sos'])\n",
    "word2id['<sos>'] = lim\n",
    "id2word[lim] = ['<sos>']\n",
    "lim += 1\n",
    "\n",
    "# add <sor> (start of review)\n",
    "sorWeight = np.random.normal(0,0.5,gloveDimension)\n",
    "weights.append(sorWeight)\n",
    "word2id['<sor>'] = lim\n",
    "id2word[lim] = '<sor>'\n",
    "lim += 1\n",
    "\n",
    "# add <unk> (unknown token)\n",
    "weights.append(glove['unk'])\n",
    "word2id['<unk>'] = lim\n",
    "id2word[lim] = '<unk>'\n",
    "\n",
    "# add <pad> \n",
    "id2word[len(word2id)] = \"<pad>\"\n",
    "word2id[\"<pad>\"] = len(word2id)\n",
    "weights.append(np.random.normal(0,0,gloveDimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(0,0,gloveDimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordToID(word,corp=word2id):\n",
    "    if word in corp:\n",
    "        return corp[word]\n",
    "    return corp['<unk>']\n",
    "\n",
    "def IDToWord(id,corp=id2word, ref=word2id):\n",
    "    if word in corp:\n",
    "        return corp[word]\n",
    "    return corp[ref['<unk>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1004741/1004741 [00:12<00:00, 78159.93it/s]\n",
      "100%|██████████| 272484/272484 [00:05<00:00, 50940.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert words to their id's in the review.\n",
    "def entriesToWordIDs(group):\n",
    "    return [[[wordToID(word) for word in seq] for seq in row] for row in tqdm(group)]\n",
    "    \n",
    "train = entriesToWordIDs(train)\n",
    "validation = entriesToWordIDs(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sequence in our dataset is 1709 tokens long.\n"
     ]
    }
   ],
   "source": [
    "sizes = {}\n",
    "for row in train:\n",
    "    for seq in row:\n",
    "        length = len(seq)\n",
    "        if length not in sizes:\n",
    "            sizes[length] = 0\n",
    "        sizes[length] += 1\n",
    "\n",
    "seqlengths = list(sorted(sizes.keys(), key=lambda x: sizes[x], reverse=True))\n",
    "print(\"The longest sequence in our dataset is\",max(seqlengths),\"tokens long.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1709):\n",
    "    if i not in sizes:\n",
    "        sizes[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ents = [x for x in range(0,70)]\n",
    "bins = [sizes[x] for x in ents]\n",
    "plt.bar(ents,bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff sequence length: 60\n"
     ]
    }
   ],
   "source": [
    "cutoff = 60\n",
    "print(\"Cutoff sequence length:\", cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1004741/1004741 [00:04<00:00, 234725.95it/s]\n",
      "100%|██████████| 272484/272484 [00:00<00:00, 627758.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def trimSeq(group):\n",
    "    return [[seq[:cutoff] for seq in row] for row in tqdm(group)]\n",
    "\n",
    "train = trimSeq(train)\n",
    "validation = trimSeq(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create container ready for use in dataset\n",
    "# we do not add padding here as we want to reduce storage size!\n",
    "container = {\n",
    "    'id2word' : id2word,\n",
    "    'word2id' : word2id,\n",
    "    'train' : train,\n",
    "    'validation': validation,\n",
    "    'weights' : np.matrix(weights),\n",
    "    'cutoff' : cutoff\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved! 218.7 MB\n"
     ]
    }
   ],
   "source": [
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    this function will convert bytes to MB.... GB... etc\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0\n",
    "        \n",
    "        \n",
    "datasetFile = '../Datasets/Reviews/dataset_ready.pkl'\n",
    "# save the dataset to a pickle file.\n",
    "output = open(datasetFile, 'wb')\n",
    "pickle.dump(container, output)\n",
    "output.close()\n",
    "print(\"Saved!\", convert_bytes(os.stat(datasetFile).st_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[0][::32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sequence lengths for train and validation data\n",
    "trainx = [x[0] for x in train]\n",
    "trainy = [x[1] for x in train]\n",
    "valx   = [x[0] for x in validation]\n",
    "valy   = [x[1] for x in validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Dataset batching mechanism\n",
    "\"\"\"\n",
    "\n",
    "def padSeq(row, maxlength, padID, cutoff):\n",
    "    currentLength = len(row)\n",
    "    difference = maxlength - currentLength\n",
    "    return row + [padID for _ in range(difference)]\n",
    "\n",
    "\n",
    "def batchData(dataset, padID, device, batchsize=32, cutoff=50):\n",
    "    \"\"\"\n",
    "    Splits the dataset into batches.\n",
    "    Each batch needs to be sorted by \n",
    "    the length of their sequence in order\n",
    "    for `pack_padded_sequence` to be used.\n",
    "    \"\"\"\n",
    "    datasize = len(dataset)\n",
    "    batches = []\n",
    "    # split data into batches.\n",
    "    for i in range(0, datasize, batchsize):\n",
    "        batches.append(dataset[i:i+batchsize])\n",
    "    # within each batch, sort the entries.\n",
    "    for i in range(len(batches)):\n",
    "        batch = batches[i]\n",
    "        # get lengths of each review in the batch\n",
    "        # based on the postion of the EOS tag.\n",
    "        lengths = [len(seq) for seq in batch]\n",
    "        indexes = [x for x in range(len(lengths))]\n",
    "        sortedindexes = sorted(list(zip(lengths, indexes)), reverse=True)\n",
    "\n",
    "        # since sentences are split by period, the period itself acts\n",
    "        # the token to identify that the sentence has ended.\n",
    "        # i.e. we don't need another token identifying the end of the subsequence.\n",
    "\n",
    "        # get the reviews based on the sorted batch lengths\n",
    "        reviews = [padSeq(batch[i[1]], cutoff, padID, cutoff)\n",
    "                   for i in sortedindexes]\n",
    "\n",
    "        reviews = torch.tensor(reviews, dtype=torch.long, device=device)\n",
    "        # re-allocate values.\n",
    "        batches[i] = (reviews, [i[0] for i in sortedindexes])\n",
    "    return batches\n",
    "device = \"cpu\"\n",
    "batchsize = 32\n",
    "cutoff = cutoff\n",
    "trainx_p = batchData(trainx, word2id['<pad>'], device, batchsize, cutoff)\n",
    "trainy_p = batchData(trainy, word2id['<pad>'], device, batchsize, cutoff)\n",
    "valx_p = batchData(valx, word2id['<pad>'], device, batchsize, cutoff)\n",
    "valy_p = batchData(valy, word2id['<pad>'], device, batchsize, cutoff)\n",
    "\n",
    "train_p = (trainx_p, trainy_p)\n",
    "val_p = (valx_p, valy_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30020, 50)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightshape = np.matrix(weights).shape\n",
    "weightshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 60, 1])\n",
      "torch.Size([32, 60, 30020])\n",
      "torch.Size([32, 60])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected target size (32, 30020), got torch.Size([32, 60, 30020])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-74e6fe07f26f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#         crit = F.cross_entropy(ignore_index=word2id['pad'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1970\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m             raise ValueError('Expected target size {}, got {}'.format(\n\u001b[0;32m-> 1800\u001b[0;31m                 out_size, target.size()))\n\u001b[0m\u001b[1;32m   1801\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected target size (32, 30020), got torch.Size([32, 60, 30020])"
     ]
    }
   ],
   "source": [
    "for batch in range(len(train_p[0])):\n",
    "    # load x, y from batch\n",
    "    entry_x, entry_y = train_p[0][batch], train_p[1][batch]\n",
    "    # sepeate data from sentence lengths\n",
    "    y_outs, y_seqs = entry_y\n",
    "    \n",
    "    # get y_length\n",
    "    y_len = len(y_outs[0])\n",
    "    \n",
    "    num_classes = weightshape[0]\n",
    "    batch_size = y_outs.shape[0]\n",
    "    \n",
    "    # iterate through the words in y\n",
    "    for w in range(y_len):\n",
    "        # get indexes of future words\n",
    "        labels = y_outs[:,w:].long().unsqueeze(2)\n",
    "        print(labels.shape)\n",
    "        # set up bag of words container\n",
    "        bow = torch.FloatTensor(batch_size, y_len-w, num_classes).zero_()\n",
    "        # create SBOW\n",
    "        bow.scatter_(2, labels, 1)\n",
    "        print(bow.shape)\n",
    "        \n",
    "        labels = labels.squeeze(2).float()\n",
    "        print(labels.shape)\n",
    "#         crit = F.cross_entropy(ignore_index=word2id['pad'])\n",
    "        loss = F.cross_entropy(bow,bow)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1])\n",
      "torch.Size([2, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 1.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 0.]]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 3\n",
    "labels = torch.LongTensor([[[2,1,0]], [[0,1,0]]]).permute(0,2,1) # Let this be your current batch\n",
    "print(labels.shape)\n",
    "batch_size, k, _ = labels.size()\n",
    "labels_one_hot = torch.FloatTensor(batch_size, k, num_classes).zero_()\n",
    "print(labels_one_hot.shape)\n",
    "labels_one_hot.scatter_(2, labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scatter_() received an invalid combination of arguments - got (int, Tensor, tuple), but expected one of:\n * (int dim, Tensor index, Tensor src)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mint\u001b[0m, \u001b[32;1mTensor\u001b[0m, \u001b[31;1mtuple\u001b[0m)\n * (int dim, Tensor index, Number value)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mint\u001b[0m, \u001b[32;1mTensor\u001b[0m, \u001b[31;1mtuple\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-b3698371c448>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: scatter_() received an invalid combination of arguments - got (int, Tensor, tuple), but expected one of:\n * (int dim, Tensor index, Tensor src)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mint\u001b[0m, \u001b[32;1mTensor\u001b[0m, \u001b[31;1mtuple\u001b[0m)\n * (int dim, Tensor index, Number value)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mint\u001b[0m, \u001b[32;1mTensor\u001b[0m, \u001b[31;1mtuple\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "torch.zeros(3, 5).scatter_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n",
      "torch.Size([2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 1.2300, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 1.2300]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwer = torch.tensor([[2], [3]])\n",
    "print(qwer.shape)\n",
    "z = torch.zeros(2, 4)\n",
    "print(z.shape)\n",
    "z.scatter_(1, qwer, 1.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
