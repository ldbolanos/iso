\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Literature Survey}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Background}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Neural Networks}{1}{subsection.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Backpropagation}{1}{subsection.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Autoencoders}{1}{subsection.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Recurrent Neural Networks}{1}{subsection.6}}
\citation{cho_properties_2014}
\citation{chung_empirical_2014}
\@writefile{toc}{\contentsline {subsubsection}{Backpropagation through Time}{2}{section*.7}}
\@writefile{toc}{\contentsline {subsubsection}{LSTM}{2}{section*.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A diagram of the LSTM model.\relax }}{2}{figure.caption.9}}
\@writefile{toc}{\contentsline {subsubsection}{Gated Recurrent Units}{2}{section*.10}}
\@writefile{brf}{\backcite{cho_properties_2014}{{2}{1.1.4}{section*.10}}}
\citation{sutskever_sequence_2014}
\citation{vaswani_attention_2017}
\citation{vaswani_attention_2017}
\citation{vaswani_attention_2017}
\@writefile{brf}{\backcite{chung_empirical_2014}{{3}{1.1.4}{section*.10}}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Related Work}{3}{section.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Seq2Seq}{3}{subsection.12}}
\@writefile{brf}{\backcite{sutskever_sequence_2014}{{3}{1.2.1}{subsection.12}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Transformers}{3}{subsection.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces The transformer model architecture.\cite  {vaswani_attention_2017} \relax }}{3}{figure.caption.14}}
\@writefile{brf}{\backcite{vaswani_attention_2017}{{3}{1.2}{figure.caption.14}}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{transformer}{{1.2}{3}{The transformer model architecture.\cite {vaswani_attention_2017} \relax }{figure.caption.14}{}}
\@writefile{brf}{\backcite{vaswani_attention_2017}{{3}{1.2.2}{figure.caption.14}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Variational Autoencoders}{4}{subsection.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces A simplified model architecture for a variational autoencoder, which takes as input some text, and it's predicted output being the same text as the input.\relax }}{4}{figure.caption.16}}
\newlabel{vae}{{1.3}{4}{A simplified model architecture for a variational autoencoder, which takes as input some text, and it's predicted output being the same text as the input.\relax }{figure.caption.16}{}}
\citation{du_variational_2018}
\citation{du_variational_2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Conditional Variational Autoencoders}{5}{subsection.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces A model architecture for a CVAE, which includes the label being fed into the encoder and decoder networks. \relax }}{5}{figure.caption.18}}
\newlabel{cvae_diagram}{{1.4}{5}{A model architecture for a CVAE, which includes the label being fed into the encoder and decoder networks. \relax }{figure.caption.18}{}}
\@writefile{brf}{\backcite{du_variational_2018}{{5}{1.2.4}{figure.caption.18}}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Variational Autoregressive Decoders}{5}{section.19}}
\@writefile{brf}{\backcite{du_variational_2018}{{5}{1.3}{section.19}}}
\bibstyle{apa}
\bibdata{ref}
\@writefile{toc}{\contentsline {subsubsection}{Sequential Model}{6}{section*.20}}
\@writefile{toc}{\contentsline {subsubsection}{Prior Model}{6}{section*.21}}
\@writefile{toc}{\contentsline {subsubsection}{Sequential Bag of Words}{6}{section*.22}}
\@writefile{toc}{\contentsline {subsubsection}{Learning Mechanism}{6}{section*.23}}
\bibcite{cho_properties_2014}{{1}{2014}{{Cho et~al.}}{{}}}
\bibcite{chung_empirical_2014}{{2}{2014}{{Chung et~al.}}{{}}}
\bibcite{du_variational_2018}{{3}{2018}{{Du et~al.}}{{}}}
\bibcite{sutskever_sequence_2014}{{4}{2014}{{Sutskever et~al.}}{{}}}
\bibcite{vaswani_attention_2017}{{5}{2017}{{Vaswani et~al.}}{{}}}
