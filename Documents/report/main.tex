\documentclass[12pt,twoside]{report}
\usepackage{tikz}
\usetikzlibrary{positioning, fit, arrows.meta}
\usepackage{amsmath,amssymb}

 
\usepackage{listings}
\usepackage{color}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=true,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Definitions for the title page
% Edit these to provide the correct information
% e.g. \newcommand{\reportauthor}{Timothy Kimber}

\newcommand{\reporttitle}{Chameleon Text: Exploring ways to increase variety in artificial data}
\newcommand{\reportauthor}{Thien P. Nguyen}
\newcommand{\supervisor}{Julia Ive, Lucia Specia}
\newcommand{\degreetype}{Computing (Machine Learning)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

\date{May 2019}

\begin{document}

% load title page
\input{titlepage}


% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{abstract}
% Your abstract.
% \end{abstract}

% \cleardoublepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{Acknowledgments}
% Comment this out if not needed.

% \clearpage{\pagestyle{empty}\cleardoublepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
% \fancyhead[RE,LO]{\sffamily {Table of Contents}}
% \tableofcontents 


% % \clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}

% \begin{figure}[tb]
% \centering
% \includegraphics[width = 0.4\hsize]{./figures/imperial}
% \caption{Imperial College Logo. It's nice blue, and the font is quite stylish. But you can choose a different one if you don't like it.}
% \label{fig:logo}
% \end{figure}

% Figure~\ref{fig:logo} is an example of a figure. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% So I would suggest go straight to the point: starting with describing the problem first, positioning it in the range of the related problems (as you mentioned yourself), introducing in details just the Seq2seq solution with some necessary background (as your baseline), mention the limitations of Seq2seq for your problem and ways they could be addressed (VAEs, etc.), describe the VAE approach and your motivation to pick it, introduce the VAE paper you work with in details.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Literature Survey}

\section{Artificial Data}


The premise of this literature survey is to describe the relevant components necessary to construct our solution, and to also discuss alternative approaches to the problem. 

% talk about the purpose:
% artificial data for NLP problems.
% why? lack of resources. datasets limited. unexplored.


\section{Text Generation}

% Text generation is a 

% - introduce the problem of text generation
% 	- large problem
% 	- lots of components
% 	- present solutions are relatively rudimentary in terms of their throughput.


% - we want to increase the lexical variety of the generated texts.
% 	- why? because we want to create data that looks similar to the original data, so we can use it as interim to other organisations that wish to request data. (That way, we don't reveal the original data in it's entirety.)

% - talk about the fact that this is an understudied problem; the described tools are commonly used in other areas of contempoary computer science.

Text generation is a a type of Language Modelling problem, which in itself, is one of the core natural language processing problems, and it used in a variety of contempoary applications, ranging from machine translation, to email response generation, to document summarisation.

In our particular case we imagine a scenario where a client requests the use of our dataset. We would permit them access to said data, but it would reveal the identities of the users in the data. The objective is to have some alternative dataset to ours such that value can be deduced from the data, but the privacy of the users in our original dataset is maintained.

This can be accomplished by creating a language model that would increase the lexical variety of our original dataset. This language model would train on our original dataset, and would produce data that is semantically and lexically similar to our original data, but diverges enough such that it could potentially be seen as an entirely new and independent dataset. This new dataset can be assigned to other clients.

% problematics of controllable text generation?
% need to choose responses that are representative of the input, artificially.

\section{Language Modelling}

% Language modelling is the task of predicting the next word in a text given the previous words. It is probably the simplest language processing task with concrete practical applications such as intelligent keyboards and email response suggestion (Kannan et al., 2016)
Language modelling is the task of predicting a word $w_i$ in a text $w$ given some sequence of previous words $(w_1, w_2, ..., w_{i-1})$. More formally, \cite{dyer_conditional_2017} describes an unconditional language model as assigning a probability to a sequence of words,  $w = (w_1, w_2, ..., w_{i-1})$. This probability can be decomposed using the chain rule:

\begin{align}
p(w) = &{} p(w_1) \times p(w_2|w_1) \times p(w_3|w_1, w_2) \times ... \times p(w_i|w_1, w_2, ..., w_{i-1}) \\
p(w) = &{} \prod^{|w|}_{t=1}p(w_t|w_1, ..., w_{t-1})
\end{align}

Traditionally, assigning words to probabilities may conflate syntactically dubious sentences but it remains to be a useful method for representing texts.  

In particular, we are more interested in conditional language modelling. This slightly differs from the definition described above - A conditional language model assigns probabilities to sequences of words, $w = (w_1, w_2, ..., w_{i-1})$, given a conditioning variable, $x$. 

\begin{align}
	p(\boldmath{w}|x) = &{} \prod^{|w|}_{t=1}p(w_t|x,w_1, ..., w_{t-1})
\end{align}

There exists different types of models, but we shall start with the n-gram, argued by \cite{le_recurrent_2018} as being the most fundamental. An n-gram is a chunk of n consecutive words. For instance, given the sentence "the quick brown fox \ldots", the respective n-grams are: 

\begin{itemize}  
	\item unigrams: "the", "quick", "brown", "fox"
	\item bigrams: "the quick", "quick brown", "brown fox"
	\item trigrams: "the quick brown", "quick brown fox"
	\item 4-grams: "the quick brown fox"
\end{itemize}

The intuition of n-grams was that statistical inference can be applied on the frequency and distribution of of the n-grams, which could be used to predict the next word. However, sparsity is not captured.

% create a better flow

Modern language models revolve around the use of neural networks, which was introduced by \cite{bengio_neural_2001}, with a simple MLP that encoded words. The use of neural networks in language modelling is often called Neural Language Modelling, of NLM for short.

Neural Networks are non-linear statistical models that generate complex relationships between input and output vectors. (This type of neural network architecture is commonly described as the multi-layer perceptron). Note that the input and output vectors are of a fixed dimension, which becomes a problem for our task at hand. Neural Networks evaluate an input using forward propagation, to produce an output. Traditionally, neural networks are trained to produce optimal outputs via the use of backpropagation. 

% something on neural language models being popular.

\section{Recurrent Neural Networks}

Recurrent neural networks (RNNs) are a class of neural networks such that the outputs are not necessarily restricted and discrete (as opposed to the MLP). RNNs operate over a sequence of variable-length vectors, and produces an output of similarly variable-length vectors. This circumvents a problem introduced with using an MLP, where sentences are not typically fixed length. RNNs can XXXXXXXXXXXXXXX.

% we need formulas.

% we need illustrations

% this is an integral part of the design report

\begin{figure}[!ht]
      
	\centering
	\includegraphics[width=100mm]{diagrams/rnn.jpeg}
	\caption{Rectangles represent vectors, with red being the input, blue the output, and green representing the state of the RNNs. Arrows represent  functions. From left to right: (1) an MLP. (2,3,4,5) are examples of different styles of recurrent neural networks, describing the different types of input and output combinations. (Diagram from \cite{karpathy_unreasonable_2015}) \label{rnn}} 
  \end{figure}
  
The architecture of RNNs make it favourable in NLP related problems as words in sentences are typically conditioned on the previous words. Given a sequence of inputs $(x_1, ..., x_M)$, a standard RNN computes a sequence of outputs $(y_1, ..., y_N)$ by performing forward-propagation in a similar to fashion to MLPs, but outputs are chained together as an additional input to other neural networks. This can be visualised best in Figure \ref{rnn}. RNNs are trained using backpropagation-through-time.

% talk about how much has been done with RNNs

RNNs showed promise but there existed a multitude of problems. Firstly, It became aparrent that it was very difficult for RNNs to leverage relationships between potentially relevant inputs and outputs - there isn't necessarily a clear indicator in the architecture that would facilitate this feature. This is described as a long range dependency problem. Furthermore, they were especially impractical due to the vanishing and exploding gradient problems having a larger effect on training. This is by design, as the architecture of the network does not provide the affordance to represent relationships between arbitary cells in the network.

\subsection{LSTMs and GRUs} 

Both gates were introduced to circumvent the issue of long range dependency problems by providing multiple avenues, with each one having a different approach.

\begin{figure}[!ht]
	\centering
	\begin{tikzpicture}[
		prod/.style={circle, draw, inner sep=0pt},
		ct/.style={circle, draw, inner sep=5pt, ultra thick, minimum width=10mm},
		ft/.style={circle, draw, minimum width=8mm, inner sep=1pt},
		filter/.style={circle, draw, minimum width=7mm, inner sep=1pt, path picture={\draw[thick, rounded corners] (path picture bounding box.center)--++(65:2mm)--++(0:1mm);
		\draw[thick, rounded corners] (path picture bounding box.center)--++(245:2mm)--++(180:1mm);}},
		mylabel/.style={font=\scriptsize\sffamily},
		>=LaTeX
		]
	
	\node[ct, label={[mylabel]Cell}] (ct) {$c_t$};
	\node[filter, right=of ct] (int1) {};
	\node[prod, right=of int1] (x1) {$\times$}; 
	\node[right=of x1] (ht) {$h_t$};
	\node[prod, left=of ct] (x2) {$\times$}; 
	\node[filter, left=of x2] (int2) {};
	\node[prod, below=5mm of ct] (x3) {$\times$}; 
	\node[ft, below=5mm of x3, label={[mylabel]right:Forget Gate}] (ft) {$f_t$};
	\node[ft, above=of x2, label={[mylabel]left:Input Gate}] (it) {$i_t$};
	\node[ft, above=of x1, label={[mylabel]left:Output Gate}] (ot) {$o_t$};
	
	\foreach \i/\j in {int2/x2, x2/ct, ct/int1, int1/x1,
				x1/ht, it/x2, ct/it, ct/ot, ot/x1, ft/x3}
		\draw[->] (\i)--(\j);
	
	\draw[->] (ct) to[bend right=45] (ft);
	
	\draw[->] (ct) to[bend right=30] (x3);
	\draw[->] (x3) to[bend right=30] (ct);
	
	\node[fit=(int2) (it) (ot) (ft), draw, inner sep=0pt] (fit) {};
	
	\draw[<-] (fit.west|-int2) coordinate (aux)--++(180:7mm) node[left]{$x_t$};
	\draw[<-] ([yshift=1mm]aux)--++(135:7mm);
	\draw[<-] ([yshift=-1mm]aux)--++(-135:7mm);
	
	\draw[<-] (fit.north-|it) coordinate (aux)--++(90:7mm) node[above]{$x_t$};
	\draw[<-] ([xshift=1mm]aux)--++(45:7mm);
	\draw[<-] ([xshift=-1mm]aux)--++(135:7mm);
	
	\draw[<-] (fit.north-|ot) coordinate (aux)--++(90:7mm) node[above]{$x_t$};
	\draw[<-] ([xshift=1mm]aux)--++(45:7mm);
	\draw[<-] ([xshift=-1mm]aux)--++(135:7mm);
	
	\draw[<-] (fit.south-|ft) coordinate (aux)--++(-90:7mm) node[below]{$x_t$};
	\draw[<-] ([xshift=1mm]aux)--++(-45:7mm);
	\draw[<-] ([xshift=-1mm]aux)--++(-135:7mm);
	\end{tikzpicture}
	\caption{A diagram of the LSTM model.}
\end{figure}
 
LSTM (Long Short Term Memory) is a type of RNN unit that attempts to retain information based on the previous inputs through the introduction of gated architectures. XXXXXXX

Introduced by \cite{cho_properties_2014}, GRUs are used to solve the common issue with LSTMs where the training time was relatively slow due to the number of derivatives necessary to compute. Within the GRU architecture, a feature to retain the previous weights remain, but there exists an direct path to the input data from the output, allowing a reduction in training time. They are unable to clearly distinguish between the performance of the two gated units they tested.
However, Some research from \cite{chung_empirical_2014} suggests that GRUs were found to perform better than LSTMs on smaller datasets. 

\section{Autoencoders}

Autoencoders are a specialised form of MLPs where the model attempts to recreate the inputs on the output. Autoencoders typically have a neural network layer in the model where its dimension is smaller than the input space, therefore representing a dimensionality reduction in the data. Autoencoders are composed of two different principal components, an encoder network $\alpha$ and a decoder network $\beta$, such that $\alpha : X \rightarrow F$ and $\beta : F \rightarrow X$. Measuring the success of the reconstruction is deduced by a reconstruction loss formula. This reconstruction loss comapres the output of the decoder and compares it against the input of the encoder. The two networks are trained together in a manner that allows them to preserve the input as much as possible.
% definitely need a diagram for autoencoders
% what is it good for?

Autoencoders are popularised through their use in Machine Translation, Word Embeddings, and document clustering.

% talk about how important this is in the context of the task.

\subsection{Measuring Performance}

"Bleu measures precision: how much the words (and/or n-grams) in the machine generated summaries appeared in the human reference summaries.

Rouge measures recall: how much the words (and/or n-grams) in the human reference summaries appeared in the machine generated summaries.

Naturally - these results are complementing, as is often the case in precision vs recall. If you have many words from the system results appearing in the human references you will have high Bleu, and if you have many words from the human references appearing in the system results you will have high Rouge."

% https://stackoverflow.com/questions/38045290/text-summarization-evaluation-bleu-vs-rouge

% http://opennmt.net OPENMNT-Py has BLEU/ROUGE on it.

\section{Related Work}


\subsection{Seq2Seq}

Seq2Seq, introduced by \cite{sutskever_sequence_2014} is a type of neural architecture that models relationships between sequences. In our particular scenario, this could be seen as a modern interpretation of the autoencoder. Seq2Seq comprises of two layers - Encoders and Decoders - which are both are represented with RNNs. This allows the architecture to take in arbitarily sized vectors, and allowing it to output similarly arbitary sized vectors. This flexibility makes it especially adventageous to NLP related problems.

\begin{figure}[!ht]
      
	\centering
	\includegraphics[width=100mm]{diagrams/seq2seq.pdf}
	\caption{An abstracted model of the seq2seq architecture, where the encoder (pink) takes in the input sequence, and the decoder (blue) shows the output sequence.\label{seq2seq}}
  \end{figure}

/ something good about seq2seq.

Traditionally, Seq2seq would be the most common method of tackling this particular problem, but it also presents problems that make it suboptimal. Seq2seq models have shown to be effective in reponse generation, but XXXXX

% fix this sentence. why is there lack of variabiity?
% but \cite{serban_hierarchical_2016}; \cite{zhao_learning_2017} suggest that one of the primary causes of the lack of variability of responses could be boiled down to the lack of model variability.
% why? remaining certain patterns?

Furthermore, it was discussed by \cite{jiang_why_2018} on the lack of diversity in responses. something something.

\subsection{Seq2Seq with Attention}

\begin{figure}[!ht]
      
	\centering
	\includegraphics[width=100mm]{diagrams/seq2seq_attention_mechanism.pdf}
	\caption{An abstracted diagram of the attention mechanism, applied to a seq2seq model.\label{seq2seq_attn}}
\end{figure}
% https://guillaumegenthial.github.io/sequence-to-sequence.html

Further work by \cite{bahdanau_neural_2014} improved the original seq2seq model by providing an attention mechanism. The attention mechanism looks at all of the inputs from the hidden states of the encoders so far. This allows the decoder network to "attend" to different parts of the source input at each step of the output generation. This circumvents the need to encode the full source input into a fixed-length vector, helping it deal with long-range dependency problems.

% fix this section
Furthermore, this attention mechanism allows the model to learn what to attend to based on the input sequence and what it has so far, represented through a weighted combination of the two. \cite{bahdanau_neural_2014} have found this to work especially well in machine translation problems where languages which are relatively well aligned (such as English and German) - The decoder is most likely able to choose to attend to the response generation sequentially, such that the first output from the decoder can be generated based on the properties of the first input of the encoder and so on.

% mention the limitations of Seq2seq for your problem and ways they could be addressed (VAEs, etc.), describe the VAE approach and your motivation to pick it, introduce the VAE paper you work with in details.

\subsection{Transformers}

\begin{figure}[!ht]
      
	\centering
	\includegraphics[width=60mm]{diagrams/transformers.png}
	\caption{The transformer model architecture.\cite{vaswani_attention_2017} \label{transformer}}
  \end{figure}

\cite{vaswani_attention_2017} introduced the idea that it is possible to avoid the use of RNNs altogether and focus on leveraging the attention mechanism introduced in seq2seq. The resulting network architecture utilises stacked layers of residual networks for the encoder and the decoder. This adds improvements to computational throughput as sequences can be computed in parallel.

% talk about multihead attention

% taalk about removing recursion/recurrentness of the network
% mention that this will be used as the baseline
% "for text generation techniques, we will use this as the baseline"

\subsubsection{Self Attention}


\begin{align}
\label{eqn:eqlabel}
\begin{split}
	Q = X \cdot W^Q \\
	K = X \cdot W^K \\
	V = X \cdot W^V 
\end{split}
\end{align}

Self attention mechanisms involve three components, a Query $Q$, Key $K$, and Value $V$. 

\begin{equation}
	z = \sigma(\frac{Q \cdot K^T}{\sqrt{d_k}})\cdot V
\end{equation}

The product of multiple $z$ variables concatenated together, with a weights matrix $W^O$ a multihead attention $\boldsymbol{z}' = \lbrack z_1,...,z_n \rbrack \cdot W^O$. 

\begin{equation}
	\boldsymbol{r} = f_{NN}(\boldsymbol{z})
\end{equation}

\subsubsection{Positionality Hack}

Sequential models circumvent the referential ambiguity problems with natural language (such as "the cat is on the mat"), with the inherent properties of time based encodings. This does not necessarily translate well to encoder models as there is no sequentiality. To circumvent this, Transformers encode the positions of words at the embedding level, by adding positional encodings of words.

\section{Variational Autoencoders}

Our proposed solution revolves around the use of Variational Autoencoders. Variational Autoencoders (VAEs) introduce a constraint on the encoding network that forces the model to generate latent vectors that roughly follow a gaussian distribution, as opposed to creating a fixed latent vector. Consequently, VAEs introduce two extra vectors, a mean vector and a standard distribution vector, which is fed from the encoder. A sample is taken from the distribution and that is then fed into the decoder. This consequently allows us to leverage bayesian inference. Model training involves learning the parameters involved for our latent distribution.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=150mm]{diagrams/variational_autoencoders.pdf}
	\caption{A simplified model architecture for a variational autoencoder, which takes as input some text, and it's predicted output being the same text as the input.\label{vae}}
  \end{figure}

  Note that the decoder receives samples from a non-standard normal distribution produced by the encoder. The average of the samples of the different distributions should approximate to a standard normal. This stochasticity allows us to model variability in the results. 
  
  Due to the stochastic nature of the network, the loss is composed of two components;  a reconstruction loss that involves an expection of the output; and a KL divergence, which measures the relative difference of two probability distributions. In this particular case, we will be comparing the distribution of the decoder outputs against a standard gaussian $ \mathcal{N}(0,1)$).
  
  $$\mathcal{L}(\theta, \phi, x, z) = \mathbb{E}_{q \phi (z|x)}[log \thinspace p_{\theta}(x|z)] - D_{KL}(q_{\phi}(z|x)\thinspace||\thinspace p(z)) $$

$$
D_{KL}(P ||Q) = \sum_{x \subset X} P(x) \cdot log (\frac{P(x)}{Q(x)})
$$

In other words, it is the expectation of the logarithmic difference between the probabilities $P$ and $Q$, where the expectation of $P$ is already known.

\subsection*{Reparameterisation Trick}
- You'll need to perform a reparameterisation trick (since you can't calculate derivatives of samples.) in order to perform backpropagation. (you can't push gradients through a sampling node.)
  
$$z = \mu + \sigma \cdot \epsilon $$ where $\epsilon \sim \mathcal{N}(0,1)$. You want to learn $\mu, \sigma$.

\subsection{Conditional Variational Autoencoders}

Although VAEs are more robust when compared to their original autoencoder counterparts, the decoder class cannot produce outputs of a particular class on demand. CVAEs are an improved model of the original VAE architecture by conditioning on another description of the data, a class descriptor $y$. 

\begin{figure}[!ht]
      
	\centering
	\includegraphics[width=150mm]{diagrams/conditional_variational_autoencoders.pdf}
	\caption{A model architecture for a CVAE, which includes the label being fed into the encoder and decoder networks. \label{cvae_diagram}}
  \end{figure}

During training time, a class (represented by some arbitary vector) is fed at the same time to the encoder and decoder. To generate an output that depends on $y$ we feed that number to the decoder along with a random point in the latent space sampled from a standard normal distribution.

Samples can be generated from the conditional distribution $p(x|y)$. By changing the value of $y$, we can get corresponding samples $x \sim p(x|y)$. The system no longer relies on the latent space to encode what output is necessary; instead the latent space encodes other information that can distinguish itself based on the differing $y$ values.

% However, it was found by \cite{du_variational_2018} that since the responses were generated from the same latent variable (induced by the class), it was difficult to model high-variability in responses.

% That being said, using this model alone is not sufficent. If we train the dataset on a vanilla VAE, assuming that the dataset can be encapsulated and modelled in such a manner that allows it to be processed in the model, outputs from the VAE alone will only produce some latent representive of the mean of the training data. 

In terms of implementation, it happens to be more feasible to concate the class variable to the dataset prior to feeding. This allows the model to retain its characteristics without needing to adjust in order to accomadate the conditioning variables.

\section{Variational Autoregressive Decoders}

To increase variability of outcomes produced by the original seq2seq model, \cite{serban_hierarchical_2016}; \cite{zhao_learning_2017} proposed the use of variational autoencoders to seq2seq models. At generation time, the latent variable $z$ can be used as a conditional signal of the decoder in the seq2seq model. Although this introduces variability in the outputs, the variability is not controlled; i.e it is produced from the randomness of $z$. The underlying seq2seq model remains suboptimal.

Introduced by \cite{du_variational_2018}, Variational Autoregressive Decoders (VADs) attempt to circumvent the sampling problem introduced from CVAEs by introducing multiple latent variables into the autoregressive Decoder. At different time-steps, this allows the decoder to produce a multimodal distribution of text sequences, allowing a variety of responses to be produced. 

We will be using this model to help solve the variability of outcomes. *WHY?*

VADs use the seq2seq architecture as the base with variable-length queries $x = \{x_1, x_2, ..., x_n\}$, and $y = \{y_1, y_2, ..., y_n\}$ representing the input and output responses respectively. The encoder network is a Bidirectional RNN with GRUs. The decoder network is an unidirectional RNN with GRUs. For each timestep $t$, each GRU in the decoder network is encoded with hidden state $h^d_t$.

\subsection{Components}
The model is comprised of a variety of components, inspired by the seq2seq network, described earlier.

\subsubsection{Encoder}

\begin{align}
\label{eqn:eqlabel}
\begin{split}
	\overrightarrow{h^e_t} = \overrightarrow{GRU}(x_t, \overrightarrow{h^e_{t-1}})
\\
\overleftarrow{h^e_t} = \overleftarrow{GRU}(x_t, \overleftarrow{h^e_{t+1}})
\end{split}
\end{align}

The encoder works in a similar fashion to the encoder described in the seq2seq network. 

\subsubsection{Backwards}

\begin{align}
\label{eqn:eqlabel}
\overleftarrow{h^d_t} = \overleftarrow{GRU}(y_{t+1}, \overleftarrow{h^d_{t+1}})
\end{align}
	
\subsubsection{Attention}

\begin{equation}
\label{eq:t}
\begin{aligned}
	\alpha_{s,t} &= f_{attention}([h^e_d, h^d_{t-1}])\\        
	c_t &= \sum^m_{s=1}\alpha_{s,t} h^e_s
\end{aligned}
\end{equation}

- Uses Luong's Attention mechanism that involves a concatenation.
- uses a backwards RNN that represents the actual response to better create a posterior distribution.

\subsubsection{Inference Model}


\begin{equation}
\label{eq:t}
\begin{aligned}
\lbrack \mu^i, \sigma^i \rbrack &=
f_{infer}([\overrightarrow{h^d_{t-1}}, c_t, \overleftarrow{h^d_t}])
\\
q_{\theta}(z_t|\boldsymbol{y}, \boldsymbol{x}) &= \mathcal{N}(\mu^i, \sigma^i)
\end{aligned}
\end{equation}

\subsubsection{Prior Model}

\begin{equation}
\label{eq:t}
\begin{aligned}
\lbrack \mu^p, \sigma^p \rbrack &=
f_{infer}([\overrightarrow{h^d_{t-1}}, c_t])
\\
p_{\phi}(z_t|\boldsymbol{y}_{<t}, \boldsymbol{x}) &= \mathcal{N}(\mu^p, \sigma^p)
\end{aligned}
\end{equation}

The prior network is restricted to using observable variables in the testing phase in order to generate $z_t$. It is designed in a similar fashion to the decoder network where the input variables are concatenated together.

\subsubsection{Decoder (Variational Autoregressive)}

\begin{equation}
	\overrightarrow{h^d_t} = \overrightarrow{GRU}([y_{t-1},c_t,z_t], \overrightarrow{h^d_{t-1}})
\end{equation}
\begin{equation}
	p_\phi(y|\boldsymbol{y}_{<t},\boldsymbol{z}_t, \boldsymbol{x}) = f_{output}([\overrightarrow{h^d_t}, c_t])
\end{equation}

This component stems away from \cite{zhao_learning_2017}, as $\boldsymbol{z}$ is now decomposed into sequential variables $\boldsymbol{z} = \{z_1,...,z_t\}$, which are generated at each time step of the decoder phase. $z_t$ is conditioned by the backwards hidden vector $h^d_{t}$. \cite{du_variational_2018} suggests that this conditioning allows the latent variables to be guided for long-term generation.

\subsubsection{Auxillary Objective}

An auxillary objective that attempts to predict the bag of words based on the latent variable. Helps to improve the output options.

\subsubsection{Learning Mechanism}

VADs use a weighted combination of ELBO and the log likelihood loss of the auxillary objective.

\subsubsection{KL Divergence Derivation}

As our model involves using multivariate gaussians, we derive the Kullback-Leibler distance such that we can calculate the loss for our distributions. Notice that we retain the means and covariances for our gaussian distributions. Let $\mu_1, \sigma_1 \rightarrow \mathcal{N}(\mu_1,\sigma_1)$ be our first distribution, and $\mu_2, \sigma_2 \rightarrow \mathcal{N}(\mu_2,\sigma_2)$ be our second distribution. By deriving this, we would be able to calculate a derivative friendly loss function for our models.

\begin{equation}
	\label{eq:t}
	\begin{aligned}
	KL &= \int \left[\log( p(x)) - \log( q(x)) \right]\ p(x)\ dx \\
	&= \frac{1}{2}\left[\log\frac{|\Sigma_2|}{|\Sigma_1|} - d + tr(\Sigma_2^{-1}\Sigma_1) + (\mu_2 - \mu_1)^T \Sigma_2^{-1}(\mu_2 - \mu_1)\right] \\
	&= \int \left[ \frac{1}{2} \log\frac{|\Sigma_2|}{|\Sigma_1|} - \frac{1}{2} (x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1) + \frac{1}{2} (x-\mu_2)^T\Sigma_2^{-1}(x-\mu_2) \right] \times p(x) dx \\
	&= \frac{1}{2} \log\frac{|\Sigma_2|}{|\Sigma_1|} - \frac{1}{2} \text{tr}\ \left\{E[(x - \mu_1)(x - \mu_1)^T] \ \Sigma_1^{-1} \right\} + \frac{1}{2} E[(x - \mu_2)^T \Sigma_2^{-1} (x - \mu_2)] \\
	&= \frac{1}{2} \log\frac{|\Sigma_2|}{|\Sigma_1|} - \frac{1}{2} \text{tr}\ \{I_d \} + \frac{1}{2} (\mu_1 - \mu_2)^T \Sigma_2^{-1} (\mu_1 - \mu_2) + \frac{1}{2} \text{tr} \{ \Sigma_2^{-1} \Sigma_1 \} \\
	&= \frac{1}{2}\left[\log\frac{|\Sigma_2|}{|\Sigma_1|} - d + \text{tr} \{ \Sigma_2^{-1}\Sigma_1 \} + (\mu_2 - \mu_1)^T \Sigma_2^{-1}(\mu_2 - \mu_1)\right].
\end{aligned}
\end{equation}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Contribution}

Although these models have been used in NMT related problems, sequence generation is often unexplored within NLP. We propose using models that traditionally perform well in NMT problems and adapt them for use with sequence generation problems. 

\section{Data Augmentation}

\begin{figure}[!ht]
\centering
\lstinputlisting[language=java]{dataset_1.txt}
\lstinputlisting[language=python]{dataset_2.txt}
\caption{A sample review and the augmented data. Note that for each sequence from line 3 onwards has the identity sequence contatenated before it. \label{aug_1}}
\end{figure}

The dataset produced has to be augmented in order for the model to process them. 

\subsection{Embeddings}

We use the glove word2vec word embeddings.

\subsection{Review Prepropressing}

The model would be forced to produce the next sentence from the previous sentence. Sentences are split by periods. Tokens are represented as words or individual punctuation marks. Additional tags are introduced. Lowercase. 

ASIN represents item ID. this is split by characters.

\subsection{Polarity Calculation}

This is calculated with:

\begin{equation}
p = \sigma_{tanh}(\frac{h_{pos} - h_{neg}}{h_{pos} + h_{neg}})
\end{equation}

The polarity word vector is a sum of the polarity value $p$ and the word vector corresponding to the word "polarity".

\subsection{Implementation}

Built on PyTorch.

\section{Testing Method}

We'll be using a regressor that determines whether the data produced corresponds to a fake model or from the dataset. This is not used as a loss function (this would consequently create a generative model).

We compare the performance of the VAD against two baselines, a seq2seq model described above and the Transformer.

Models are computed on an 4.4GHz intel 3770k with a GTX 1080 with 8GB VRAM.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Experimental Results}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Conclusion}


%% bibliography
\bibliographystyle{apa}
\bibliography{ref} 

\end{document}
