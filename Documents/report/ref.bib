
@misc{musyoku_chainer_2018,
	title = {Chainer implementation of adversarial autoencoder ({AAE}): musyoku/adversarial-autoencoder},
	shorttitle = {Chainer implementation of adversarial autoencoder ({AAE})},
	url = {https://github.com/musyoku/adversarial-autoencoder},
	urldate = {2018-12-27},
	author = {musyoku},
	month = dec,
	year = {2018},
	note = {original-date: 2016-02-23T08:16:41Z}
}

@misc{noauthor_probability_nodate,
	title = {Probability {Distributions}},
	url = {http://students.brown.edu/seeing-theory/probability-distributions},
	abstract = {A probability distribution specifies the relative likelihoods of all possible outcomes.},
	language = {en},
	urldate = {2018-12-27},
	file = {Snapshot:/Users/t/Zotero/storage/4KUWKFEG/index.html:text/html}
}

@inproceedings{du_variational_2018,
	address = {Brussels, Belgium},
	title = {Variational {Autoregressive} {Decoder} for {Neural} {Response} {Generation}},
	url = {http://www.aclweb.org/anthology/D18-1354},
	urldate = {2018-12-27},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Du, Jiachen and Li, Wenjie and He, Yulan and Xu, Ruifeng and Bing, Lidong and Wang, Xuan},
	month = nov,
	year = {2018},
	pages = {3154--3163},
	file = {Full Text PDF:/Users/t/Zotero/storage/QLFI535F/Du et al. - 2018 - Variational Autoregressive Decoder for Neural Resp.pdf:application/pdf}
}

@article{lucas_auxiliary_nodate,
	title = {Auxiliary {Guided} {Autoregressive} {Variational} {Autoencoders}},
	abstract = {Generative modeling of high-dimensional data is a key problem in machine learning. Successful approaches include latent variable models and autoregressive models. The complementary strengths of these approaches, to model global and local image statistics respectively, suggest hybrid models that encode global image structure into latent variables while autoregressively modeling low level detail. Previous approaches to such hybrid models restrict the capacity of the autoregressive decoder to prevent degenerate models that ignore the latent variables and only rely on autoregressive modeling. Our contribution is a training procedure relying on an auxiliary loss function that controls which information is captured by the latent variables and what is left to the autoregressive decoder. Our approach can leverage arbitrarily powerful autoregressive decoders, achieves state-of-the art quantitative performance among models with latent variables, and generates qualitatively convincing samples.},
	language = {en},
	author = {Lucas, Thomas and Verbeek, Jakob},
	pages = {16},
	file = {Lucas and Verbeek - Auxiliary Guided Autoregressive Variational Autoen.pdf:/Users/t/Zotero/storage/7K994SP8/Lucas and Verbeek - Auxiliary Guided Autoregressive Variational Autoen.pdf:application/pdf}
}

@article{le_tutorial_nodate,
	title = {A {Tutorial} on {Deep} {Learning} {Part} 2: {Autoencoders}, {Convolutional} {Neural} {Networks} and {Recurrent} {Neural} {Networks}},
	language = {en},
	author = {Le, Quoc V},
	pages = {20},
	file = {Le - A Tutorial on Deep Learning Part 2 Autoencoders, .pdf:/Users/t/Zotero/storage/395HSEH4/Le - A Tutorial on Deep Learning Part 2 Autoencoders, .pdf:application/pdf}
}

@misc{karpathy_unreasonable_2015,
	title = {The {Unreasonable} {Effectiveness} of {Recurrent} {Neural} {Networks}},
	url = {https://karpathy.github.io/2015/05/21/rnn-effectiveness/},
	urldate = {2018-12-27},
	author = {Karpathy, Andrej},
	month = may,
	year = {2015},
	file = {The Unreasonable Effectiveness of Recurrent Neural Networks:/Users/t/Zotero/storage/KWU3I4E9/rnn-effectiveness.html:text/html}
}

@article{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1409.3215},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	urldate = {2018-12-27},
	journal = {arXiv:1409.3215 [cs]},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.3215},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv\:1409.3215 PDF:/Users/t/Zotero/storage/TKB5QRV5/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/VPP3FLY2/1409.html:text/html}
}

@article{kingma_auto-encoding_2013,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2018-12-27},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1312.6114 PDF:/Users/t/Zotero/storage/7R5U4ZCG/Kingma and Welling - 2013 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/GY6WVJJT/1312.html:text/html}
}

@misc{noauthor_opennmt_nodate,
	title = {{OpenNMT} - {Open}-{Source} {Neural} {Machine} {Translation}},
	url = {http://opennmt.net/},
	urldate = {2018-12-27},
	file = {OpenNMT - Open-Source Neural Machine Translation:/Users/t/Zotero/storage/LS8GXSRX/opennmt.net.html:text/html}
}

@incollection{kingma_semi-supervised_2014,
	title = {Semi-supervised {Learning} with {Deep} {Generative} {Models}},
	url = {http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf},
	urldate = {2018-12-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Kingma, Durk P and Mohamed, Shakir and Jimenez Rezende, Danilo and Welling, Max},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {3581--3589},
	file = {NIPS Full Text PDF:/Users/t/Zotero/storage/DJ27AES2/Kingma et al. - 2014 - Semi-supervised Learning with Deep Generative Mode.pdf:application/pdf;NIPS Snapshot:/Users/t/Zotero/storage/TXNHD7DH/5352-semi-supervised-learning-with-deep-generative-models.html:text/html}
}

@misc{chromiak_transformer_2017,
	title = {The {Transformer} – {Attention} is all you need.},
	url = {https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/},
	abstract = {Transformer - more than meets the eye! Are we there yet? Well... not really, but... How about eliminating recurrence and convolution from transduction? Sequence modeling and transduction (e.g. language modeling, machine translation) problems solutions has been dominated by RNN (especially gated RNN) or LSTM, additionally employing the attention mechanism. Main sequence transduction models are based on RNN or CNN including encoder and decoder. The new transformer architecture is claimed however, to be more parallelizable and requiring significantly less time to train, solely focusing on attention mechanisms.},
	language = {en},
	urldate = {2018-12-28},
	journal = {Michał Chromiak's blog},
	author = {Chromiak, Michał},
	month = sep,
	year = {2017},
	file = {Snapshot:/Users/t/Zotero/storage/WNHDIXJS/Transformer-Attention-is-all-you-need.html:text/html}
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2018-12-28},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv\:1706.03762 PDF:/Users/t/Zotero/storage/9NZR54JH/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/HG9EWFTT/1706.html:text/html}
}

@inproceedings{peng_towards_2018,
	address = {New Orleans, Louisiana},
	title = {Towards {Controllable} {Story} {Generation}},
	url = {http://www.aclweb.org/anthology/W18-1505},
	abstract = {We present a general framework of analyzing existing story corpora to generate controllable and creative new stories. The proposed framework needs little manual annotation to achieve controllable story generation. It creates a new interface for humans to interact with computers to generate personalized stories. We apply the framework to build recurrent neural network (RNN)-based generation models to control story ending valence and storyline. Experiments show that our methods successfully achieve the control and enhance the coherence of stories through introducing storylines. with additional control factors, the generation model gets lower perplexity, and yields more coherent stories that are faithful to the control factors according to human evaluation.},
	urldate = {2018-12-28},
	booktitle = {Proceedings of the {First} {Workshop} on {Storytelling}},
	publisher = {Association for Computational Linguistics},
	author = {Peng, Nanyun and Ghazvininejad, Marjan and May, Jonathan and Knight, Kevin},
	month = jun,
	year = {2018},
	pages = {43--49},
	file = {Full Text PDF:/Users/t/Zotero/storage/FPXDFHHI/Peng et al. - 2018 - Towards Controllable Story Generation.pdf:application/pdf}
}

@misc{dykeman_conditional_nodate,
	title = {Conditional {Variational} {Autoencoders}},
	url = {http://ijdykeman.github.io/ml/2016/12/21/cvae.html},
	urldate = {2018-12-28},
	author = {Dykeman, Issac},
	file = {Conditional Variational Autoencoders:/Users/t/Zotero/storage/YJ3GML4T/cvae.html:text/html}
}

@misc{normandin_conditional_2018,
	title = {Conditional {Variational} {Autoencoder}},
	url = {https://web.archive.org/web/20180417081246/http://nnormandin.com/science/2017/07/01/cvae.html},
	urldate = {2018-12-28},
	author = {Normandin, Nick},
	month = apr,
	year = {2018},
	file = {Conditional Variational Autoencoder:/Users/t/Zotero/storage/9CKR5TRI/cvae.html:text/html}
}

@article{cho_properties_2014,
	title = {On the {Properties} of {Neural} {Machine} {Translation}: {Encoder}-{Decoder} {Approaches}},
	shorttitle = {On the {Properties} of {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1409.1259},
	abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
	urldate = {2018-12-29},
	journal = {arXiv:1409.1259 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.1259},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv\:1409.1259 PDF:/Users/t/Zotero/storage/EZMM3C87/Cho et al. - 2014 - On the Properties of Neural Machine Translation E.pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/FMZNDDTL/1409.html:text/html}
}

@article{chung_empirical_2014,
	title = {Empirical {Evaluation} of {Gated} {Recurrent} {Neural} {Networks} on {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1412.3555},
	abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
	urldate = {2018-12-29},
	journal = {arXiv:1412.3555 [cs]},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.3555},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning},
	file = {arXiv\:1412.3555 PDF:/Users/t/Zotero/storage/QL622TL9/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/CI25DIY9/1412.html:text/html}
}

@article{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2019-01-02},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.0473},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1409.0473 PDF:/Users/t/Zotero/storage/NWWP8J55/Bahdanau et al. - 2014 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/W9PVX9S7/1409.html:text/html}
}

@inproceedings{jiang_why_2018,
	address = {Brussels, Belgium},
	title = {Why are {Sequence}-to-{Sequence} {Models} {So} {Dull}? {Understanding} the {Low}-{Diversity} {Problem} of {Chatbots}},
	shorttitle = {Why are {Sequence}-to-{Sequence} {Models} {So} {Dull}?},
	url = {http://www.aclweb.org/anthology/W18-5712},
	abstract = {Diversity is a long-studied topic in information retrieval that usually refers to the requirement that retrieved results should be non-repetitive and cover different aspects. In a conversational setting, an additional dimension of diversity matters: an engaging response generation system should be able to output responses that are diverse and interesting. Sequence-to-sequence (Seq2Seq) models have been shown to be very effective for response generation. However, dialogue responses generated by Seq2Seq models tend to have low diversity. In this paper, we review known sources and existing approaches to this low-diversity problem. We also identify a source of low diversity that has been little studied so far, namely model over-confidence. We sketch several directions for tackling model over-confidence and, hence, the low-diversity problem, including confidence penalties and label smoothing.},
	urldate = {2019-01-09},
	booktitle = {Proceedings of the 2018 {EMNLP} {Workshop} {SCAI}: {The} 2nd {International} {Workshop} on {Search}-{Oriented} {Conversational} {AI}},
	publisher = {Association for Computational Linguistics},
	author = {Jiang, Shaojie and de Rijke, Maarten},
	month = oct,
	year = {2018},
	pages = {81--86},
	file = {Full Text PDF:/Users/t/Zotero/storage/D6KHEQFH/Jiang and de Rijke - 2018 - Why are Sequence-to-Sequence Models So Dull Under.pdf:application/pdf}
}

@incollection{bengio_neural_2001,
	title = {A {Neural} {Probabilistic} {Language} {Model}},
	url = {http://papers.nips.cc/paper/1839-a-neural-probabilistic-language-model.pdf},
	urldate = {2019-01-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 13},
	publisher = {MIT Press},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal},
	editor = {Leen, T. K. and Dietterich, T. G. and Tresp, V.},
	year = {2001},
	pages = {932--938},
	file = {NIPS Full Text PDF:/Users/t/Zotero/storage/9ETMQCSM/Bengio et al. - 2001 - A Neural Probabilistic Language Model.pdf:application/pdf;NIPS Snapshot:/Users/t/Zotero/storage/V5JI3LYU/1839-a-neural-probabilistic-language-model.html:text/html}
}

@misc{dyer_conditional_2017,
	address = {University of Oxford},
	title = {Conditional {Language} {Modelling}},
	url = {https://github.com/oxford-cs-deepnlp-2017/lectures},
	urldate = {2019-01-10},
	author = {Dyer, Chris},
	month = jan,
	year = {2017},
	note = {original-date: 2017-02-06T11:32:46Z}
}

@misc{le_recurrent_2018,
	title = {Recurrent {Neural} {Networks}: {The} {Powerhouse} of {Language} {Modeling}},
	shorttitle = {Recurrent {Neural} {Networks}},
	url = {https://towardsdatascience.com/recurrent-neural-networks-the-powerhouse-of-language-modeling-d45acc50444f},
	abstract = {Introduction},
	urldate = {2019-01-10},
	journal = {Towards Data Science},
	author = {Le, James},
	month = sep,
	year = {2018},
	file = {Snapshot:/Users/t/Zotero/storage/BBEJG963/Le - 2018 - Recurrent Neural Networks The Powerhouse of Langu.html:text/html}
}

@inproceedings{mcauley_image-based_2015,
	address = {Santiago, Chile},
	title = {Image-{Based} {Recommendations} on {Styles} and {Substitutes}},
	isbn = {978-1-4503-3621-5},
	url = {http://dl.acm.org/citation.cfm?doid=2766462.2767755},
	doi = {10.1145/2766462.2767755},
	abstract = {Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on ﬁne-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem deﬁned on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.},
	language = {en},
	urldate = {2019-01-14},
	booktitle = {Proceedings of the 38th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval} - {SIGIR} '15},
	publisher = {ACM Press},
	author = {McAuley, Julian and Targett, Christopher and Shi, Qinfeng and van den Hengel, Anton},
	year = {2015},
	pages = {43--52},
	file = {McAuley et al. - 2015 - Image-Based Recommendations on Styles and Substitu.pdf:/Users/t/Zotero/storage/4FW9E8EQ/McAuley et al. - 2015 - Image-Based Recommendations on Styles and Substitu.pdf:application/pdf}
}

@book{jurafsky_speech_2019,
	edition = {3},
	title = {Speech and {Language} {Processing}},
	url = {https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf},
	abstract = {An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition.},
	urldate = {2019-01-09},
	author = {Jurafsky, Daniel and Martin, James},
	year = {2019},
	file = {ed3book.pdf:/Users/t/Zotero/storage/DP5U952Q/ed3book.pdf:application/pdf}
}

@inproceedings{wang_hybrid_2018,
	address = {Brussels, Belgium},
	title = {A {Hybrid} {Approach} to {Automatic} {Corpus} {Generation} for {Chinese} {Spelling} {Check}},
	url = {http://aclweb.org/anthology/D18-1273},
	urldate = {2019-01-14},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Dingmin and Song, Yan and Li, Jing and Han, Jialong and Zhang, Haisong},
	year = {2018},
	pages = {2517--2527},
	file = {Full Text PDF:/Users/t/Zotero/storage/3GVW9RMH/Wang et al. - 2018 - A Hybrid Approach to Automatic Corpus Generation f.pdf:application/pdf}
}

@inproceedings{xu_diversity-promoting_2018,
	address = {Brussels, Belgium},
	title = {Diversity-{Promoting} {GAN}: {A} {Cross}-{Entropy} {Based} {Generative} {Adversarial} {Network} for {Diversified} {Text} {Generation}},
	shorttitle = {Diversity-{Promoting} {GAN}},
	url = {http://aclweb.org/anthology/D18-1428},
	urldate = {2019-01-14},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Jingjing and Ren, Xuancheng and Lin, Junyang and Sun, Xu},
	year = {2018},
	pages = {3940--3949},
	file = {Full Text PDF:/Users/t/Zotero/storage/BVCXKBDL/Xu et al. - 2018 - Diversity-Promoting GAN A Cross-Entropy Based Gen.pdf:application/pdf}
}

@inproceedings{baheti_generating_2018,
	address = {Brussels, Belgium},
	title = {Generating {More} {Interesting} {Responses} in {Neural} {Conversation} {Models} with {Distributional} {Constraints}},
	url = {http://aclweb.org/anthology/D18-1431},
	urldate = {2019-01-14},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Baheti, Ashutosh and Ritter, Alan and Li, Jiwei and Dolan, Bill},
	year = {2018},
	pages = {3970--3980},
	file = {Full Text PDF:/Users/t/Zotero/storage/LCD4DYS5/Baheti et al. - 2018 - Generating More Interesting Responses in Neural Co.pdf:application/pdf}
}

@inproceedings{fikri_stylistically_2018,
	address = {Tilburg University, The Netherlands},
	title = {Stylistically {User}-{Specific} {Generation}},
	url = {http://aclweb.org/anthology/W18-6510},
	urldate = {2019-01-14},
	booktitle = {Proceedings of the 11th {International} {Conference} on {Natural} {Language} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Fikri, Abdurrisyad and Takamura, Hiroya and Okumura, Manabu},
	year = {2018},
	pages = {89--98},
	file = {Full Text PDF:/Users/t/Zotero/storage/RBDBGDN7/Fikri et al. - 2018 - Stylistically User-Specific Generation.pdf:application/pdf}
}

@article{serban_hierarchical_2016,
	title = {A {Hierarchical} {Latent} {Variable} {Encoder}-{Decoder} {Model} for {Generating} {Dialogues}},
	url = {http://arxiv.org/abs/1605.06069},
	abstract = {Sequential data often possesses a hierarchical structure with complex dependencies between subsequences, such as found between the utterances in a dialogue. In an effort to model this kind of generative process, we propose a neural network-based generative architecture, with latent stochastic variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare it with recent neural network architectures. We evaluate the model performance through automatic evaluation metrics and by carrying out a human evaluation. The experiments demonstrate that our model improves upon recently proposed models and that the latent variables facilitate the generation of long outputs and maintain the context.},
	urldate = {2019-01-14},
	journal = {arXiv:1605.06069 [cs]},
	author = {Serban, Iulian Vlad and Sordoni, Alessandro and Lowe, Ryan and Charlin, Laurent and Pineau, Joelle and Courville, Aaron and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv: 1605.06069},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.7, I.5.1},
	file = {arXiv\:1605.06069 PDF:/Users/t/Zotero/storage/GRAMVTBJ/Serban et al. - 2016 - A Hierarchical Latent Variable Encoder-Decoder Mod.pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/E5F8DDQE/1605.html:text/html}
}

@article{zhao_learning_2017,
	title = {Learning {Discourse}-level {Diversity} for {Neural} {Dialog} {Models} using {Conditional} {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1703.10960},
	abstract = {While recent neural encoder-decoder models have shown great promise in modeling open-domain conversations, they often generate dull and generic responses. Unlike past work that has focused on diversifying the output of the decoder at word-level to alleviate this problem, we present a novel framework based on conditional variational autoencoders that captures the discourse-level diversity in the encoder. Our model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders. We have further developed a novel variant that is integrated with linguistic prior knowledge for better performance. Finally, the training procedure is improved by introducing a bag-of-word loss. Our proposed models have been validated to generate significantly more diverse responses than baseline approaches and exhibit competence in discourse-level decision-making.},
	urldate = {2019-01-14},
	journal = {arXiv:1703.10960 [cs]},
	author = {Zhao, Tiancheng and Zhao, Ran and Eskenazi, Maxine},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.10960},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv\:1703.10960 PDF:/Users/t/Zotero/storage/JSUESJHK/Zhao et al. - 2017 - Learning Discourse-level Diversity for Neural Dial.pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/CHKCVSPW/1703.html:text/html}
}

@article{bowman_generating_2015,
	title = {Generating {Sentences} from a {Continuous} {Space}},
	url = {http://arxiv.org/abs/1511.06349},
	abstract = {The standard recurrent neural network language model (rnnlm) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an rnn-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the diﬃcult learning problem presented by this model, demonstrate its eﬀectiveness in imputing missing words, explore many interesting properties of the model’s latent sentence space, and present negative results on the use of the model in language modeling.},
	language = {en},
	urldate = {2019-04-11},
	journal = {arXiv:1511.06349 [cs]},
	author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06349},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Bowman et al. - 2015 - Generating Sentences from a Continuous Space.pdf:/Users/t/Zotero/storage/HWDHEYJ8/Bowman et al. - 2015 - Generating Sentences from a Continuous Space.pdf:application/pdf}
}

@article{raiko_techniques_2014,
	title = {Techniques for {Learning} {Binary} {Stochastic} {Feedforward} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1406.2989},
	abstract = {Stochastic binary hidden units in a multi-layer perceptron (MLP) network give at least three potential beneﬁts when compared to deterministic MLP networks. (1) They allow to learn one-to-many type of mappings. (2) They can be used in structured prediction problems, where modeling the internal structure of the output is important. (3) Stochasticity has been shown to be an excellent regularizer, which makes generalization performance potentially better in general. However, training stochastic networks is considerably more difﬁcult. We study training using M samples of hidden activations per input. We show that the case M = 1 leads to a fundamentally different behavior where the network tries to avoid stochasticity. We propose two new estimators for the training gradient and propose benchmark tests for comparing training algorithms. Our experiments conﬁrm that training stochastic networks is difﬁcult and show that the proposed two estimators perform favorably among all the ﬁve known estimators.},
	language = {en},
	urldate = {2019-04-11},
	journal = {arXiv:1406.2989 [cs, stat]},
	author = {Raiko, Tapani and Berglund, Mathias and Alain, Guillaume and Dinh, Laurent},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2989},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Raiko et al. - 2014 - Techniques for Learning Binary Stochastic Feedforw.pdf:/Users/t/Zotero/storage/V838ID8Q/Raiko et al. - 2014 - Techniques for Learning Binary Stochastic Feedforw.pdf:application/pdf}
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2019-04-11},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
	file = {2604.pdf:/Users/t/Zotero/storage/V2QJ4TUY/2604.pdf:application/pdf;Snapshot:/Users/t/Zotero/storage/2X8HWQES/neco.1997.9.8.html:text/html}
}

@article{srivastava_dropout:_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	shorttitle = {Dropout},
	url = {http://jmlr.org/papers/v15/srivastava14a.html},
	urldate = {2019-04-11},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
	file = {Full Text PDF:/Users/t/Zotero/storage/ADLS4SUC/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf;Snapshot:/Users/t/Zotero/storage/AJPRFSKT/srivastava14a.html:text/html;Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:/Users/t/Zotero/storage/ZW85FXMH/Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf}
}

@article{lamb_professor_2016,
	title = {Professor {Forcing}: {A} {New} {Algorithm} for {Training} {Recurrent} {Networks}},
	shorttitle = {Professor {Forcing}},
	url = {http://arxiv.org/abs/1610.09038},
	abstract = {The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.},
	urldate = {2019-04-12},
	journal = {arXiv:1610.09038 [cs, stat]},
	author = {Lamb, Alex and Goyal, Anirudh and Zhang, Ying and Zhang, Saizheng and Courville, Aaron and Bengio, Yoshua},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.09038},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1610.09038 PDF:/Users/t/Zotero/storage/MYXRB8YH/Lamb et al. - 2016 - Professor Forcing A New Algorithm for Training Re.pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/22FQNIVB/1610.html:text/html}
}

@article{williams_learning_1989,
	title = {A {Learning} {Algorithm} for {Continually} {Running} {Fully} {Recurrent} {Neural} {Networks}},
	volume = {1},
	issn = {0899-7667},
	doi = {10.1162/neco.1989.1.2.270},
	abstract = {The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.},
	number = {2},
	journal = {Neural Computation},
	author = {Williams, R. J. and Zipser, D.},
	month = jun,
	year = {1989},
	pages = {270--280},
	file = {IEEE Xplore Abstract Record:/Users/t/Zotero/storage/NA6ZHGG6/6795228.html:text/html}
}

@inproceedings{papineni_bleu:_2001,
	address = {Philadelphia, Pennsylvania},
	title = {{BLEU}: a method for automatic evaluation of machine translation},
	shorttitle = {{BLEU}},
	url = {http://portal.acm.org/citation.cfm?doid=1073083.1073135},
	doi = {10.3115/1073083.1073135},
	language = {en},
	urldate = {2019-04-13},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}  - {ACL} '02},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	year = {2001},
	pages = {311},
	file = {Papineni et al. - 2001 - BLEU a method for automatic evaluation of machine.pdf:/Users/t/Zotero/storage/T6JI7WRI/Papineni et al. - 2001 - BLEU a method for automatic evaluation of machine.pdf:application/pdf}
}

@inproceedings{lin_rouge:_2004,
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	shorttitle = {{ROUGE}},
	url = {https://aclweb.org/anthology/papers/W/W04/W04-1013/},
	language = {en-us},
	urldate = {2019-04-13},
	author = {Lin, Chin-Yew},
	year = {2004},
	pages = {74--81},
	file = {Full Text PDF:/Users/t/Zotero/storage/KLU7HHJC/Lin - 2004 - ROUGE A Package for Automatic Evaluation of Summa.pdf:application/pdf;Snapshot:/Users/t/Zotero/storage/BTZ799GI/W04-1013.html:text/html}
}

@book{bird_natural_nodate,
	title = {Natural {Language} {Processing} with {Python}},
	isbn = {978-0-596-51649-9},
	url = {http://shop.oreilly.com/product/9780596516499.do},
	abstract = {This book offers a highly accessible introduction to Natural Language Processing, the field that underpins a variety of language technologies ranging from predictive text and email filtering to automatic summarization and translation. You'll learn...},
	language = {en},
	urldate = {2019-04-15},
	author = {Bird, Steven and Klein, Ewan and Loper, Edward},
	file = {Snapshot:/Users/t/Zotero/storage/LWFICYPQ/9780596516499.html:text/html}
}

@inproceedings{he_ups_2016,
	address = {Montr\&\#233;al, Qu\&\#233;bec, Canada},
	title = {Ups and {Downs}: {Modeling} the {Visual} {Evolution} of {Fashion} {Trends} with {One}-{Class} {Collaborative} {Filtering}},
	isbn = {978-1-4503-4143-1},
	shorttitle = {Ups and {Downs}},
	url = {http://dl.acm.org/citation.cfm?doid=2872427.2883037},
	doi = {10.1145/2872427.2883037},
	abstract = {Building a successful recommender system depends on understanding both the dimensions of people’s preferences as well as their dynamics. In certain domains, such as fashion, modeling such preferences can be incredibly difﬁcult, due to the need to simultaneously model the visual appearance of products as well as their evolution over time. The subtle semantics and non-linear dynamics of fashion evolution raise unique challenges especially considering the sparsity and large scale of the underlying datasets. In this paper we build novel models for the One-Class Collaborative Filtering setting, where our goal is to estimate users’ fashion-aware personalized ranking functions based on their past feedback. To uncover the complex and evolving visual factors that people consider when evaluating products, our method combines high-level visual features extracted from a deep convolutional neural network, users’ past feedback, as well as evolving trends within the community. Experimentally we evaluate our method on two large real-world datasets from Amazon.com, where we show it to outperform stateof-the-art personalized ranking measures, and also use it to visualize the high-level fashion trends across the 11-year span of our dataset.},
	language = {en},
	urldate = {2019-04-15},
	booktitle = {Proceedings of the 25th {International} {Conference} on {World} {Wide} {Web} - {WWW} '16},
	publisher = {ACM Press},
	author = {He, Ruining and McAuley, Julian},
	year = {2016},
	pages = {507--517},
	file = {He and McAuley - 2016 - Ups and Downs Modeling the Visual Evolution of Fa.pdf:/Users/t/Zotero/storage/ZPIPSRI9/He and McAuley - 2016 - Ups and Downs Modeling the Visual Evolution of Fa.pdf:application/pdf}
}

@article{lison_opensubtitles2016:_2016,
	title = {{OpenSubtitles}2016: {Extracting} {Large} {Parallel} {Corpora} from {Movie} and {TV} {Subtitles}},
	abstract = {We present a new major release of the OpenSubtitles collection of parallel corpora. The release is compiled from a large database of movie and TV subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages. The release also incorporates a number of enhancements in the preprocessing and alignment of the subtitles, such as the automatic correction of OCR errors and the use of meta-data to estimate the quality of each subtitle and score subtitle pairs.},
	language = {en},
	author = {Lison, Pierre and Tiedemann, Jorg},
	year = {2016},
	pages = {7},
	file = {Lison and Tiedemann - OpenSubtitles2016 Extracting Large Parallel Corpo.pdf:/Users/t/Zotero/storage/S3E2FQCQ/Lison and Tiedemann - OpenSubtitles2016 Extracting Large Parallel Corpo.pdf:application/pdf}
}

@article{marcus_building_2002,
	title = {Building a {Large} {Annotated} {Corpus} of {English}: {The} {Penn} {Treebank}},
	volume = {19},
	shorttitle = {Building a {Large} {Annotated} {Corpus} of {English}},
	abstract = {The abstract for this document is available on CSA Illumina.To view the Abstract, click the Abstract button above the document title.},
	journal = {Computational Linguistics},
	author = {Marcus, Mitchell and Ann Marcinkiewicz, Mary and Santorini, Beatrice},
	month = jul,
	year = {2002},
	pages = {313--330},
	file = {Full Text PDF:/Users/t/Zotero/storage/RFQD4XIN/Marcus et al. - 2002 - Building a Large Annotated Corpus of English The .pdf:application/pdf}
}

@misc{honnibal_spacy_2017,
	title = {{spaCy} 2: {Natural} language understanding with {Bloom} embeddings, convolutional neural networks and incremental parsing},
	url = {https://spacy.io/},
	abstract = {spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.},
	author = {Honnibal, Matthew and Montani, Ines},
	year = {2017}
}

@inproceedings{pennington_glove:_2014,
	title = {{GloVe}: {Global} {Vectors} for {Word} {Representation}},
	url = {http://www.aclweb.org/anthology/D14-1162},
	booktitle = {Empirical {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
	year = {2014},
	pages = {1532--1543}
}

@article{kingma_adam:_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {https://arxiv.org/abs/1412.6980v9},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization
of stochastic objective functions, based on adaptive estimates of lower-order
moments. The method is straightforward to implement, is computationally
efficient, has little memory requirements, is invariant to diagonal rescaling
of the gradients, and is well suited for problems that are large in terms of
data and/or parameters. The method is also appropriate for non-stationary
objectives and problems with very noisy and/or sparse gradients. The
hyper-parameters have intuitive interpretations and typically require little
tuning. Some connections to related algorithms, on which Adam was inspired, are
discussed. We also analyze the theoretical convergence properties of the
algorithm and provide a regret bound on the convergence rate that is comparable
to the best known results under the online convex optimization framework.
Empirical results demonstrate that Adam works well in practice and compares
favorably to other stochastic optimization methods. Finally, we discuss AdaMax,
a variant of Adam based on the infinity norm.},
	language = {en},
	urldate = {2019-04-23},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	file = {Full Text PDF:/Users/t/Zotero/storage/D3CH47XB/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;Snapshot:/Users/t/Zotero/storage/LT4HUVZQ/1412.html:text/html}
}

@misc{gpyopt_authors_gpyopt:_2016,
	title = {{GPyOpt}: {A} {Bayesian} {Optimization} framework in python},
	url = {http://github.com/SheffieldML/GPyOpt},
	abstract = {A Bayesian Optimization framework in python, from the University of Sheffield.},
	journal = {GPyOpt},
	author = {{GPyOpt authors}},
	year = {2016}
}

@phdthesis{kovalenko_controllable_2017,
	title = {Controllable text generation},
	url = {http://cs229.stanford.edu/proj2017/final-reports/5244223.pdf},
	abstract = {In last few years generative models advanced greatly in the visual domain.
Proposed solutions to problems like image generation and interpretable image
representation learning achieve very impressive results and advance quickly. Unlike the tasks of text generation. Text generation is a challenging task. Test
samples are discrete and as a result are non-differentiable. This doesnt allow
the use of global discriminator, which are common in the visual domain. For
example in generative adversarial networks used for image generation. An alternative is a variational autoencoder with element-wise reconstruction loss, but
this approach loses the ability to assess generated sentence as a whole. For
controllable text generation, one more challenge is learning disentangled latent
representation. Varying individual elements of latent representation can cause
unpredictable results in the generated sample.
In my project, I work on model described in paper ”Toward Controllable
Text Generation”. The paper proposes a model that addresses issues stated
above. The model is variational autoencoder with an extended wake-sleep procedure and structured latent representation, consisting of vector sampled from
prior distribution and attributes used for imposing desired semantic properties.
Each attribute has dedicated discriminator, which makes learning model, which
produces text samples with desired semantic properties possible.},
	language = {en},
	school = {Stanford University},
	author = {Kovalenko, Boris},
	year = {2017},
	file = {Kovalenko - Controllable text generation.pdf:/Users/t/Zotero/storage/Z645FIQP/Kovalenko - Controllable text generation.pdf:application/pdf}
}

@misc{kovalenko_controllable_2017-1,
	title = {Controllable text generation},
	url = {http://cs229.stanford.edu/proj2017/final-posters/5147211.pdf},
	abstract = {In last few years generative models advanced greatly in the visual domain.
Proposed solutions to problems like image generation and interpretable image
representation learning achieve very impressive results and advance quickly. Unlike the tasks of text generation. Text generation is a challenging task. Test
samples are discrete and as a result are non-differentiable. This doesnt allow
the use of global discriminator, which are common in the visual domain. For
example in generative adversarial networks used for image generation. An alternative is a variational autoencoder with element-wise reconstruction loss, but
this approach loses the ability to assess generated sentence as a whole. For
controllable text generation, one more challenge is learning disentangled latent
representation. Varying individual elements of latent representation can cause
unpredictable results in the generated sample.
In my project, I work on model described in paper ”Toward Controllable
Text Generation”. The paper proposes a model that addresses issues stated
above. The model is variational autoencoder with an extended wake-sleep procedure and structured latent representation, consisting of vector sampled from
prior distribution and attributes used for imposing desired semantic properties.
Each attribute has dedicated discriminator, which makes learning model, which
produces text samples with desired semantic properties possible.},
	language = {en},
	author = {Kovalenko, Boris},
	year = {2017},
	file = {Kovalenko - Controllable text generation.pdf:/Users/t/Zotero/storage/XUAIKGJD/Kovalenko - Controllable text generation.pdf:application/pdf}
}

@article{e._rumelhart_learning_1986,
	title = {Learning {Internal} {Representations} by {Error} {Propagation}},
	volume = {1},
	doi = {10.1016/B978-1-4832-1446-7.50035-2},
	journal = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition},
	author = {E. Rumelhart, D and H. Hinton, G and RJ, Williams},
	month = jul,
	year = {1986}
}

@article{fu*_cyclical_2019,
	title = {Cyclical {Annealing} {Schedule}: {A} {Simple} {Approach} to {Mitigating} {KL} {Vanishing}},
	shorttitle = {Cyclical {Annealing} {Schedule}},
	url = {https://arxiv.org/abs/1903.10145v1},
	abstract = {Variational autoencoders (VAEs) with an auto-regressive decoder have been
applied for many natural language processing (NLP) tasks. The VAE objective
consists of two terms, (i) reconstruction and (ii) KL regularization, balanced
by a weighting hyper-parameter β. One notorious training difficulty is that
the KL term tends to vanish. In this paper we study scheduling schemes for
β, and show that KL vanishing is caused by the lack of good latent codes in
training the decoder at the beginning of optimization.
  To remedy this, we propose a cyclical annealing schedule, which repeats the
process of increasing βmultiple times. This new procedure allows the
progressive learning of more meaningful latent codes, by leveraging the
informative representations of previous cycles as warm re-starts. The
effectiveness of cyclical annealing is validated on a broad range of NLP tasks,
including language modeling, dialog response generation and unsupervised
language pre-training.},
	language = {en},
	urldate = {2019-04-24},
	author = {Fu*, Hao and Li*, Chunyuan and Liu, Xiaodong and Gao, Jianfeng and Celikyilmaz, Asli and Carin, Lawrence},
	month = mar,
	year = {2019},
	file = {Full Text PDF:/Users/t/Zotero/storage/8CP6PM8K/Fu et al. - 2019 - Cyclical Annealing Schedule A Simple Approach to .pdf:application/pdf;Snapshot:/Users/t/Zotero/storage/7WM5BMS6/1903.html:text/html}
}

@inproceedings{glorot_understanding_2010,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	url = {http://proceedings.mlr.press/v9/glorot10a.html},
	abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental resul...},
	language = {en},
	urldate = {2019-04-24},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Glorot, Xavier and Bengio, Yoshua},
	month = mar,
	year = {2010},
	pages = {249--256},
	file = {Full Text PDF:/Users/t/Zotero/storage/CB58KQVG/Glorot and Bengio - 2010 - Understanding the difficulty of training deep feed.pdf:application/pdf;Snapshot:/Users/t/Zotero/storage/82MCEX8K/glorot10a.html:text/html}
}

@inproceedings{kannan_smart_2016,
	address = {San Francisco, California, USA},
	title = {Smart {Reply}: {Automated} {Response} {Suggestion} for {Email}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {Smart {Reply}},
	url = {http://dl.acm.org/citation.cfm?doid=2939672.2939801},
	doi = {10.1145/2939672.2939801},
	abstract = {In this paper we propose and investigate a novel end-to-end method for automatically generating short email responses, called Smart Reply. It generates semantically diverse suggestions that can be used as complete email responses with just one tap on mobile. The system is currently used in Inbox by Gmail and is responsible for assisting with 10\% of all mobile responses. It is designed to work at very high throughput and process hundreds of millions of messages daily. The system exploits state-of-the-art, large-scale deep learning.},
	language = {en},
	urldate = {2019-04-24},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining} - {KDD} '16},
	publisher = {ACM Press},
	author = {Kannan, Anjuli and Young, Peter and Ramavajjala, Vivek and Kurach, Karol and Ravi, Sujith and Kaufmann, Tobias and Tomkins, Andrew and Miklos, Balint and Corrado, Greg and Lukacs, Laszlo and Ganea, Marina},
	year = {2016},
	pages = {955--964},
	file = {Kannan et al. - 2016 - Smart Reply Automated Response Suggestion for Ema.pdf:/Users/t/Zotero/storage/ULBYTHRH/Kannan et al. - 2016 - Smart Reply Automated Response Suggestion for Ema.pdf:application/pdf}
}

@article{nallapati_summarunner_2016,
	title = {{SummaRuNNer} - {A} {Recurrent} {Neural} {Network} based {Sequence} {Model} for {Extractive} {Summarization} of {Documents}},
	shorttitle = {{SummaRuNNer}},
	url = {https://arxiv.org/abs/1611.04230v1},
	abstract = {We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model
for extractive summarization of documents and show that it achieves performance
better than or comparable to state-of-the-art. Our model has the additional
advantage of being very interpretable, since it allows visualization of its
predictions broken up by abstract features such as information content,
salience and novelty. Another novel contribution of our work is abstractive
training of our extractive model that can train on human generated reference
summaries alone, eliminating the need for sentence-level extractive labels.},
	language = {en},
	urldate = {2019-04-24},
	author = {Nallapati, Ramesh and Zhai, Feifei and Zhou, Bowen},
	year = {2016},
	file = {Full Text PDF:/Users/t/Zotero/storage/C6DSKUXP/Nallapati et al. - 2016 - SummaRuNNer A Recurrent Neural Network based Sequ.pdf:application/pdf;Snapshot:/Users/t/Zotero/storage/RPXSHYM8/1611.html:text/html}
}

@article{noauthor_cs546:learning_nodate,
	title = {{CS}546:{Learning} and {NLP}   {Lec} 6: {Ngrams} and {Backoff} {Models}},
	language = {en},
	pages = {45},
	file = {CS546Learning and NLP   Lec 6 Ngrams and Backoff.pdf:/Users/t/Zotero/storage/HI3MAWJ5/CS546Learning and NLP   Lec 6 Ngrams and Backoff.pdf:application/pdf}
}

@misc{genthial_seq2seq_2017,
	title = {Seq2Seq with {Attention} and {Beam} {Search}},
	url = {https://guillaumegenthial.github.io/sequence-to-sequence.html},
	abstract = {Sequence to Sequence basics for Neural Machine Translation using Attention and Beam Search},
	language = {en},
	urldate = {2019-04-27},
	journal = {Guillaume Genthial blog},
	author = {Genthial, Guillaume},
	month = nov,
	year = {2017},
	file = {Snapshot:/Users/t/Zotero/storage/TUULMIUZ/sequence-to-sequence.html:text/html}
}