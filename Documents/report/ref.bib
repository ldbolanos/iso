
@misc{musyoku_chainer_2018,
	title = {Chainer implementation of adversarial autoencoder ({AAE}): musyoku/adversarial-autoencoder},
	shorttitle = {Chainer implementation of adversarial autoencoder ({AAE})},
	url = {https://github.com/musyoku/adversarial-autoencoder},
	urldate = {2018-12-27},
	author = {musyoku},
	month = dec,
	year = {2018},
	note = {original-date: 2016-02-23T08:16:41Z}
}

@misc{noauthor_probability_nodate,
	title = {Probability {Distributions}},
	url = {http://students.brown.edu/seeing-theory/probability-distributions},
	abstract = {A probability distribution specifies the relative likelihoods of all possible outcomes.},
	language = {en},
	urldate = {2018-12-27},
	file = {Snapshot:/Users/t/Zotero/storage/4KUWKFEG/index.html:text/html}
}

@inproceedings{du_variational_2018,
	address = {Brussels, Belgium},
	title = {Variational {Autoregressive} {Decoder} for {Neural} {Response} {Generation}},
	url = {http://www.aclweb.org/anthology/D18-1354},
	urldate = {2018-12-27},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Du, Jiachen and Li, Wenjie and He, Yulan and Xu, Ruifeng and Bing, Lidong and Wang, Xuan},
	month = nov,
	year = {2018},
	pages = {3154--3163},
	file = {Full Text PDF:/Users/t/Zotero/storage/QLFI535F/Du et al. - 2018 - Variational Autoregressive Decoder for Neural Resp.pdf:application/pdf}
}

@article{lucas_auxiliary_nodate,
	title = {Auxiliary {Guided} {Autoregressive} {Variational} {Autoencoders}},
	abstract = {Generative modeling of high-dimensional data is a key problem in machine learning. Successful approaches include latent variable models and autoregressive models. The complementary strengths of these approaches, to model global and local image statistics respectively, suggest hybrid models that encode global image structure into latent variables while autoregressively modeling low level detail. Previous approaches to such hybrid models restrict the capacity of the autoregressive decoder to prevent degenerate models that ignore the latent variables and only rely on autoregressive modeling. Our contribution is a training procedure relying on an auxiliary loss function that controls which information is captured by the latent variables and what is left to the autoregressive decoder. Our approach can leverage arbitrarily powerful autoregressive decoders, achieves state-of-the art quantitative performance among models with latent variables, and generates qualitatively convincing samples.},
	language = {en},
	author = {Lucas, Thomas and Verbeek, Jakob},
	pages = {16},
	file = {Lucas and Verbeek - Auxiliary Guided Autoregressive Variational Autoen.pdf:/Users/t/Zotero/storage/7K994SP8/Lucas and Verbeek - Auxiliary Guided Autoregressive Variational Autoen.pdf:application/pdf}
}

@article{le_tutorial_nodate,
	title = {A {Tutorial} on {Deep} {Learning} {Part} 2: {Autoencoders}, {Convolutional} {Neural} {Networks} and {Recurrent} {Neural} {Networks}},
	language = {en},
	author = {Le, Quoc V},
	pages = {20},
	file = {Le - A Tutorial on Deep Learning Part 2 Autoencoders, .pdf:/Users/t/Zotero/storage/395HSEH4/Le - A Tutorial on Deep Learning Part 2 Autoencoders, .pdf:application/pdf}
}

@misc{karpathy_unreasonable_2015,
	title = {The {Unreasonable} {Effectiveness} of {Recurrent} {Neural} {Networks}},
	url = {https://karpathy.github.io/2015/05/21/rnn-effectiveness/},
	urldate = {2018-12-27},
	author = {Karpathy, Andrej},
	month = may,
	year = {2015},
	file = {The Unreasonable Effectiveness of Recurrent Neural Networks:/Users/t/Zotero/storage/KWU3I4E9/rnn-effectiveness.html:text/html}
}

@article{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1409.3215},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	urldate = {2018-12-27},
	journal = {arXiv:1409.3215 [cs]},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.3215},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv\:1409.3215 PDF:/Users/t/Zotero/storage/TKB5QRV5/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/VPP3FLY2/1409.html:text/html}
}

@article{kingma_auto-encoding_2013,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2018-12-27},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1312.6114 PDF:/Users/t/Zotero/storage/7R5U4ZCG/Kingma and Welling - 2013 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/GY6WVJJT/1312.html:text/html}
}

@misc{noauthor_opennmt_nodate,
	title = {{OpenNMT} - {Open}-{Source} {Neural} {Machine} {Translation}},
	url = {http://opennmt.net/},
	urldate = {2018-12-27},
	file = {OpenNMT - Open-Source Neural Machine Translation:/Users/t/Zotero/storage/LS8GXSRX/opennmt.net.html:text/html}
}

@incollection{kingma_semi-supervised_2014,
	title = {Semi-supervised {Learning} with {Deep} {Generative} {Models}},
	url = {http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf},
	urldate = {2018-12-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Kingma, Durk P and Mohamed, Shakir and Jimenez Rezende, Danilo and Welling, Max},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {3581--3589},
	file = {NIPS Full Text PDF:/Users/t/Zotero/storage/DJ27AES2/Kingma et al. - 2014 - Semi-supervised Learning with Deep Generative Mode.pdf:application/pdf;NIPS Snapshot:/Users/t/Zotero/storage/TXNHD7DH/5352-semi-supervised-learning-with-deep-generative-models.html:text/html}
}

@misc{chromiak_transformer_2017,
	title = {The {Transformer} – {Attention} is all you need.},
	url = {https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/},
	abstract = {Transformer - more than meets the eye! Are we there yet? Well... not really, but... How about eliminating recurrence and convolution from transduction? Sequence modeling and transduction (e.g. language modeling, machine translation) problems solutions has been dominated by RNN (especially gated RNN) or LSTM, additionally employing the attention mechanism. Main sequence transduction models are based on RNN or CNN including encoder and decoder. The new transformer architecture is claimed however, to be more parallelizable and requiring significantly less time to train, solely focusing on attention mechanisms.},
	language = {en},
	urldate = {2018-12-28},
	journal = {Michał Chromiak's blog},
	author = {Chromiak, Michał},
	month = sep,
	year = {2017},
	file = {Snapshot:/Users/t/Zotero/storage/WNHDIXJS/Transformer-Attention-is-all-you-need.html:text/html}
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2018-12-28},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv\:1706.03762 PDF:/Users/t/Zotero/storage/9NZR54JH/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/HG9EWFTT/1706.html:text/html}
}

@inproceedings{peng_towards_2018,
	address = {New Orleans, Louisiana},
	title = {Towards {Controllable} {Story} {Generation}},
	url = {http://www.aclweb.org/anthology/W18-1505},
	abstract = {We present a general framework of analyzing existing story corpora to generate controllable and creative new stories. The proposed framework needs little manual annotation to achieve controllable story generation. It creates a new interface for humans to interact with computers to generate personalized stories. We apply the framework to build recurrent neural network (RNN)-based generation models to control story ending valence and storyline. Experiments show that our methods successfully achieve the control and enhance the coherence of stories through introducing storylines. with additional control factors, the generation model gets lower perplexity, and yields more coherent stories that are faithful to the control factors according to human evaluation.},
	urldate = {2018-12-28},
	booktitle = {Proceedings of the {First} {Workshop} on {Storytelling}},
	publisher = {Association for Computational Linguistics},
	author = {Peng, Nanyun and Ghazvininejad, Marjan and May, Jonathan and Knight, Kevin},
	month = jun,
	year = {2018},
	pages = {43--49},
	file = {Full Text PDF:/Users/t/Zotero/storage/FPXDFHHI/Peng et al. - 2018 - Towards Controllable Story Generation.pdf:application/pdf}
}

@misc{dykeman_conditional_nodate,
	title = {Conditional {Variational} {Autoencoders}},
	url = {http://ijdykeman.github.io/ml/2016/12/21/cvae.html},
	urldate = {2018-12-28},
	author = {Dykeman, Issac},
	file = {Conditional Variational Autoencoders:/Users/t/Zotero/storage/YJ3GML4T/cvae.html:text/html}
}

@misc{normandin_conditional_2018,
	title = {Conditional {Variational} {Autoencoder}},
	url = {https://web.archive.org/web/20180417081246/http://nnormandin.com/science/2017/07/01/cvae.html},
	urldate = {2018-12-28},
	author = {Normandin, Nick},
	month = apr,
	year = {2018},
	file = {Conditional Variational Autoencoder:/Users/t/Zotero/storage/9CKR5TRI/cvae.html:text/html}
}

@article{cho_properties_2014,
	title = {On the {Properties} of {Neural} {Machine} {Translation}: {Encoder}-{Decoder} {Approaches}},
	shorttitle = {On the {Properties} of {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1409.1259},
	abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
	urldate = {2018-12-29},
	journal = {arXiv:1409.1259 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.1259},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv\:1409.1259 PDF:/Users/t/Zotero/storage/EZMM3C87/Cho et al. - 2014 - On the Properties of Neural Machine Translation E.pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/FMZNDDTL/1409.html:text/html}
}

@article{chung_empirical_2014,
	title = {Empirical {Evaluation} of {Gated} {Recurrent} {Neural} {Networks} on {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1412.3555},
	abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
	urldate = {2018-12-29},
	journal = {arXiv:1412.3555 [cs]},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.3555},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning},
	file = {arXiv\:1412.3555 PDF:/Users/t/Zotero/storage/QL622TL9/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/CI25DIY9/1412.html:text/html}
}

@article{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2019-01-02},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.0473},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1409.0473 PDF:/Users/t/Zotero/storage/NWWP8J55/Bahdanau et al. - 2014 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/W9PVX9S7/1409.html:text/html}
}

@inproceedings{jiang_why_2018,
	address = {Brussels, Belgium},
	title = {Why are {Sequence}-to-{Sequence} {Models} {So} {Dull}? {Understanding} the {Low}-{Diversity} {Problem} of {Chatbots}},
	shorttitle = {Why are {Sequence}-to-{Sequence} {Models} {So} {Dull}?},
	url = {http://www.aclweb.org/anthology/W18-5712},
	abstract = {Diversity is a long-studied topic in information retrieval that usually refers to the requirement that retrieved results should be non-repetitive and cover different aspects. In a conversational setting, an additional dimension of diversity matters: an engaging response generation system should be able to output responses that are diverse and interesting. Sequence-to-sequence (Seq2Seq) models have been shown to be very effective for response generation. However, dialogue responses generated by Seq2Seq models tend to have low diversity. In this paper, we review known sources and existing approaches to this low-diversity problem. We also identify a source of low diversity that has been little studied so far, namely model over-confidence. We sketch several directions for tackling model over-confidence and, hence, the low-diversity problem, including confidence penalties and label smoothing.},
	urldate = {2019-01-09},
	booktitle = {Proceedings of the 2018 {EMNLP} {Workshop} {SCAI}: {The} 2nd {International} {Workshop} on {Search}-{Oriented} {Conversational} {AI}},
	publisher = {Association for Computational Linguistics},
	author = {Jiang, Shaojie and de Rijke, Maarten},
	month = oct,
	year = {2018},
	pages = {81--86},
	file = {Full Text PDF:/Users/t/Zotero/storage/D6KHEQFH/Jiang and de Rijke - 2018 - Why are Sequence-to-Sequence Models So Dull Under.pdf:application/pdf}
}

@incollection{bengio_neural_2001,
	title = {A {Neural} {Probabilistic} {Language} {Model}},
	url = {http://papers.nips.cc/paper/1839-a-neural-probabilistic-language-model.pdf},
	urldate = {2019-01-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 13},
	publisher = {MIT Press},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal},
	editor = {Leen, T. K. and Dietterich, T. G. and Tresp, V.},
	year = {2001},
	pages = {932--938},
	file = {NIPS Full Text PDF:/Users/t/Zotero/storage/9ETMQCSM/Bengio et al. - 2001 - A Neural Probabilistic Language Model.pdf:application/pdf;NIPS Snapshot:/Users/t/Zotero/storage/V5JI3LYU/1839-a-neural-probabilistic-language-model.html:text/html}
}

@misc{dyer_conditional_2017,
	address = {University of Oxford},
	title = {Conditional {Language} {Modelling}},
	url = {https://github.com/oxford-cs-deepnlp-2017/lectures},
	urldate = {2019-01-10},
	author = {Dyer, Chris},
	month = jan,
	year = {2017},
	note = {original-date: 2017-02-06T11:32:46Z}
}

@misc{noauthor_seq2seq_2017,
	title = {Seq2Seq with {Attention} and {Beam} {Search}},
	url = {https://guillaumegenthial.github.io/sequence-to-sequence.html},
	abstract = {Sequence to Sequence basics for Neural Machine Translation using Attention and Beam Search},
	language = {en},
	urldate = {2019-01-10},
	journal = {Guillaume Genthial blog},
	month = nov,
	year = {2017},
	file = {Snapshot:/Users/t/Zotero/storage/BT9PVEXS/2017 - Seq2Seq with Attention and Beam Search.html:text/html}
}

@misc{le_recurrent_2018,
	title = {Recurrent {Neural} {Networks}: {The} {Powerhouse} of {Language} {Modeling}},
	shorttitle = {Recurrent {Neural} {Networks}},
	url = {https://towardsdatascience.com/recurrent-neural-networks-the-powerhouse-of-language-modeling-d45acc50444f},
	abstract = {Introduction},
	urldate = {2019-01-10},
	journal = {Towards Data Science},
	author = {Le, James},
	month = sep,
	year = {2018},
	file = {Snapshot:/Users/t/Zotero/storage/BBEJG963/Le - 2018 - Recurrent Neural Networks The Powerhouse of Langu.html:text/html}
}

@inproceedings{he_ups_2016,
	address = {Montr\&\#233;al, Qu\&\#233;bec, Canada},
	title = {Ups and {Downs}: {Modeling} the {Visual} {Evolution} of {Fashion} {Trends} with {One}-{Class} {Collaborative} {Filtering}},
	isbn = {978-1-4503-4143-1},
	shorttitle = {Ups and {Downs}},
	url = {http://dl.acm.org/citation.cfm?doid=2872427.2883037},
	doi = {10.1145/2872427.2883037},
	abstract = {Building a successful recommender system depends on understanding both the dimensions of people’s preferences as well as their dynamics. In certain domains, such as fashion, modeling such preferences can be incredibly difﬁcult, due to the need to simultaneously model the visual appearance of products as well as their evolution over time. The subtle semantics and non-linear dynamics of fashion evolution raise unique challenges especially considering the sparsity and large scale of the underlying datasets. In this paper we build novel models for the One-Class Collaborative Filtering setting, where our goal is to estimate users’ fashion-aware personalized ranking functions based on their past feedback. To uncover the complex and evolving visual factors that people consider when evaluating products, our method combines high-level visual features extracted from a deep convolutional neural network, users’ past feedback, as well as evolving trends within the community. Experimentally we evaluate our method on two large real-world datasets from Amazon.com, where we show it to outperform stateof-the-art personalized ranking measures, and also use it to visualize the high-level fashion trends across the 11-year span of our dataset.},
	language = {en},
	urldate = {2019-01-14},
	booktitle = {Proceedings of the 25th {International} {Conference} on {World} {Wide} {Web} - {WWW} '16},
	publisher = {ACM Press},
	author = {He, Ruining and McAuley, Julian},
	year = {2016},
	pages = {507--517},
	file = {He and McAuley - 2016 - Ups and Downs Modeling the Visual Evolution of Fa.pdf:/Users/t/Zotero/storage/S6P3GMIZ/He and McAuley - 2016 - Ups and Downs Modeling the Visual Evolution of Fa.pdf:application/pdf}
}

@inproceedings{mcauley_image-based_2015,
	address = {Santiago, Chile},
	title = {Image-{Based} {Recommendations} on {Styles} and {Substitutes}},
	isbn = {978-1-4503-3621-5},
	url = {http://dl.acm.org/citation.cfm?doid=2766462.2767755},
	doi = {10.1145/2766462.2767755},
	abstract = {Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on ﬁne-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem deﬁned on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.},
	language = {en},
	urldate = {2019-01-14},
	booktitle = {Proceedings of the 38th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval} - {SIGIR} '15},
	publisher = {ACM Press},
	author = {McAuley, Julian and Targett, Christopher and Shi, Qinfeng and van den Hengel, Anton},
	year = {2015},
	pages = {43--52},
	file = {McAuley et al. - 2015 - Image-Based Recommendations on Styles and Substitu.pdf:/Users/t/Zotero/storage/4FW9E8EQ/McAuley et al. - 2015 - Image-Based Recommendations on Styles and Substitu.pdf:application/pdf}
}

@book{jurafsky_speech_nodate,
	edition = {3},
	title = {Speech and {Language} {Processing}},
	url = {https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf},
	abstract = {An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition.},
	urldate = {2019-01-09},
	author = {Jurafsky, Daniel and Martin, James},
	file = {ed3book.pdf:/Users/t/Zotero/storage/DP5U952Q/ed3book.pdf:application/pdf}
}

@inproceedings{wang_hybrid_2018,
	address = {Brussels, Belgium},
	title = {A {Hybrid} {Approach} to {Automatic} {Corpus} {Generation} for {Chinese} {Spelling} {Check}},
	url = {http://aclweb.org/anthology/D18-1273},
	urldate = {2019-01-14},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Dingmin and Song, Yan and Li, Jing and Han, Jialong and Zhang, Haisong},
	year = {2018},
	pages = {2517--2527},
	file = {Full Text PDF:/Users/t/Zotero/storage/3GVW9RMH/Wang et al. - 2018 - A Hybrid Approach to Automatic Corpus Generation f.pdf:application/pdf}
}

@inproceedings{xu_diversity-promoting_2018,
	address = {Brussels, Belgium},
	title = {Diversity-{Promoting} {GAN}: {A} {Cross}-{Entropy} {Based} {Generative} {Adversarial} {Network} for {Diversified} {Text} {Generation}},
	shorttitle = {Diversity-{Promoting} {GAN}},
	url = {http://aclweb.org/anthology/D18-1428},
	urldate = {2019-01-14},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Jingjing and Ren, Xuancheng and Lin, Junyang and Sun, Xu},
	year = {2018},
	pages = {3940--3949},
	file = {Full Text PDF:/Users/t/Zotero/storage/BVCXKBDL/Xu et al. - 2018 - Diversity-Promoting GAN A Cross-Entropy Based Gen.pdf:application/pdf}
}

@inproceedings{baheti_generating_2018,
	address = {Brussels, Belgium},
	title = {Generating {More} {Interesting} {Responses} in {Neural} {Conversation} {Models} with {Distributional} {Constraints}},
	url = {http://aclweb.org/anthology/D18-1431},
	urldate = {2019-01-14},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Baheti, Ashutosh and Ritter, Alan and Li, Jiwei and Dolan, Bill},
	year = {2018},
	pages = {3970--3980},
	file = {Full Text PDF:/Users/t/Zotero/storage/LCD4DYS5/Baheti et al. - 2018 - Generating More Interesting Responses in Neural Co.pdf:application/pdf}
}

@inproceedings{fikri_stylistically_2018,
	address = {Tilburg University, The Netherlands},
	title = {Stylistically {User}-{Specific} {Generation}},
	url = {http://aclweb.org/anthology/W18-6510},
	urldate = {2019-01-14},
	booktitle = {Proceedings of the 11th {International} {Conference} on {Natural} {Language} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Fikri, Abdurrisyad and Takamura, Hiroya and Okumura, Manabu},
	year = {2018},
	pages = {89--98},
	file = {Full Text PDF:/Users/t/Zotero/storage/RBDBGDN7/Fikri et al. - 2018 - Stylistically User-Specific Generation.pdf:application/pdf}
}

@article{serban_hierarchical_2016,
	title = {A {Hierarchical} {Latent} {Variable} {Encoder}-{Decoder} {Model} for {Generating} {Dialogues}},
	url = {http://arxiv.org/abs/1605.06069},
	abstract = {Sequential data often possesses a hierarchical structure with complex dependencies between subsequences, such as found between the utterances in a dialogue. In an effort to model this kind of generative process, we propose a neural network-based generative architecture, with latent stochastic variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare it with recent neural network architectures. We evaluate the model performance through automatic evaluation metrics and by carrying out a human evaluation. The experiments demonstrate that our model improves upon recently proposed models and that the latent variables facilitate the generation of long outputs and maintain the context.},
	urldate = {2019-01-14},
	journal = {arXiv:1605.06069 [cs]},
	author = {Serban, Iulian Vlad and Sordoni, Alessandro and Lowe, Ryan and Charlin, Laurent and Pineau, Joelle and Courville, Aaron and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv: 1605.06069},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, I.2.7, I.5.1},
	file = {arXiv\:1605.06069 PDF:/Users/t/Zotero/storage/GRAMVTBJ/Serban et al. - 2016 - A Hierarchical Latent Variable Encoder-Decoder Mod.pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/E5F8DDQE/1605.html:text/html}
}

@article{zhao_learning_2017,
	title = {Learning {Discourse}-level {Diversity} for {Neural} {Dialog} {Models} using {Conditional} {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1703.10960},
	abstract = {While recent neural encoder-decoder models have shown great promise in modeling open-domain conversations, they often generate dull and generic responses. Unlike past work that has focused on diversifying the output of the decoder at word-level to alleviate this problem, we present a novel framework based on conditional variational autoencoders that captures the discourse-level diversity in the encoder. Our model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders. We have further developed a novel variant that is integrated with linguistic prior knowledge for better performance. Finally, the training procedure is improved by introducing a bag-of-word loss. Our proposed models have been validated to generate significantly more diverse responses than baseline approaches and exhibit competence in discourse-level decision-making.},
	urldate = {2019-01-14},
	journal = {arXiv:1703.10960 [cs]},
	author = {Zhao, Tiancheng and Zhao, Ran and Eskenazi, Maxine},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.10960},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}