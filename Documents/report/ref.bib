
@misc{musyoku_chainer_2018,
	title = {Chainer implementation of adversarial autoencoder ({AAE}): musyoku/adversarial-autoencoder},
	shorttitle = {Chainer implementation of adversarial autoencoder ({AAE})},
	url = {https://github.com/musyoku/adversarial-autoencoder},
	urldate = {2018-12-27},
	author = {musyoku},
	month = dec,
	year = {2018},
	note = {original-date: 2016-02-23T08:16:41Z}
}

@misc{noauthor_probability_nodate,
	title = {Probability {Distributions}},
	url = {http://students.brown.edu/seeing-theory/probability-distributions},
	abstract = {A probability distribution specifies the relative likelihoods of all possible outcomes.},
	language = {en},
	urldate = {2018-12-27},
	file = {Snapshot:/Users/t/Zotero/storage/4KUWKFEG/index.html:text/html}
}

@inproceedings{du_variational_2018,
	address = {Brussels, Belgium},
	title = {Variational {Autoregressive} {Decoder} for {Neural} {Response} {Generation}},
	url = {http://www.aclweb.org/anthology/D18-1354},
	urldate = {2018-12-27},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Du, Jiachen and Li, Wenjie and He, Yulan and Xu, Ruifeng and Bing, Lidong and Wang, Xuan},
	month = nov,
	year = {2018},
	pages = {3154--3163},
	file = {Full Text PDF:/Users/t/Zotero/storage/QLFI535F/Du et al. - 2018 - Variational Autoregressive Decoder for Neural Resp.pdf:application/pdf}
}

@article{lucas_auxiliary_nodate,
	title = {Auxiliary {Guided} {Autoregressive} {Variational} {Autoencoders}},
	abstract = {Generative modeling of high-dimensional data is a key problem in machine learning. Successful approaches include latent variable models and autoregressive models. The complementary strengths of these approaches, to model global and local image statistics respectively, suggest hybrid models that encode global image structure into latent variables while autoregressively modeling low level detail. Previous approaches to such hybrid models restrict the capacity of the autoregressive decoder to prevent degenerate models that ignore the latent variables and only rely on autoregressive modeling. Our contribution is a training procedure relying on an auxiliary loss function that controls which information is captured by the latent variables and what is left to the autoregressive decoder. Our approach can leverage arbitrarily powerful autoregressive decoders, achieves state-of-the art quantitative performance among models with latent variables, and generates qualitatively convincing samples.},
	language = {en},
	author = {Lucas, Thomas and Verbeek, Jakob},
	pages = {16},
	file = {Lucas and Verbeek - Auxiliary Guided Autoregressive Variational Autoen.pdf:/Users/t/Zotero/storage/7K994SP8/Lucas and Verbeek - Auxiliary Guided Autoregressive Variational Autoen.pdf:application/pdf}
}

@article{le_tutorial_nodate,
	title = {A {Tutorial} on {Deep} {Learning} {Part} 2: {Autoencoders}, {Convolutional} {Neural} {Networks} and {Recurrent} {Neural} {Networks}},
	language = {en},
	author = {Le, Quoc V},
	pages = {20},
	file = {Le - A Tutorial on Deep Learning Part 2 Autoencoders, .pdf:/Users/t/Zotero/storage/395HSEH4/Le - A Tutorial on Deep Learning Part 2 Autoencoders, .pdf:application/pdf}
}

@misc{noauthor_unreasonable_nodate,
	title = {The {Unreasonable} {Effectiveness} of {Recurrent} {Neural} {Networks}},
	url = {https://karpathy.github.io/2015/05/21/rnn-effectiveness/},
	urldate = {2018-12-27},
	file = {The Unreasonable Effectiveness of Recurrent Neural Networks:/Users/t/Zotero/storage/KWU3I4E9/rnn-effectiveness.html:text/html}
}

@article{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1409.3215},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	urldate = {2018-12-27},
	journal = {arXiv:1409.3215 [cs]},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.3215},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv\:1409.3215 PDF:/Users/t/Zotero/storage/TKB5QRV5/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/t/Zotero/storage/VPP3FLY2/1409.html:text/html}
}