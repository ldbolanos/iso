{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import bcolz\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "# plotting\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from vad import batchData\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hiddenSize': 512, 'latentSize': 400, 'batchSize': 64, 'iterations': 200, 'learningRate': 0.0001, 'gradientClip': 3, 'useBOW': True, 'bidirectionalEncoder': True, 'reduction': 256, 'device': 'cuda', 'useLatent': True}\n"
     ]
    }
   ],
   "source": [
    "model_path = \"models/20190411 17-58-12\"\n",
    "param_path = os.path.join(model_path,\"model_parameters.json\")\n",
    "with open(param_path) as json_file:  \n",
    "    params = json.load(json_file)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Set seed #\n",
    "\"\"\"\n",
    "seed = 1337\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "def loadDataset(path = '../Datasets/Reviews/dataset_ready.pkl'):\n",
    "    return pickle.load(open(path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parameters.. Done.\n",
      "Loading dataset.. Done.\n",
      "Converting dataset weights into tensors.. Done.\n",
      "Batching Data.. Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading parameters..\", end=\" \")\n",
    "batchSize  = 10\n",
    "iterations = 1\n",
    "bidirectionalEncoder = params['bidirectionalEncoder']\n",
    "# device = \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Loading dataset..\", end=\" \")\n",
    "dataset = loadDataset()\n",
    "# setup store parameters\n",
    "id2word = dataset['id2word']\n",
    "word2id = dataset['word2id']\n",
    "weightMatrix = dataset['weights']\n",
    "train = dataset['train']\n",
    "validation = dataset['validation']\n",
    "cutoff = dataset['cutoff']\n",
    "paddingID = word2id['<pad>']\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Converting dataset weights into tensors..\", end=\" \")\n",
    "# convert dataset into tensors\n",
    "weightMatrix = torch.tensor(weightMatrix, dtype=torch.float)\n",
    "print(\"Done.\")\n",
    "\n",
    "# batching data\n",
    "print(\"Batching Data..\",end=\" \")\n",
    "\n",
    "random.shuffle(validation)\n",
    "\n",
    "trainx = [x[0] for x in train]\n",
    "trainy = [x[1] for x in train]\n",
    "valx = [x[0] for x in validation]\n",
    "valy = [x[1] for x in validation]\n",
    "\n",
    "trainx = batchData(trainx, paddingID, device, batchSize, cutoff)\n",
    "trainy = batchData(trainy, paddingID, device, batchSize, cutoff)\n",
    "valx = batchData(valx, paddingID, device, batchSize, cutoff)\n",
    "valy = batchData(valy, paddingID, device, batchSize, cutoff)\n",
    "\n",
    "traindata = (trainx, trainy)\n",
    "valdata = (valx, valy)\n",
    "print(\"Done.\")\n",
    "\n",
    "# setup variables for model components initialisation\n",
    "maxReviewLength = cutoff\n",
    "vocabularySize = len(id2word)\n",
    "embeddingDim = weightMatrix.shape[1]\n",
    "\n",
    "embedding_shape = weightMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising model components.. Done.\n",
      "Loading model weights.. Done.\n"
     ]
    }
   ],
   "source": [
    "hiddenSize = params['hiddenSize']\n",
    "latentSize = params['latentSize']\n",
    "\n",
    "from vad import Encoder, Decoder, loss_function\n",
    "\n",
    "print(\"Initialising model components..\", end=\" \")\n",
    "embedding = nn.Embedding(\n",
    "        num_embeddings=vocabularySize,\n",
    "        embedding_dim=embeddingDim,\n",
    "        padding_idx=paddingID,\n",
    "    _weight=weightMatrix\n",
    "    )\n",
    "\n",
    "modelEncoder = Encoder(embedding, vocabularySize,\n",
    "                       paddingID, hiddenSize, bidirectionalEncoder).to(device)\n",
    "\n",
    "modelDecoder = Decoder(embedding, vocabularySize,\n",
    "                       paddingID, batchSize, maxReviewLength, hiddenSize, latentSize, bidirectionalEncoder).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss(ignore_index=paddingID)\n",
    "\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "# load models\n",
    "print(\"Loading model weights..\", end=\" \")\n",
    "modelEncoder.load_state_dict(torch.load(os.path.join(model_path,'encoder.pth')))\n",
    "modelDecoder.load_state_dict(torch.load(os.path.join(model_path,'decoder.pth')))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x,\n",
    "             xLength,\n",
    "             y,\n",
    "             yLength,\n",
    "             encoder,\n",
    "             decoder,\n",
    "             device,\n",
    "             criterion,\n",
    "             word2id\n",
    "            ):\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # initalise input and target lengths\n",
    "    inputLength = x[0].size(0)\n",
    "    targetLength = y[0].size(0)\n",
    "    batchSize = x.shape[0]\n",
    "\n",
    "    # set up encoder computation\n",
    "    encoderHidden = encoder.initHidden(batchSize).to(device)\n",
    "    \n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    # set up encoder outputs\n",
    "    encoderOutputs, encoderHidden = encoder(x, encoderHidden, xLength)\n",
    "\n",
    "    # set up the variables for decoder computation\n",
    "    decoderInput = torch.tensor([word2id[\"<sos>\"]] * batchSize, dtype=torch.long, device=device)\n",
    "    \n",
    "    decoderHidden = encoderHidden[-1]\n",
    "    decoderOutput = None\n",
    "    decoderOutputs = []\n",
    "    \n",
    "    # Run through the decoder one step at a time. This seems to be common practice across\n",
    "    # all of the seq2seq based implementations I've come across on GitHub.\n",
    "    for t in range(yLength[0]):\n",
    "        # compute the output of each decoder state\n",
    "        decoderOutput, decoderHidden = decoder(decoderInput, encoderOutputs, xLength, decoderHidden, device, back=None)\n",
    "\n",
    "        decoderOutputs.append(decoderOutput)\n",
    "\n",
    "        decoderInput = decoderOutput.argmax(1)\n",
    "        decoderHidden = decoderHidden.squeeze(0)\n",
    "        \n",
    "    return decoderOutputs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from train_vad import evalVAD as evaluate\n",
    "def evaluateModel(batched_data,\n",
    "                   encoder,\n",
    "                   decoder,\n",
    "                   criterion,\n",
    "                   id2word,\n",
    "                  ):\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    numbatches = len(batched_data)\n",
    "    for batch in tqdm(range(0,3)):\n",
    "        # each batch is composed of the \n",
    "        # reviews, and a sentence length.\n",
    "        x, xLength = batched_data[0][batch][0], batched_data[0][batch][1]\n",
    "        y, yLength = batched_data[1][batch][0], batched_data[1][batch][1]\n",
    "        \n",
    "        outputs, losses = evaluate(x,\n",
    "                                   xLength,\n",
    "                                   y,\n",
    "                                   yLength,\n",
    "                                   encoder,\n",
    "                                   decoder,\n",
    "                                   device,\n",
    "                                   criterion,\n",
    "                                   word2id)\n",
    "        break\n",
    "    return x, y, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "x, y, outputs = evaluateModel(traindata,\n",
    "                   modelEncoder,\n",
    "                   modelDecoder,\n",
    "                   criterion,\n",
    "                   id2word\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertRealID2Word(id2word, y):\n",
    "    entries = [[id2word[x.item()] for x in y[entry].cpu()] for entry in range(len(y))]\n",
    "    for i in range(len(entries)):\n",
    "        entries[i] = \" \".join(entries[i])\n",
    "        entries[i] = entries[i].replace(\"<pad> \", \"\")\n",
    "#         entries[i] = entries[i][43:]\n",
    "    return entries\n",
    "        \n",
    "def convertDecoderID2Word(id2word, outputs):\n",
    "    entries = []\n",
    "    for batch_line in outputs:\n",
    "        entry = [torch.argmax(batch_line[i]).cpu().item() for i in range(len(batch_line))]\n",
    "        entries.append([id2word[i] for i in entry])\n",
    "     \n",
    "    words = []\n",
    "    for i in range(len(outputs[0])):\n",
    "        tokens = [entries[j][i] for j in range(len(entries))][1:]\n",
    "#         print(tokens)\n",
    "        words.append(\" \".join(tokens))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> <unk> <unk> the the , , , the . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "<unk> <unk> <unk> , , . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "<unk> <unk> <unk> , , , . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "<unk> <unk> <unk> , , . . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "<unk> <unk> <unk> , , . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "<unk> <unk> <unk> , the . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "<unk> <unk> <unk> , , . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "<unk> <unk> <unk> , . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "<unk> <unk> <unk> , . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "<unk> <unk> <unk> , . . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n"
     ]
    }
   ],
   "source": [
    "entries = [torch.argmax(entry, dim=1) for entry in outputs]\n",
    "textes  = np.array([[id2word[x.item()] for x in y] for y in entries]).transpose()\n",
    "for row in textes:\n",
    "    print(\" \".join(row.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: b 0 0 0 0 0 j 1 u b rating_5.0 polarity_0.0 kb at to ps/2 adapter <eos> <pad>\n",
      "REFERENCE: the package arrived in a timely fashion and in good shape . <eos> <pad>\n",
      "MODEL:     <unk> <unk> the . . . <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "before = convertRealID2Word(id2word,x)\n",
    "reference = convertRealID2Word(id2word,y)\n",
    "words = convertDecoderID2Word(id2word,outputs)\n",
    "\n",
    "for i in range(len(reference)):\n",
    "    print(\"INPUT:\", before[i])\n",
    "    print(\"REFERENCE:\", reference[i])\n",
    "    print(\"MODEL:    \",words[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "for x in os.listdir('models'):\n",
    "    subpath = os.path.join('models', x)\n",
    "    if \"encoder.pth\" not in os.listdir(subpath):\n",
    "        print(\"bad\", subpath, os.listdir(subpath))\n",
    "        shutil.rmtree(subpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
