{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import bcolz\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "# plotting\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from vad import batchData\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\"\"\"\n",
    "Set seed #\n",
    "\"\"\"\n",
    "seed = 1337\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "def loadDataset(path = '../Datasets/Reviews/dataset_ready.pkl'):\n",
    "    return pickle.load(open(path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parameters.. Done.\n",
      "Loading dataset.. Done.\n",
      "Converting dataset weights into tensors.. Done.\n",
      "Batching Data.. Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading parameters..\", end=\" \")\n",
    "hiddenSize = 512\n",
    "latentSize = 400\n",
    "batchSize  = 32\n",
    "iterations = 3\n",
    "learningRate = 0.0001\n",
    "bidirectionalEncoder = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Loading dataset..\", end=\" \")\n",
    "dataset = loadDataset()\n",
    "# setup store parameters\n",
    "id2word = dataset['id2word']\n",
    "word2id = dataset['word2id']\n",
    "weightMatrix = dataset['weights']\n",
    "train = dataset['train']\n",
    "validation = dataset['validation']\n",
    "cutoff = dataset['cutoff']\n",
    "paddingID = word2id['<pad>']\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Converting dataset weights into tensors..\", end=\" \")\n",
    "# convert dataset into tensors\n",
    "weightMatrix = torch.tensor(weightMatrix, dtype=torch.float)\n",
    "print(\"Done.\")\n",
    "\n",
    "# batching data\n",
    "print(\"Batching Data..\",end=\" \")\n",
    "\n",
    "# shuffle data rows\n",
    "random.shuffle(train)\n",
    "random.shuffle(validation)\n",
    "\n",
    "trainx = [x[0] for x in train]\n",
    "trainy = [x[1] for x in train]\n",
    "valx = [x[0] for x in validation]\n",
    "valy = [x[1] for x in validation]\n",
    "\n",
    "# shuffle data row\n",
    "\n",
    "trainx = batchData(trainx, paddingID, device, batchSize, cutoff)\n",
    "trainy = batchData(trainy, paddingID, device, batchSize, cutoff)\n",
    "valx = batchData(valx, paddingID, device, batchSize, cutoff)\n",
    "valy = batchData(valy, paddingID, device, batchSize, cutoff)\n",
    "\n",
    "traindata = (trainx, trainy)\n",
    "valdata= (valx, valy)\n",
    "print(\"Done.\")\n",
    "\n",
    "# setup variables for model components initialisation\n",
    "maxReviewLength = cutoff\n",
    "vocabularySize = len(id2word)\n",
    "embeddingDim = weightMatrix.shape[1]\n",
    "embedding_shape = weightMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising model components.. Done.\n",
      "Loading model weights.. Done.\n"
     ]
    }
   ],
   "source": [
    "from vad import Encoder, Decoder, Attn, Backwards, Prior, Inference, loss_function\n",
    "\n",
    "print(\"Initialising model components..\", end=\" \")\n",
    "\n",
    "modelEncoder = Encoder(weightMatrix, vocabularySize,\n",
    "                       paddingID, hiddenSize, bidirectionalEncoder).to(device)\n",
    "# modelAttention = Attention(maxLength=maxReviewLength).to(device)\n",
    "modelAttention = Attn(hiddenSize, bidirectionalEncoder).to(device)\n",
    "modelBackwards = Backwards(weightMatrix, vocabularySize,\n",
    "                           paddingID, hiddenSize, bidirectionalEncoder).to(device)\n",
    "modelInference = Inference(\n",
    "    hiddenSize, latentSize, bidirectionalEncoder).to(device)\n",
    "modelPrior = Prior(hiddenSize, latentSize, bidirectionalEncoder).to(device)\n",
    "modelDecoder = Decoder(weightMatrix, vocabularySize,\n",
    "                       paddingID, batchSize, maxReviewLength, hiddenSize, latentSize, bidirectionalEncoder).to(device)\n",
    "criterion = nn.NLLLoss(ignore_index=paddingID)\n",
    "print(\"Done.\")\n",
    "\n",
    "\n",
    "# load models\n",
    "print(\"Loading model weights..\", end=\" \")\n",
    "modelEncoder.load_state_dict(torch.load('encoder.pth'))\n",
    "modelAttention.load_state_dict(torch.load('attention.pth'))\n",
    "modelBackwards.load_state_dict(torch.load('backwards.pth'))\n",
    "modelInference.load_state_dict(torch.load('inference.pth'))\n",
    "modelPrior.load_state_dict(torch.load('prior.pth'))\n",
    "modelDecoder.load_state_dict(torch.load('decoder.pth'))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x,\n",
    "             xLength,\n",
    "             y,\n",
    "             yLength,\n",
    "             encoder,\n",
    "             attention,\n",
    "             backwards,\n",
    "             inference,\n",
    "             prior,\n",
    "             decoder,\n",
    "             criterion\n",
    "            ):\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # initalise input and target lengths\n",
    "    inputLength = x[0].size(0)\n",
    "    targetLength = y[0].size(0)\n",
    "    batchSize = x.shape[0]\n",
    "\n",
    "    # set up encoder computation\n",
    "    encoderHidden = encoder.initHidden(batchSize).to(device)\n",
    "    backwardHidden = backwards.initHidden(batchSize).to(device)\n",
    "    \n",
    "    # set up encoder outputs\n",
    "    encoderOutputs, encoderHidden = encoder(x, encoderHidden, xLength)\n",
    "\n",
    "    # compute backwards outputs\n",
    "    backwardOutput, backwardHidden = backwards(torch.flip(\n",
    "        y, [0, 1]), yLength, backwardHidden)\n",
    "\n",
    "    # set up the variables for decoder computation\n",
    "    decoderInput = torch.tensor([[word2id[\"<sos>\"]]] * batchSize, dtype=torch.long, device=device)\n",
    "    \n",
    "    decoderHidden = encoderHidden[-1]\n",
    "    decoderOutput = None\n",
    "    decoderOutputs = []\n",
    "    \n",
    "    # Run through the decoder one step at a time. This seems to be common practice across\n",
    "    # all of the seq2seq based implementations I've come across on GitHub.\n",
    "    for t in range(yLength[0]):\n",
    "        # get the context vector c\n",
    "        c = attention(encoderOutputs, decoderHidden)\n",
    "        \n",
    "        # compute the inference layer\n",
    "        z_infer, infer_mu, infer_logvar = inference(decoderHidden, c, backwardOutput[:,t])\n",
    "        \n",
    "        # compute the prior layer\n",
    "        z_prior, prior_mu, prior_logvar = prior(decoderHidden, c)\n",
    "    \n",
    "        # compute the output of each decoder state\n",
    "        DecoderOut = decoder(decoderInput, c, z_prior, decoderHidden)\n",
    "        \n",
    "        # update variables\n",
    "        decoderOutput, decoderHidden = DecoderOut\n",
    "        decoderOutputs.append(decoderOutput)\n",
    "        seqloss = loss_function(decoderOutput, y[:, t], infer_mu, infer_logvar, prior_mu, prior_logvar, criterion)\n",
    "        loss += seqloss\n",
    "\n",
    "        # feed this output to the next input\n",
    "        decoderInput = y[:,t]\n",
    "        decoderHidden = decoderHidden.squeeze(0)\n",
    "        \n",
    "    loss = loss.item()/targetLength\n",
    "        \n",
    "    return decoderOutputs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(batched_data,\n",
    "                   encoder,\n",
    "                   backward,\n",
    "                   attention,\n",
    "                   inference,\n",
    "                   prior,\n",
    "                   decoder,\n",
    "                   criterion,\n",
    "                   id2word,\n",
    "                  ):\n",
    "    \n",
    "    encoder.eval()\n",
    "    backward.eval()\n",
    "    attention.eval()\n",
    "    inference.eval()\n",
    "    prior.eval()\n",
    "    decoder.eval()\n",
    "    numbatches = len(batched_data)\n",
    "    for batch in tqdm(range(0,3)):\n",
    "        # each batch is composed of the \n",
    "        # reviews, and a sentence length.\n",
    "        x, xLength = batched_data[0][batch][0], batched_data[0][batch][1]\n",
    "        y, yLength = batched_data[1][batch][0], batched_data[1][batch][1]\n",
    "        \n",
    "        outputs, losses = evaluate(x,\n",
    "                                   xLength,\n",
    "                                   y,\n",
    "                                   yLength,\n",
    "                                   encoder,\n",
    "                                   attention,\n",
    "                                   backward,\n",
    "                                   inference,\n",
    "                                   prior,\n",
    "                                   decoder,\n",
    "                                   criterion)\n",
    "        \n",
    "        print(y.shape)\n",
    "#         reference = convertRealID2Word(id2word,y)\n",
    "#         for e in reference:\n",
    "#             print(e)\n",
    "        words = convertDecoderID2Word(id2word,outputs)\n",
    "\n",
    "        print(\"LOSS:\", losses)\n",
    "#         break\n",
    "\n",
    "def convertRealID2Word(id2word, y):\n",
    "    return [[id2word[x.item()] for x in y[entry].cpu()] for entry in range(len(y))]\n",
    "        \n",
    "def convertDecoderID2Word(id2word, outputs):\n",
    "    entries = []\n",
    "    for batch_line in outputs:\n",
    "        entry = [torch.argmax(batch_line[i]).cpu().item() for i in range(len(batch_line))]\n",
    "        entries.append([id2word[i] for i in entry])\n",
    "        \n",
    "    for i in range(len(outputs[0])):\n",
    "        line = \" \".join([entries[j][i] for j in range(12,len(entries))])\n",
    "        line = line.replace(\" ' \", \"'\")\n",
    "        line = line.replace(\" ) \", \")\")\n",
    "        line = line.replace(\" . \", \".\")\n",
    "        line = line.replace(\" , \", \", \")\n",
    "        line = line.replace(\" ( \", \"(\")\n",
    "        print(line)\n",
    "#         break\n",
    "    return entries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:01<00:02,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'' t know to problem with the, ,, products, the, but'the)) but the <unk> <unk>, but, , <unk> the)).i i are be least of a lot more to the <unk>.but\n",
      "i is the to for the the time <unk> <unk> of the )')))). the, <unk> of <unk> <unk> <unk> &.. the all, i's a great of to, is be be a.of.the\n",
      "i'a a problems with, time, had it the, my, the <unk>, <unk>)<unk>)) <unk>, <unk>), <unk>'s a a <unk> <unk> for the <unk> <unk> <unk> <unk>, the <unk> <unk> <unk> <unk> <unk>\n",
      "i'' a <unk> to the computer and, but i i computer phone is a longer, it the computer, a problem, i'' have to the else'to <eos> the <unk>, i'' use the the on the computer\n",
      "i with the <unk> of to, and i'the to'find the to and)of)), you'' t have to to but i reviewers the other devices.. been.are the.they the.cards.<eos>\n",
      "i are't a of, , i the <unk> of the <unk>, the other of, but'm say the <unk> wire, you'' t work.. of <eos>'' t had the <unk> in the.. be the\n",
      "polarity_0.0 have have a $ <unk> replace 3 and(<unk> 3, the 3.0., to to the <unk>.<eos> i is a good connection, <unk> is have recommend.have to the <unk>...the the.the .\n",
      "i far ,'the <unk> and it <unk> <unk> is well well as i, <unk> thing is that it'to same software.. connection.the same...and is the.5.and the have the the the into the\n",
      "i like the to than the have to the one of, for the and but i else what can the lot to a.a.... <eos>................ .\n",
      "i i got the <unk> <unk>, the, ,, is a the the <unk>, the the <unk> <unk> the is be the the <unk> <unk>....................\n",
      "i is no of of be the <unk> to to the <unk> and the for to the <unk>.the to...the.<eos>.................... .\n",
      "i'a easy and the the, the <unk>, the, the'this of these and the the for to the different.<eos>.................... .\n",
      "i'not that is, a <unk> drive, i i be t be it lot that the i'great well.<eos>...................... .\n",
      "i <unk> is of up and the be used and the wall, the bottom of the it in little.the ipad.<eos>...................... .\n",
      "i'that was a, the price, of but i was not to using with it was a than.. <eos>........................\n",
      "i you are looking, , you the to use the device, , the seconds away is a the.<eos>..........................\n",
      "i'' the the to a and, it of the and and can to lot bit mouse.. <eos>..........................\n",
      "i is has is is.a bit thing and is be up much bit buy.the the.<eos>.......................... .\n",
      "the i is to be a, i is is is not of to.rating.i.<eos>............................\n",
      "i'to to use and <unk> and a lighting and the the the.the screen.<eos>............................ .\n",
      "i'it are be been the for to they to the to return <unk>.. <eos>..............................\n",
      "i'to you is supposed to be to the future.. it recommended.<eos>.............................. .\n",
      "i is'to use and else'it the.month.weeks.<eos>................................\n",
      "i <unk> is is a and the the.the.camera.. <eos>................................ .\n",
      "i <unk> is a, the timely manner, the the condition.<eos>..................................\n",
      "i'the <unk> <unk> and i keyboard and the keyboard is.<eos>..................................\n",
      "i i is, it'the the the.it.. <eos>..................................\n",
      "the is the in the'the <unk>.<eos>.<eos>....................................\n",
      "polarity_0.0 best i, came with the <unk>.not.<eos>....................................\n",
      "polarity_0.0 to are'it.a <unk>.. <eos>.................................... .\n",
      "i is a great problem.<eos>........................................ .\n",
      "the bit is <eos>............................................\n",
      "LOSS: 4.3611302693684895\n",
      "torch.Size([32, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:02<00:01,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if'this is a a'' t work a problem to the, use it to try, the i <unk> are is not much the.. i the of of.the <unk>.have't use to be the to.the\n",
      "polarity_0.0 a'using with the product.the for to the.is the the months of use.<eos> have able the 7, a years the <unk> and and.. ago.i got to new of.. the.5..\n",
      "i <unk> is is me issues, and the'not a than.the the <unk>.the.the was the <unk> of.the i was the <unk> is the <unk>, had the the i was the the the <unk> weeks.. i\n",
      "the luck, i's a heavy to the way, not of, the are no way to be in <unk>.the little...the but the.and can to use the volume.the own.and the <unk>.the\n",
      "polarity_0.0 been'm had it <unk> and and the computer, the the the to the, my'the the.computers.. the past...the'to use a few amount of time.get.<unk>.. the <unk>\n",
      "i the, the i sure you you <unk> is is is can to to a to.. 0.. and can need the you.the reviews.the users.. you the can't be to problems.... <eos>\n",
      "i's not good, but it'not a a of to the the the, i good good touch and, and, , the <unk>.. gen.<eos>..............\n",
      "polarity_0.0, have that is a great good product, you have't have it cover on the the <unk>.<unk> <unk>, to.. the the the box.<eos>..............\n",
      "the'a the, i i is it, i is be a to the to but it the.s not.it.. <eos>....................\n",
      "i is <unk> <unk> <unk> <unk> <unk> <unk>.<unk> <unk> <unk>, <unk> <unk> <unk>.s <unk>.... <unk>........................\n",
      "the <unk> <unk> to the year of i is and it of be the the the minutes.the day.<eos>..........................\n",
      "i'had the of to but i the into the the drive and not was a issue.. <eos>.......................... .\n",
      "i'this to the <unk> of the and the the the out, of and they cancelling i ,............................\n",
      "i have <unk> <unk> well with the product and and.but with <unk>.very.. <eos>............................ .\n",
      "i, , the good price, the a as the quality the the box, not.<eos>............................ .\n",
      "i the <unk> is be a)) the but the the is is)the few)))))))))))))))))))))))))).... .\n",
      "i to the the of of the price, ,, a price.<eos>.<eos>.............................. .\n",
      "i great of, to to the, the, , the.. service.<eos>.............................. .\n",
      "i the, the <unk>, ,, the <unk> <unk> <unk>...<eos>................................\n",
      "i click the i i'fine the of few of the.<eos>..................................\n",
      "i'be a to to the <unk> is a the features.<eos>..................................\n",
      "i'this <unk> to the and the the and the files.<eos>..................................\n",
      "i i is a a, the, , it the easy.<eos>..................................\n",
      "i the a excellent of <unk> but it <unk> is very.<eos>.................................. .\n",
      "i it than the hours the year of a the.<eos>....................................\n",
      "the'the the and and it was the <unk>.<eos>....................................\n",
      "i i is be the drive').. 5.................................... .\n",
      "i'a the features of have looking for the......................................\n",
      "polarity_0.0 best to a, the not a good.<eos>.................................... .\n",
      "i is is is a easy.use.<eos>......................................\n",
      "i works.. best time.<eos>........................................\n",
      "i wife loves.. <eos>..........................................\n",
      "LOSS: 4.6163579305013025\n",
      "torch.Size([32, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i % the'the <unk> to the, , i comes to be the and i the <unk> <unk>, the, ,, , <unk>), <unk> the))).'to one a issue choice to get the <unk>.. .\n",
      "i is the, and <unk> <unk>, <unk>, <unk>), is is a small easier than the',, is to't have you <unk> to use it of ,.. the than.. the and as a other, <unk>\n",
      "i, <unk> is the.. <unk> is is a good, the, the'it'much more the.the.. the <unk> i is the the cover cover, , the.., is the.. of get out is\n",
      "i have i is is a to be a <unk> of, but i extra of 20 for the of the one to the.i i i is is like a to you have the 3.0.the to <unk>.you't have\n",
      "i are re not a, the same of of, , but with be the good to.the <unk> of to the.the <unk>...<unk>.the the.. though you <unk> is of)a the)<unk>))\n",
      "i's a a a'it be able the the, you, to the <unk> inch is is be a, but it the you issue for ,.card.a.. are)))lot of than the <unk>)card\n",
      "i the the the, i have the the hour <unk> <unk> <unk>'not research <unk> the <unk> com <unk> <unk> reviewers reviewers, the...<unk> the the the <unk> is the <unk> <unk> <unk>.the <unk> <unk>.of <unk>..\n",
      "polarity_0.0 the of for a the.<unk> ghz, well cables the of are been a out to <unk> of the drive, center a to bit deal.. new.... <eos>.......... .\n",
      "i've had a with with to for a the <unk> <unk> and'in the the the, i is about the best of that the <unk> of.the.<eos>..............\n",
      "i'that had have to of to the tv, the a, <unk> of the <unk>.a a.<eos> is great.<unk> <unk>.great.<eos>................\n",
      "i you are you'have to spend the ray player, you the not'the the of.but.....the box.<eos>..................\n",
      "i good, be a to get the who little to'want the the <unk>.and.the.a.<eos>........................\n",
      "polarity_0.0 are great and i't have, , the the)) but they's not little)the). <eos>...................... .\n",
      "i you are to <unk> basics, can, for <unk> to get the the tv.the tv...<eos>........................ .\n",
      "i is is a and to be the to the and hours to and to). the speed.. <eos>..........................\n",
      "i've had that of about complained that with the few.the <unk> jack.the sound.<eos>.......................... .\n",
      "i is a a good as the <unk> <unk> <unk> <unk> 20., have ve had.<eos>............................ .\n",
      "i you are looking to use the, the home, you are will a <unk>.<eos>..............................\n",
      "the'the <unk> <unk>, and the 20, , and and the, i, ,..............................\n",
      "i the the other are, well are be the problems ,.to <eos>................................ .\n",
      "polarity_0.0 for for i only interface is a a better and easy.<eos>..................................\n",
      "i is, of a best <unk> you can lot of system <eos>.................................. .\n",
      "i'this to <unk> weeks and the <unk> and and <eos>....................................\n",
      "the is the <unk> a.. few...<eos>....................................\n",
      "i though the the, ,, is a good and <eos>....................................\n",
      "i is a great good product <unk>.. <eos>......................................\n",
      "<unk> the, <unk> of very.<eos>.<eos>......................................\n",
      "i the <unk> quality on and..........................................\n",
      "i'the a got that <eos>........................................ .\n",
      "the'm not with <eos>..........................................\n",
      "i'the ! <eos>.......................................... .\n",
      "i problems with are <eos>.......................................... .\n",
      "LOSS: 4.672455342610677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = evaluateModel(valdata,\n",
    "                   modelEncoder,\n",
    "                   modelBackwards,\n",
    "                   modelAttention,\n",
    "                   modelInference,\n",
    "                   modelPrior,\n",
    "                   modelDecoder,\n",
    "                   criterion,\n",
    "                   id2word\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
