{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import gzip\n",
    "import bcolz\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import inspect\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "# seed = 1337\n",
    "# pickleFile = '../Datasets/Reviews/dataset.pkl'\n",
    "# gloveDimension = 50\n",
    "# glovePath = \"/media/data/Datasets/glove\"\n",
    "# dataset_reduction = 16\n",
    "# trainPortion = 0.80\n",
    "# vocabularyLimit = 30000\n",
    "# cutoff = 60\n",
    "# datasetFile = '../Datasets/Reviews/dataset_ready.pkl'\n",
    "# example_item_id = 19\n",
    "\n",
    "# params = {\n",
    "#     \"seed\" : 1337,\n",
    "#     \"pickleFile\" :  '../Datasets/Reviews/dataset.pkl',\n",
    "#     \"gloveDimension\" : 50,\n",
    "#     \"glovePath\" : \"/media/data/Datasets/glove\",\n",
    "#     \"datasetReduction\" : 16,\n",
    "#     \"trainPortion\" : 0.80,\n",
    "#     \"vocabularyLimit\" : 30000,\n",
    "#     \"cutoff\" : 60,\n",
    "#     \"datasetFile\" : '../Datasets/Reviews/dataset_ready.pkl',\n",
    "#     \"example_item_id\" : 19, \n",
    "#     \"saveDataset\" : False\n",
    "# }\n",
    "\n",
    "params = {\n",
    "    \"seed\" : 1337,\n",
    "    \"pickleFile\" :  '../Datasets/Reviews/dataset.pkl',\n",
    "    \"gloveDimension\" : 50,\n",
    "    \"glovePath\" : \"/media/data/Datasets/glove\",\n",
    "    \"datasetReduction\" : 16,\n",
    "    \"trainPortion\" : 0.80,\n",
    "    \"vocabularyLimit\" : 30000,\n",
    "    \"cutoff\" : 60,\n",
    "    \"datasetFile\" : '../Datasets/Reviews/dataset_ready.pkl',\n",
    "    \"example_item_id\" : 19, \n",
    "    \"saveDataset\" : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(params['seed'])\n",
    "np.random.seed(params['seed'])\n",
    "\n",
    "nlp = English()\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sentencizer)\n",
    "tokenizer = nlp.create_pipe(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the dataset in 6.91 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "dataset = pickle.load( open( params['pickleFile'], \"rb\" ))\n",
    "duration = time.clock() - start\n",
    "print(\"Loaded the dataset in\", round(duration,2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 63001 amazon items.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\",len(dataset), \"amazon items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Glove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading files.. Done.\n"
     ]
    }
   ],
   "source": [
    "def loadGlove(glove_path, dim=50):\n",
    "    acceptedDimensions = [50, 100, 200, 300]\n",
    "    if dim not in acceptedDimensions:\n",
    "        print(\"You didn't choose a right dimension.\")\n",
    "        print(\"Try one of these:\", acceptedDimensions)\n",
    "        return None\n",
    "    pickleWordFile = f'{glove_path}/6B.'+str(dim)+'_words.pkl'\n",
    "    pickleIdFile   = f'{glove_path}/6B.'+str(dim)+'_idx.pkl'\n",
    "    pickleDatFile  = f'{glove_path}/glove.6B.'+str(dim)+'.dat'\n",
    "    pickleDataset  = f'{glove_path}/glove.6B.'+str(dim)+'d.txt'\n",
    "    \n",
    "    if os.path.isfile(pickleWordFile):\n",
    "        # check if we've made the outputs before\n",
    "        print(\"Preloading files..\", end=\" \")\n",
    "        vectors = bcolz.open(pickleDatFile)[:]\n",
    "        words = pickle.load(open(pickleWordFile, 'rb'))\n",
    "        word2idx = pickle.load(open(pickleIdFile, 'rb'))\n",
    "        glove = {w: vectors[word2idx[w]] for w in words}\n",
    "        print(\"Done.\")\n",
    "        return glove\n",
    "    else:\n",
    "        print(\"Doesn't work.\", end=\" \")\n",
    "\n",
    "glove = loadGlove(params['glovePath'], dim=params['gloveDimension'])\n",
    "gloveWords = glove.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Vocabulary Size: 400000\n"
     ]
    }
   ],
   "source": [
    "print(\"Glove Vocabulary Size:\",len(gloveWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(paragraph):\n",
    "    # split paragraph by full stops\n",
    "    paragraph = paragraph.lower()\n",
    "    paragraph = re.sub(\"([,!?()-+&Â£$.%*'])\", r' \\1 ', paragraph)\n",
    "    paragraph = re.sub('\\s{2,}', ' ', paragraph)\n",
    "    paragraph = paragraph.split(\" \")\n",
    "    # remove empty string\n",
    "    return paragraph\n",
    "    \n",
    "def discretise(value, word):\n",
    "    return word + \"_\" + str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleItem(itemID, dataset=dataset, printDebug=False):\n",
    "    \"\"\"\n",
    "    Filters words out based on whether they're in the GloVe dataset or not.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    reviews = []\n",
    "    for i in range(len(dataset[itemID])):\n",
    "        # initialise variables\n",
    "        entry = dataset[itemID][i]\n",
    "        reviewerID = entry['reviewerID']\n",
    "        \n",
    "        if len(entry['reviewText']) < 1:\n",
    "            continue\n",
    "\n",
    "        \"\"\"\n",
    "        Review Text Processing\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        # spacy method\n",
    "        sentences = [[str(i).lower() for i in tokenizer(str(sentence))] for sentence in nlp(entry['reviewText']).sents]\n",
    "        \n",
    "        # preprocess summary\n",
    "        summary = [str(i).lower() for i in tokenizer(entry['summary'])]\n",
    "        \n",
    "        # merge summary sequence and review sequences together into overall entries.\n",
    "#         if len(sentences) < 2:\n",
    "#             entries =  [[\"<sos>\", \"<summary>\"] + summary + [\"</summary>\"]] + [[\"<sos>\", \"<text>\"] + sentences[0] + [\"</text>\", \"<eor>\", \"<eos>\"]]\n",
    "#         else:\n",
    "#             subset = [[\"<sos>\", \"<text>\"] + x + [\"</text>\"] for x in sentences[:-1]]\n",
    "#             entries =  [[\"<sos>\", \"<summary>\"] + summary + [\"</summary>\"]] + subset + [[\"<sos>\", \"<text>\"] + sentences[-1] + [\"</text>\", \"<eor>\", \"<eos>\"]]\n",
    "        \n",
    "        entries = [summary] + sentences\n",
    "        entries = [x + [\"<eos>\"] for x in entries]\n",
    "  \n",
    "        # setup review parameters\n",
    "        rating   = [discretise(entry['overall'], \"rating\")]\n",
    "\n",
    "        # compute polarity\n",
    "        good, bad = entry['helpful'][0], entry['helpful'][1]\n",
    "        \n",
    "        try:\n",
    "            polarity = (good - bad) / (good + bad)\n",
    "        except ZeroDivisionError:\n",
    "            polarity = 0\n",
    "        polarity = np.tanh(polarity)\n",
    "        polarity = np.round(polarity, 1)\n",
    "        polarity = [discretise(polarity, \"polarity\")]\n",
    "\n",
    "        # create identity/conditioning entry\n",
    "        identifier = itemID.lower()\n",
    "        identity = [l for l in identifier] + rating + polarity\n",
    "\n",
    "        # add conditionining entry to each entry\n",
    "        formatted = [entry for entry in entries]\n",
    "\n",
    "        if printDebug:\n",
    "            print(\"ENTRY:\",dataset[itemID][i])\n",
    "            print(\"IDENTITY:\",identity)\n",
    "\n",
    "        for i in range(len(formatted)-1):\n",
    "            # add the conditioning variable to the input. the output value does not have the conditioning variable.\n",
    "            reviews.append([identity + formatted[i], formatted[i+1]])\n",
    "            if printDebug:\n",
    "                print(reviews[-1][0], \"->\", reviews[-1][1])\n",
    "        if printDebug:\n",
    "            break\n",
    "            \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTRY: {'reviewerID': 'AA8JH8LD2H4P9', 'asin': '7214047977', 'reviewerName': 'Claudia J. Frier', 'helpful': [3, 4], 'reviewText': 'This fits my 7\" kindle fire hd perfectly! I love it. It even has a slot for a stylus. The kindle is velcroed in so it\\'s nice and secure. Very glad I bought this!', 'overall': 5.0, 'summary': 'love it', 'unixReviewTime': 1354665600, 'reviewTime': '12 5, 2012'}\n",
      "IDENTITY: ['7', '2', '1', '4', '0', '4', '7', '9', '7', '7', 'rating_5.0', 'polarity_-0.1']\n",
      "['7', '2', '1', '4', '0', '4', '7', '9', '7', '7', 'rating_5.0', 'polarity_-0.1', 'love', 'it', '<eos>'] -> ['this', 'fits', 'my', '7', '\"', 'kindle', 'fire', 'hd', 'perfectly', '!', '<eos>']\n",
      "['7', '2', '1', '4', '0', '4', '7', '9', '7', '7', 'rating_5.0', 'polarity_-0.1', 'this', 'fits', 'my', '7', '\"', 'kindle', 'fire', 'hd', 'perfectly', '!', '<eos>'] -> ['i', 'love', 'it', '.', '<eos>']\n",
      "['7', '2', '1', '4', '0', '4', '7', '9', '7', '7', 'rating_5.0', 'polarity_-0.1', 'i', 'love', 'it', '.', '<eos>'] -> ['it', 'even', 'has', 'a', 'slot', 'for', 'a', 'stylus', '.', '<eos>']\n",
      "['7', '2', '1', '4', '0', '4', '7', '9', '7', '7', 'rating_5.0', 'polarity_-0.1', 'it', 'even', 'has', 'a', 'slot', 'for', 'a', 'stylus', '.', '<eos>'] -> ['the', 'kindle', 'is', 'velcroed', 'in', 'so', 'it', \"'s\", 'nice', 'and', 'secure', '.', '<eos>']\n",
      "['7', '2', '1', '4', '0', '4', '7', '9', '7', '7', 'rating_5.0', 'polarity_-0.1', 'the', 'kindle', 'is', 'velcroed', 'in', 'so', 'it', \"'s\", 'nice', 'and', 'secure', '.', '<eos>'] -> ['very', 'glad', 'i', 'bought', 'this', '!', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "datasetKeys = list(dataset.keys())\n",
    "example_set = handleItem(datasetKeys[params['example_item_id']],printDebug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "identity = example_set[0][0][:example_set[0][0].index(\"<eos>\")]\n",
    "\n",
    "example_tag = {\"reference\":dataset[datasetKeys[params['example_item_id']]][0],\n",
    "               \"result\":example_set}\n",
    "# print(json.dumps(example_tag, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63001\n"
     ]
    }
   ],
   "source": [
    "print(len(datasetKeys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processItems(func, args, n_processes = 7):\n",
    "    p = Pool(n_processes)\n",
    "    res_list = []\n",
    "    with tqdm(total = len(args)) as pbar:\n",
    "        for i, res in enumerate(p.imap_unordered(func, args)):\n",
    "            pbar.update()\n",
    "            res_list.append(res)\n",
    "    pbar.close()\n",
    "    p.close()\n",
    "    p.join()\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 3938/3938 [00:47<00:00, 83.15it/s]\n"
     ]
    }
   ],
   "source": [
    "reviews = processItems(handleItem,datasetKeys[::params['datasetReduction']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Reviews:\n",
      "Training: 3150 \t\tValidation: 788\n",
      "Sequences:\n",
      "Training: 574362 \tValidation: 153211\n"
     ]
    }
   ],
   "source": [
    "datasetSize = len(reviews)\n",
    "trainRatio = int(datasetSize * params['trainPortion'])\n",
    "\n",
    "train = reviews[:trainRatio]\n",
    "validation = reviews[trainRatio:]\n",
    "\n",
    "print(\"Num Reviews:\")\n",
    "print(\"Training:\", len(train), \"\\t\\tValidation:\",len(validation))\n",
    "\n",
    "# now we need to flatten train and validation.\n",
    "trainents = []\n",
    "for review in train:\n",
    "    trainents += [entry for entry in review]\n",
    "valents = []\n",
    "for review in validation:\n",
    "    valents += [entry for entry in review]\n",
    "    \n",
    "train = trainents\n",
    "validation = valents\n",
    "\n",
    "print(\"Sequences:\")\n",
    "print(\"Training:\",len(train),\"\\tValidation:\",len(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['b', '0', '0', '0', '0', '0', 'j', '1', 'u', 'b', 'rating_5.0', 'polarity_0.0', 'kb', 'at', 'to', 'ps/2', 'adapter', '<eos>'], ['the', 'package', 'arrived', 'in', 'a', 'timely', 'fashion', 'and', 'in', 'good', 'shape', '.', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "# get the number of itemIDs\n",
    "for row in train:\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ID's of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the throughput of the model, we should reduce the embedding size. Here we'll look at all the words and keep track ones that exist. We'll make a reduced word2id based on this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting Reviews..\n",
      "We now have 574362 reviews.\n"
     ]
    }
   ],
   "source": [
    "# here we reduce the size of the dataset so we can debug our model.\n",
    "print(\"Subsetting Reviews..\")\n",
    "print(\"We now have\", len(train), \"reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 574362/574362 [00:09<00:00, 61306.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# get word frequency for words in training data\n",
    "for row in tqdm(train):\n",
    "    for sequences in row:\n",
    "        for word in sequences:\n",
    "            word = str(word)\n",
    "            # setup container if word does not exist\n",
    "            if word not in wordcounts:\n",
    "                wordcounts[word] = 0\n",
    "            # increment\n",
    "            wordcounts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get words that are not in the glove dataset\n",
    "knowns   = [word for word in wordcounts if word in glove]\n",
    "unknowns = [word for word in wordcounts if word not in glove]\n",
    "# sort words by their frequency\n",
    "wordOrder = list(sorted(knowns, key=lambda x: wordcounts[x], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41822 84686\n"
     ]
    }
   ],
   "source": [
    "print(len(knowns), len(unknowns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold number of tokens based on the vocabulary limit.\n",
    "wordOrder = wordOrder[:params['vocabularyLimit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store glove vectors for each token\n",
    "weights = [glove[word] for word in wordOrder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectors for rating and polarity\n",
    "for word in unknowns:\n",
    "    if (\"rating\" in word) or (\"polarity\" in word):\n",
    "        try:\n",
    "            part = word.split(\"_\")\n",
    "            if part[1] == \"-0.0\":\n",
    "                part[1] = \"0.0\"\n",
    "            weight = glove[part[0]] + glove[part[1]]\n",
    "            wordOrder.append(word)\n",
    "            weights.append(weight)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries for constant time referencing\n",
    "id2word = {idx: w for (idx, w) in enumerate(wordOrder)}\n",
    "word2id = {w: idx for (idx, w) in enumerate(wordOrder)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = len(word2id)\n",
    "\n",
    "# add <sos> (start of sequence)\n",
    "weights.append(glove['sos'])\n",
    "word2id['<sos>'] = lim\n",
    "id2word[lim] = ['<sos>']\n",
    "lim += 1\n",
    "\n",
    "# add <eos> (end of sequence)\n",
    "weights.append(glove['eos'])\n",
    "word2id['<eos>'] = lim\n",
    "id2word[lim] = '<eos>'\n",
    "lim += 1\n",
    "\n",
    "\n",
    "gloveDimension = params['gloveDimension']\n",
    "\n",
    "sorWeight = np.random.normal(0,0.5,gloveDimension)\n",
    "weights.append(sorWeight)\n",
    "word2id['<summary>'] = lim\n",
    "id2word[lim] = '<summary>'\n",
    "lim += 1\n",
    "\n",
    "sorWeight = np.random.normal(0,0.5,gloveDimension)\n",
    "weights.append(sorWeight)\n",
    "word2id['</summary>'] = lim\n",
    "id2word[lim] = '</summary>'\n",
    "lim += 1\n",
    "\n",
    "sorWeight = np.random.normal(0,0.5,gloveDimension)\n",
    "weights.append(sorWeight)\n",
    "word2id['<text>'] = lim\n",
    "id2word[lim] = '<text>'\n",
    "lim += 1\n",
    "\n",
    "sorWeight = np.random.normal(0,0.5,gloveDimension)\n",
    "weights.append(sorWeight)\n",
    "word2id['</text>'] = lim\n",
    "id2word[lim] = '</text>'\n",
    "lim += 1\n",
    "\n",
    "\n",
    "# add <unk> (unknown token)\n",
    "weights.append(glove['unk'])\n",
    "word2id['<unk>'] = lim\n",
    "id2word[lim] = '<unk>'\n",
    "\n",
    "# add <pad> \n",
    "id2word[len(word2id)] = \"<pad>\"\n",
    "word2id[\"<pad>\"] = len(word2id)\n",
    "weights.append(np.random.normal(0,0,gloveDimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create functions that incorporate dictionary search\n",
    "# and returns <unk> token upon failure\n",
    "def wordToID(word,corp=word2id):\n",
    "    if word in corp:\n",
    "        return corp[word]\n",
    "    return corp['<unk>']\n",
    "\n",
    "def IDToWord(id,corp=id2word, ref=word2id):\n",
    "    if id in corp:\n",
    "        return corp[id]\n",
    "    return corp[ref['<unk>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 574362/574362 [00:06<00:00, 88259.51it/s]\n",
      "100%|ââââââââââ| 153211/153211 [00:01<00:00, 107271.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert words to their id's in the review.\n",
    "def entriesToWordIDs(group):\n",
    "    return [[[wordToID(word) for word in seq] for seq in row] for row in tqdm(group)]\n",
    "    \n",
    "train = entriesToWordIDs(train)\n",
    "validation = entriesToWordIDs(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sequence in our dataset is 1442 tokens long.\n"
     ]
    }
   ],
   "source": [
    "sizes = {}\n",
    "for i in range(len(train)):\n",
    "    row = train[i]\n",
    "    for seq in row:\n",
    "        length = len(seq)\n",
    "        if length not in sizes:\n",
    "            sizes[length] = []\n",
    "        sizes[length].append(i)\n",
    "\n",
    "seqlengths = list(sorted(sizes.keys(), key=lambda x: len(sizes[x]), reverse=True))\n",
    "print(\"The longest sequence in our dataset is\",max(seqlengths),\"tokens long.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"lcd 's need native resolution , i 've included below what took me a while to find : <unk> <unk> 256 mb supported video <unk> 0 : <unk> 256 colours ( 8-bit ) <unk> 1 : <unk> 256 colours ( 8-bit ) <unk> 2 : <unk> 64 k <unk> ( 16-bit ) <unk> 3 : <unk> 64 k <unk> ( 16-bit ) <unk> 4 : <unk> <unk> <unk> ( 32-bit ) <unk> 5 : <unk> <unk> <unk> ( 32-bit ) <unk> 6 : 320x240 256 colours ( 8-bit ) <unk> 7 : 320x240 256 colours ( 8-bit ) <unk> 8 : 320x240 64 k <unk> ( 16-bit ) <unk> 9 : 320x240 64 k <unk> ( 16-bit ) <unk> 10 : 320x240 <unk> <unk> ( 32-bit ) <unk> 11 : 320x240 <unk> <unk> ( 32-bit ) <unk> 12 : <unk> 256 colours ( 8-bit ) <unk> 13 : <unk> 256 colours ( 8-bit ) <unk> 14 : <unk> 64 k <unk> ( 16-bit ) <unk> 15 : <unk> 64 k <unk> ( 16-bit ) <unk> 16 : <unk> <unk> <unk> ( 32-bit ) <unk> 17 : <unk> <unk> <unk> ( 32-bit ) <unk> 18 : <unk> 256 colours ( 8-bit ) <unk> 19 : <unk> 64 k <unk> ( 16-bit ) <unk> 20 : <unk> <unk> <unk> ( 32-bit ) <unk> 21 : <unk> 256 colours ( 8-bit ) <unk> 22 : <unk> 256 colours ( 8-bit ) <unk> 23 : <unk> 64 k <unk> ( 16-bit ) <unk> 24 : <unk> 64 k <unk> ( 16-bit ) <unk> 25 : <unk> <unk> <unk> ( 32-bit ) <unk> 26 : <unk> <unk> <unk> ( 32-bit ) <unk> 27 : 640x480 256 colours ( 8-bit ) <unk> 28 : 640x480 256 colours ( 8-bit ) <unk> 29 : 640x480 256 colours ( 8-bit ) <unk> 30 : 640x480 64 k <unk> ( 16-bit ) <unk> 31 : 640x480 64 k <unk> ( 16-bit ) <unk> 32 : 640x480 64 k <unk> ( 16-bit ) <unk> 33 : 640x480 <unk> <unk> ( 32-bit ) <unk> 34 : 640x480 <unk> <unk> ( 32-bit ) <unk> 35 : 640x480 <unk> <unk> ( 32-bit ) <unk> 36 : <unk> 256 colours ( 8-bit ) <unk> 37 : <unk> 256 colours ( 8-bit ) <unk> 38 : <unk> 64 k <unk> ( 16-bit ) <unk> 39 : <unk> 64 k <unk> ( 16-bit ) <unk> 40 : <unk> <unk> <unk> ( 32-bit ) <unk> 41 : <unk> <unk> <unk> ( 32-bit ) <unk> 42 : <unk> 256 colours ( 8-bit ) <unk> 43 : <unk> 256 colours ( 8-bit ) <unk> 44 : <unk> 256 colours ( 8-bit ) <unk> 45 : <unk> 64 k <unk> ( 16-bit ) <unk> 46 : <unk> 64 k <unk> ( 16-bit ) <unk> 47 : <unk> 64 k <unk> ( 16-bit ) <unk> 48 : <unk> <unk> <unk> ( 32-bit ) <unk> 49 : <unk> <unk> <unk> ( 32-bit ) <unk> 50 : <unk> <unk> <unk> ( 32-bit ) <unk> 51 : 800x600 256 colours ( 8-bit ) <unk> 52 : 800x600 256 colours ( 8-bit ) <unk> 53 : 800x600 256 colours ( 8-bit ) <unk> 54 : 800x600 256 colours ( 8-bit ) <unk> 55 : 800x600 256 colours ( 8-bit ) <unk> 56 : 800x600 64 k <unk> ( 16-bit ) <unk> 57 : 800x600 64 k <unk> ( 16-bit ) <unk> 58 : 800x600 64 k <unk> ( 16-bit ) <unk> 59 : 800x600 64 k <unk> ( 16-bit ) <unk> 60 : 800x600 64 k <unk> ( 16-bit ) <unk> 61 : 800x600 <unk> <unk> ( 32-bit ) <unk> 62 : 800x600 <unk> <unk> ( 32-bit ) <unk> 63 : 800x600 <unk> <unk> ( 32-bit ) <unk> 64 : 800x600 <unk> <unk> ( 32-bit ) <unk> 65 : 800x600 <unk> <unk> ( 32-bit ) <unk> 66 : <unk> 256 colours ( 8-bit ) <unk> 67 : <unk> 256 colours ( 8-bit ) <unk> 68 : <unk> 64 k <unk> ( 16-bit ) <unk> 69 : <unk> 64 k <unk> ( 16-bit ) <unk> 70 : <unk> <unk> <unk> ( 32-bit ) <unk> 71 : <unk> <unk> <unk> ( 32-bit ) <unk> 72 : <unk> 256 colours ( 8-bit ) <unk> 73 : <unk> 64 k <unk> ( 16-bit ) <unk> 74 : <unk> <unk> <unk> ( 32-bit ) <unk> 75 : 1024x768 256 colours ( 8-bit ) <unk> 76 : 1024x768 256 colours ( 8-bit ) <unk> 77 : 1024x768 256 colours ( 8-bit ) <unk> 78 : 1024x768 256 colours ( 8-bit ) <unk> 79 : 1024x768 64 k <unk> ( 16-bit ) <unk> 80 : 1024x768 64 k <unk> ( 16-bit ) <unk> 81 : 1024x768 64 k <unk> ( 16-bit ) <unk> 82 : 1024x768 64 k <unk> ( 16-bit ) <unk> 83 : 1024x768 <unk> <unk> ( 32-bit ) <unk> 84 : 1024x768 <unk> <unk> ( 32-bit ) <unk> 85 : 1024x768 <unk> <unk> ( 32-bit ) <unk> 86 : 1024x768 <unk> <unk> ( 32-bit ) <unk> 87 : <unk> 256 colours ( 8-bit ) <unk> 88 : <unk> 256 colours ( 8-bit ) <unk> 89 : <unk> 256 colours ( 8-bit ) <unk> 90 : <unk> 64 k <unk> ( 16-bit ) <unk> 91 : <unk> 64 k <unk> ( 16-bit ) <unk> 92 : <unk> 64 k <unk> ( 16-bit ) <unk> 93 : <unk> <unk> <unk> ( 32-bit ) <unk> 94 : <unk> <unk> <unk> ( 32-bit ) <unk> 95 : <unk> <unk> <unk> ( 32-bit ) <unk> 96 : <unk> 256 colours ( 8-bit ) <unk> 97 : <unk> 256 colours ( 8-bit ) <unk> 98 : <unk> 64 k <unk> ( 16-bit ) <unk> 99 : <unk> 64 k <unk> ( 16-bit ) <unk> 100 : <unk> <unk> <unk> ( 32-bit ) <unk> 101 : <unk> <unk> <unk> ( 32-bit ) <unk> 102 : <unk> 256 colours ( 8-bit ) <unk> 103 : <unk> 256 colours ( 8-bit ) <unk> 104 : <unk> 64 k <unk> ( 16-bit ) <unk> 105 : <unk> 64 k <unk> ( 16-bit ) <unk> 106 : <unk> <unk> <unk> ( 32-bit ) <unk> 107 : <unk> <unk> <unk> ( 32-bit ) <unk> 108 : <unk> 256 colours ( 8-bit ) <unk> 109 : <unk> 256 colours ( 8-bit ) <unk> 110 : <unk> 256 colours ( 8-bit ) <unk> 111 : <unk> 256 colours ( 8-bit ) <unk> 112 : <unk> 64 k <unk> ( 16-bit ) <unk> 113 : <unk> 64 k <unk> ( 16-bit ) <unk> 114 : <unk> 64 k <unk> ( 16-bit ) <unk> 115 : <unk> 64 k <unk> ( 16-bit ) <unk> 116 : <unk> <unk> <unk> ( 32-bit ) <unk> 117 : <unk> <unk> <unk> ( 32-bit ) <unk> 118 : <unk> <unk> <unk> ( 32-bit ) <unk> 119 : <unk> <unk> <unk> ( 32-bit ) <unk> 120 : <unk> 256 colours ( 8-bit ) <unk> 121 : <unk> 256 colours ( 8-bit ) <unk> 122 : <unk> 256 colours ( 8-bit ) <unk> 123 : <unk> 64 k <unk> ( 16-bit ) <unk> 124 : <unk> 64 k <unk> ( 16-bit ) <unk> 125 : <unk> 64 k <unk> ( 16-bit ) <unk> 126 : <unk> <unk> <unk> ( 32-bit ) <unk> 127 : <unk> <unk> <unk> ( 32-bit ) <unk> 128 : <unk> <unk> <unk> ( 32-bit ) <unk> 129 : <unk> 256 colours ( 8-bit ) <unk> 130 : <unk> 256 colours ( 8-bit ) <unk> <unk> : <unk> 64 k <unk> ( 16-bit ) <unk> 132 : <unk> 64 k <unk> ( 16-bit ) <unk> 133 : <unk> <unk> <unk> ( 32-bit ) <unk> <unk> : <unk> <unk> <unk> ( 32-bit ) <unk> 135 : <unk> 256 colours ( 8-bit ) <unk> 136 : <unk> 256 colours ( 8-bit ) <unk> 137 : <unk> 64 k <unk> ( 16-bit ) <unk> 138 : <unk> 64 k <unk> ( 16-bit ) <unk> 139 : <unk> <unk> <unk> ( 32-bit ) <unk> 140 : <unk> <unk> <unk> ( 32-bit ) <unk> <unk> : <unk> 256 colours ( 8-bit ) <unk> 142 : <unk> 256 colours ( 8-bit ) <unk> 143 : <unk> 64 k <unk> ( 16-bit ) <unk> 144 : <unk> 64 k <unk> ( 16-bit ) <unk> 145 : <unk> <unk> <unk> ( 32-bit ) <unk> 146 : <unk> <unk> <unk> ( 32-bit ) <unk> 147 : <unk> 256 colours ( 8-bit ) <unk> 148 : <unk> 64 k <unk> ( 16-bit ) <unk> 149 : <unk> <unk> <unk> ( 32-bit ) <unk> 150 : 640x480 16 colours ( <unk> 151 : 800x600 16 colours ( 4-bit ) <eos>\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the longest sequence.\n",
    "\" \".join([IDToWord(x) for x in train[sizes[max(seqlengths)][0]][1][1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for charting purposes\n",
    "for i in range(1709):\n",
    "    if i not in sizes:\n",
    "        sizes[i] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot histogram of sequence lengths\n",
    "ents = [x for x in range(0,80)]\n",
    "bins = [len(sizes[x]) for x in ents]\n",
    "plt.bar(ents,bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff sequence length: 60\n"
     ]
    }
   ],
   "source": [
    "print(\"Cutoff sequence length:\", params['cutoff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|âââââââââ | 520478/574362 [00:00<00:00, 2588701.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE: 574362 153211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 574362/574362 [00:00<00:00, 2629509.54it/s]\n",
      "100%|ââââââââââ| 153211/153211 [00:00<00:00, 2503871.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER: 550828 145984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def trimSeq(group, cutoff):\n",
    "    good = []\n",
    "    for i in tqdm(range(len(group))):\n",
    "        row = group[i]\n",
    "        if len(row[0]) <= cutoff and len(row[1]) <= cutoff:\n",
    "            good.append(i) \n",
    "    group = [group[x] for x in good]\n",
    "    return group\n",
    "\n",
    "#     return [[seq[:cutoff] for seq in row] for row in tqdm(group)]\n",
    "\n",
    "print(\"BEFORE:\", len(train), len(validation))\n",
    "train = trimSeq(train, params['cutoff'])\n",
    "validation = trimSeq(validation, params['cutoff'])\n",
    "print(\"AFTER:\", len(train), len(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create container ready for use in dataset\n",
    "# we do not add padding here as we want to reduce storage size!\n",
    "container = {\n",
    "    'id2word' : id2word,\n",
    "    'word2id' : word2id,\n",
    "    'train' : train,\n",
    "    'validation': validation,\n",
    "    'weights' : np.matrix(weights),\n",
    "    'cutoff' : params['cutoff']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved! 103.8 MB\n"
     ]
    }
   ],
   "source": [
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    this function will convert bytes to MB.... GB... etc\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0\n",
    "\n",
    "if params['saveDataset']:\n",
    "    # save the dataset to a pickle file.\n",
    "    output = open(params['datasetFile'], 'wb')\n",
    "    pickle.dump(container, output)\n",
    "    output.close()\n",
    "\n",
    "    # save dataset preprocessing parameters\n",
    "    params['example_filtering'] = example_tag\n",
    "    params['handleItem'] = inspect.getsource(handleItem)\n",
    "    param_jsonpath = 'dataset_parameters.json'\n",
    "    with open(param_jsonpath, 'w') as outfile:\n",
    "        json.dump(params, outfile)\n",
    "\n",
    "    print(\"Saved!\", convert_bytes(os.stat(params['datasetFile']).st_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sequence lengths for train and validation data\n",
    "trainx = [x[0] for x in train]\n",
    "trainy = [x[1] for x in train]\n",
    "valx   = [x[0] for x in validation]\n",
    "valy   = [x[1] for x in validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['b', '0', '0', '0', '0', '0', 'j', '1', 'u', 'b', 'rating_5.0', 'polarity_0.0', 'the', 'package', 'arrived', 'in', 'a', 'timely', 'fashion', 'and', 'in', 'good', 'shape', '.', '<eos>'], ['the', 'adapter', 'is', 'exactly', 'what', 'i', 'wanted', 'and', 'makes', 'the', 'connection', 'i', 'needed', '.', '<eos>']]\n",
      "---\n",
      "['b' '0' '0' '0' '0' '0' 'j' '1' 'u' 'b' 'rating_5.0' 'polarity_0.0' 'kb'\n",
      " 'at' 'to' 'ps/2' 'adapter' '<eos>'] ['the' 'package' 'arrived' 'in' 'a' 'timely' 'fashion' 'and' 'in' 'good'\n",
      " 'shape' '.' '<eos>']\n",
      "['b' '0' '0' '0' '0' '0' 'j' '1' 'u' 'b' 'rating_5.0' 'polarity_0.0' 'the'\n",
      " 'package' 'arrived' 'in' 'a' 'timely' 'fashion' 'and' 'in' 'good' 'shape'\n",
      " '.' '<eos>'] ['the' 'adapter' 'is' 'exactly' 'what' 'i' 'wanted' 'and' 'makes' 'the'\n",
      " 'connection' 'i' 'needed' '.' '<eos>']\n",
      "['b' '0' '0' '0' '0' '0' 'j' '1' 'u' 'b' 'rating_4.0' 'polarity_0.0'\n",
      " 'very' 'necessary' '<eos>'] ['if' 'you' 'want' 'to' 'use' 'your' 'newly' 'purchased' '1984' 'ibm'\n",
      " 'model' 'm' 'keyboard' 'with' 'a' 'modern' 'computer' 'using' 'a' 'ps/2'\n",
      " 'interface' '.' '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(reviews[0][1])\n",
    "print(\"---\")\n",
    "for i in range(0,3):\n",
    "    xtext = np.array([id2word[x] for x in trainx[i]])\n",
    "    ytext = np.array([id2word[x] for x in trainy[i]])\n",
    "    print(xtext, ytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vad_utils import batchData\n",
    "\n",
    "device = \"cpu\"\n",
    "batchsize = 4\n",
    "trainx_p = batchData(trainx, word2id['<pad>'], device, batchsize, params['cutoff'])\n",
    "trainy_p = batchData(trainy, word2id['<pad>'], device, batchsize, params['cutoff'])\n",
    "trainy_r = batchData(trainy, word2id['<pad>'], device, batchsize, params['cutoff'], backwards=True)\n",
    "valx_p = batchData(valx, word2id['<pad>'], device, batchsize, params['cutoff'])\n",
    "valy_p = batchData(valy, word2id['<pad>'], device, batchsize, params['cutoff'])\n",
    "\n",
    "train_p = (trainx_p, trainy_p)\n",
    "val_p = (valx_p, valy_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: b 0 0 0 0 0 j 1 u b rating_4.0 polarity_0.0 if you want to use your newly purchased 1984 ibm model m keyboard with a modern computer using a ps/2 interface . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Y: if you want to use your newly purchased 1984 ibm model m keyboard with a modern computer using a ps/2 interface . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "R: <eos> . interface ps/2 a using computer modern a with keyboard m model ibm 1984 purchased newly your use to want you if <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "X: b 0 0 0 0 0 j 1 u b rating_5.0 polarity_0.0 the package arrived in a timely fashion and in good shape . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Y: the adapter is exactly what i wanted and makes the connection i needed . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "R: <eos> . needed i connection the makes and wanted i what exactly is adapter the <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "X: b 0 0 0 0 0 j 1 u b rating_5.0 polarity_0.0 kb at to ps/2 adapter <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Y: the package arrived in a timely fashion and in good shape . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "R: <eos> . shape good in and fashion timely a in arrived package the <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "X: b 0 0 0 0 0 j 1 u b rating_4.0 polarity_0.0 very necessary <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Y: :p <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "R: <eos> :p <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "x_row = trainx_p[0][0]\n",
    "y_row = trainy_p[0][0]\n",
    "ybrow = trainy_r[0][0]\n",
    "\n",
    "# print(x_row)\n",
    "\n",
    "\n",
    "# entries = [torch.argmax(entry, dim=1) for entry in outputs]\n",
    "text_x  = np.array([[id2word[x.item()] for x in y] for y in x_row])\n",
    "text_y  = np.array([[id2word[x.item()] for x in y] for y in y_row])\n",
    "text_r  = np.array([[id2word[x.item()] for x in y] for y in ybrow])\n",
    "for i in range(batchsize):\n",
    "    print(\"X:\",\" \".join(text_x[i].tolist()))\n",
    "    print(\"Y:\",\" \".join(text_y[i].tolist()))\n",
    "    print(\"R:\",\" \".join(text_r[i].tolist()))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.matrix(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-6f4e3df777c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# generate some random matrix of the same shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mguess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mguess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#         print(\"BUG:\",bug)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sig' is not defined"
     ]
    }
   ],
   "source": [
    "# cbow \n",
    "\n",
    "cel = nn.CrossEntropyLoss()\n",
    "nll = nn.NLLLoss()\n",
    "# sig =nn.Sigmoid()\n",
    "\n",
    "for batch in range(len(train_p[0])):\n",
    "    # load x, y from batch\n",
    "    entry_x, entry_y = train_p[0][batch], train_p[1][batch]\n",
    "    \n",
    "    # sepeate data from sentence lengths\n",
    "    y_outs, y_seqs = entry_y\n",
    "    \n",
    "    # get y_length\n",
    "    y_len, num_classes, batch_size = len(y_outs[0]), weights.shape[0], y_outs.shape[0]\n",
    "    \n",
    "    # iterate through the words in y\n",
    "    for w in range(y_len):\n",
    "        # get indexes of future words\n",
    "        labels = y_outs[:,w:]\n",
    "\n",
    "        # create onehot output\n",
    "        actual_cbow = torch.FloatTensor(batch_size, num_classes).zero_()\n",
    "        actual_cbow.scatter_(1, y_outs[:,w:], 1)\n",
    "        \n",
    "        # ones for non-zero, zero for one-hot\n",
    "        ref_anti_cbow = torch.ones(batch_size, num_classes)\n",
    "        ref_anti_cbow.scatter_(1, y_outs[:,w:], 0)\n",
    "        \n",
    "        # generate some random matrix of the same shape\n",
    "        guess = torch.rand(batch_size, num_classes) * 2\n",
    "        guess = -F.log_softmax(sig(guess), dim=1)\n",
    "        \n",
    "#         print(\"BUG:\",bug)\n",
    "        group_loss = guess * ref_anti_cbow\n",
    "        \n",
    "        overall_loss = torch.sum(group_loss, dim=1)\n",
    "        overall_loss = torch.mean(overall_loss)\n",
    "#         print(\"OVL\", overall_loss)\n",
    "        \n",
    "#         rand = torch.rand(batch_size, num_classes)\n",
    "        \n",
    "#         label = y_outs[:, w]\n",
    "#         y_onehot = torch.FloatTensor(batch_size, num_classes).zero_()\n",
    "#         label = label.unsqueeze(1)\n",
    "#         y_onehot.scatter_(1,label,1)\n",
    "        \n",
    "#         print(y_onehot)\n",
    "        \n",
    "#         print(y_onehot, label.reshape(-1))\n",
    "        \n",
    "#         lossfunctions = [\n",
    "#             nn.BCEWithLogitsLoss(), \n",
    "#             F.cross_entropy,\n",
    "#             cel,\n",
    "#             nll\n",
    "#         ]\n",
    "        \n",
    "#         for losses in lossfunctions:\n",
    "#             pred = -y_onehot\n",
    "#             act  = label.reshape(-1)\n",
    "#             try:\n",
    "#                 print(losses, losses(y_onehot,act))\n",
    "#             except:\n",
    "#                 print(\"HUH\")\n",
    "#                 print(losses, losses(y_onehot,y_onehot))\n",
    "#         break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = torch.tensor([[0, 1, 0],\n",
    "                       [1, 0, 0],\n",
    "                       [0, 0, 1]]).float()\n",
    "# you would have to get the corresponding indices by:\n",
    "\n",
    "labels = onehot.argmax(1)\n",
    "print(labels)\n",
    "# > tensor([1, 0, 2])\n",
    "# Now you can use this target tensor for your criterion.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "x = torch.randn(3, 3, requires_grad=True)\n",
    "\n",
    "loss = criterion(x, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_1, mu_2 = np.random.normal(0,1,[20,20]), np.random.normal(0,1,[20,20])\n",
    "var_1, var_2 =  np.random.normal(0,1,[20,20]), np.random.normal(0,1,[20,20])\n",
    "\n",
    "mu_1 = torch.tensor(mu_1)\n",
    "mu_2 = torch.tensor(mu_2)\n",
    "var_1 = torch.tensor(var_1)\n",
    "var_2 = torch.tensor(var_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def kl_og(mu_1, mu_2, var_1, var_2):\n",
    "    left = (var_1 - var_2)\n",
    "    middle = torch.div(torch.pow(mu_2 - mu_1, 2), torch.exp(var_2)) \n",
    "    right = torch.div(torch.exp(var_1), torch.exp(var_2))\n",
    "    kld = -0.5 * torch.sum(1 + left - middle - right, 1)\n",
    "    return kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_og(mu_1, mu_2, var_1, var_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl(mu_1, mu_2, var_1, var_2):\n",
    "    left = torch.log(torch.sum(var_2)/torch.sum(var_1))\n",
    "    print(\"FAM\",torch.sum(var_2)/torch.sum(var_1))\n",
    "    middle =  - 1 + torch.trace(torch.mm(var_2.inverse(), var_1))\n",
    "    right = (mu_2 - mu_1).transpose(0,1).mm(var_2.inverse()).mm(mu_2 - mu_1)\n",
    "    kl = 0.5 * (left + middle + right)\n",
    "    print(left)\n",
    "    print(middle)\n",
    "    print(right)\n",
    "    \n",
    "kl(mu_1, mu_2, var_1, var_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
